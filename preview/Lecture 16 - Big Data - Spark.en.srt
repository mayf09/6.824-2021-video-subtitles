1
00:00:00,000 --> 00:00:03,840
be talking about uh is spark

2
00:00:03,840 --> 00:00:05,759
and so this goes back to sort of almost

3
00:00:05,759 --> 00:00:07,680
through the beginning of the semester

4
00:00:07,680 --> 00:00:10,080
uh where you know we talked a lot quite

5
00:00:10,080 --> 00:00:11,759
a bit about mapreduce in fact you know

6
00:00:11,759 --> 00:00:12,559
lab one

7
00:00:12,559 --> 00:00:15,759
you implemented mapreduce

8
00:00:15,759 --> 00:00:18,800
so and really what's you know one sort

9
00:00:18,800 --> 00:00:20,000
of informally you know

10
00:00:20,000 --> 00:00:25,109
the spark is basically the successor

11
00:00:25,119 --> 00:00:28,790
you know to hadoop

12
00:00:28,800 --> 00:00:30,880
and hadoop is the sort of open source

13
00:00:30,880 --> 00:00:31,840
version

14
00:00:31,840 --> 00:00:38,549
of uh of mapreduce

15
00:00:38,559 --> 00:00:41,600
so i think today uh you know people

16
00:00:41,600 --> 00:00:43,040
typically will use

17
00:00:43,040 --> 00:00:46,640
spark instead of hadoop

18
00:00:46,640 --> 00:00:52,709
and so it's really widely used

19
00:00:52,719 --> 00:00:55,280
and it's widely used for data science

20
00:00:55,280 --> 00:00:57,440
computation so people that you know have

21
00:00:57,440 --> 00:00:59,280
lots and lots of data that need to run

22
00:00:59,280 --> 00:01:00,800
some computation over it

23
00:01:00,800 --> 00:01:03,120
require a ton of machines uh you know

24
00:01:03,120 --> 00:01:04,000
spark

25
00:01:04,000 --> 00:01:07,920
is designed for that particular case

26
00:01:07,920 --> 00:01:10,840
it is commercialized by a company called

27
00:01:10,840 --> 00:01:14,789
databricks

28
00:01:14,799 --> 00:01:16,479
who's the offer or the main offer of

29
00:01:16,479 --> 00:01:17,920
this paper in his it's his

30
00:01:17,920 --> 00:01:20,479
phd phases started with a number of

31
00:01:20,479 --> 00:01:22,400
other people this company databricks

32
00:01:22,400 --> 00:01:25,680
uh which you know commercializes spark

33
00:01:25,680 --> 00:01:28,320
but it also supports the apache open

34
00:01:28,320 --> 00:01:29,840
source

35
00:01:29,840 --> 00:01:33,520
smart version this is a

36
00:01:33,520 --> 00:01:36,320
pretty popular open source project or

37
00:01:36,320 --> 00:01:39,280
very popular open source project

38
00:01:39,280 --> 00:01:42,399
it is one reason has sort of replaced

39
00:01:42,399 --> 00:01:43,439
you know the use of

40
00:01:43,439 --> 00:01:45,360
hadoop is because it actually supports a

41
00:01:45,360 --> 00:01:47,520
wider range

42
00:01:47,520 --> 00:01:49,439
uh wider range of applications than

43
00:01:49,439 --> 00:01:52,230
mapreduce can

44
00:01:52,240 --> 00:01:54,799
in particular they're very good at this

45
00:01:54,799 --> 00:01:56,960
iterative

46
00:01:56,960 --> 00:02:00,709
what happened there

47
00:02:00,719 --> 00:02:03,680
let me see something maybe external

48
00:02:03,680 --> 00:02:04,240
crash

49
00:02:04,240 --> 00:02:20,830
hold on a second

50
00:02:20,840 --> 00:02:34,830
uh

51
00:02:34,840 --> 00:02:36,239
story

52
00:02:36,239 --> 00:02:39,760
okay um fortunately i think we're in

53
00:02:39,760 --> 00:02:42,959
good shape uh okay so it supports a

54
00:02:42,959 --> 00:02:44,879
wider range of applications

55
00:02:44,879 --> 00:02:46,879
um and in particular is good at these

56
00:02:46,879 --> 00:02:48,800
iterative applications so applications

57
00:02:48,800 --> 00:02:49,360
where

58
00:02:49,360 --> 00:02:51,120
there are multiple rounds of mapreduce

59
00:02:51,120 --> 00:02:52,800
operations so if you have an application

60
00:02:52,800 --> 00:02:53,920
that requires sort of one

61
00:02:53,920 --> 00:02:55,599
set of mapreduce followed by another set

62
00:02:55,599 --> 00:02:57,040
of mapreduce followed by another mapper

63
00:02:57,040 --> 00:02:57,760
series

64
00:02:57,760 --> 00:02:59,519
computation you know spark is really

65
00:02:59,519 --> 00:03:01,200
good at it and the reason it's good at

66
00:03:01,200 --> 00:03:02,879
it is because basically it keeps the

67
00:03:02,879 --> 00:03:04,480
intermediate results in memory and

68
00:03:04,480 --> 00:03:05,920
that's really good support programming

69
00:03:05,920 --> 00:03:06,959
support for

70
00:03:06,959 --> 00:03:11,190
uh doing so

71
00:03:11,200 --> 00:03:13,840
in some ways uh you know the sort of if

72
00:03:13,840 --> 00:03:14,800
there's any

73
00:03:14,800 --> 00:03:16,159
connection at all between the previous

74
00:03:16,159 --> 00:03:17,840
paper and this paper which basically is

75
00:03:17,840 --> 00:03:18,480
not

76
00:03:18,480 --> 00:03:20,879
uh but you know they're all both

77
00:03:20,879 --> 00:03:21,599
targeted

78
00:03:21,599 --> 00:03:27,910
to sort of in memory computations

79
00:03:27,920 --> 00:03:29,360
you know for data sets that basically

80
00:03:29,360 --> 00:03:32,080
fit in memory in the previous paper and

81
00:03:32,080 --> 00:03:33,840
farm paper is all about you know the

82
00:03:33,840 --> 00:03:35,280
database fitting in memory

83
00:03:35,280 --> 00:03:37,599
here is the data set of the data science

84
00:03:37,599 --> 00:03:38,959
computation for the data science

85
00:03:38,959 --> 00:03:42,949
computation that you want to do

86
00:03:42,959 --> 00:03:44,640
of course you know since 2012 whenever

87
00:03:44,640 --> 00:03:46,159
this paper was published a lot of things

88
00:03:46,159 --> 00:03:47,280
have happened

89
00:03:47,280 --> 00:03:50,309
uh the

90
00:03:50,319 --> 00:03:53,360
spark is not really tied to uh scala as

91
00:03:53,360 --> 00:03:54,799
i sort of described in this paper but

92
00:03:54,799 --> 00:03:56,159
there are other language front ends for

93
00:03:56,159 --> 00:03:57,360
example

94
00:03:57,360 --> 00:03:59,040
but probably more importantly you know

95
00:03:59,040 --> 00:04:01,280
the

96
00:04:01,280 --> 00:04:03,760
rdds as defined in this paper slightly

97
00:04:03,760 --> 00:04:05,280
deprecated

98
00:04:05,280 --> 00:04:09,270
and replaced by

99
00:04:09,280 --> 00:04:11,920
data frames but data frames the way to

100
00:04:11,920 --> 00:04:12,879
think about it or

101
00:04:12,879 --> 00:04:14,000
the way i think about it this is

102
00:04:14,000 --> 00:04:16,959
basically column is an rdd with

103
00:04:16,959 --> 00:04:19,759
explicit columns and all the good ideas

104
00:04:19,759 --> 00:04:22,880
of rdds are also true for

105
00:04:22,880 --> 00:04:25,919
data frames and so from the rest of the

106
00:04:25,919 --> 00:04:27,040
lecture i'm just going to talk about

107
00:04:27,040 --> 00:04:29,680
rdds

108
00:04:29,680 --> 00:04:33,040
and think about them as equivalently as

109
00:04:33,040 --> 00:04:33,520
to

110
00:04:33,520 --> 00:04:37,120
data frames any questions

111
00:04:37,120 --> 00:04:46,150
before i proceed

112
00:04:46,160 --> 00:04:48,560
uh and then a quick other point maybe

113
00:04:48,560 --> 00:04:50,400
this is this research is really uh quite

114
00:04:50,400 --> 00:04:52,080
successful and except you know

115
00:04:52,080 --> 00:04:55,360
it's a white reuse also uh mate

116
00:04:55,360 --> 00:04:58,080
uh you know received the uh acm doctoral

117
00:04:58,080 --> 00:04:58,639
thesis

118
00:04:58,639 --> 00:05:00,960
uh award for it is for the for his uh

119
00:05:00,960 --> 00:05:02,000
thesis that basically

120
00:05:02,000 --> 00:05:05,199
is all about spark so it's quite unusual

121
00:05:05,199 --> 00:05:06,479
actually for uh

122
00:05:06,479 --> 00:05:08,000
you know doctoral thesis that to have

123
00:05:08,000 --> 00:05:16,390
that kind of impact

124
00:05:16,400 --> 00:05:20,560
okay so the way

125
00:05:20,560 --> 00:05:22,240
i want to talk about spark is by just

126
00:05:22,240 --> 00:05:25,039
looking at the some of the examples

127
00:05:25,039 --> 00:05:28,880
because i think you you get the best

128
00:05:28,880 --> 00:05:32,000
you understand the programming model and

129
00:05:32,000 --> 00:05:34,479
that is based on rdds

130
00:05:34,479 --> 00:05:40,230
best by just i think looking at examples

131
00:05:40,240 --> 00:05:42,240
and you get the best idea of what

132
00:05:42,240 --> 00:05:43,840
actually an rdg is

133
00:05:43,840 --> 00:05:45,520
and so let me pull up some of the

134
00:05:45,520 --> 00:05:48,000
examples that are in the paper

135
00:05:48,000 --> 00:05:51,199
and then we'll walk through those

136
00:05:51,199 --> 00:05:55,680
um pick this one so let's start here

137
00:05:55,680 --> 00:05:59,440
a very simple uh example and so

138
00:05:59,440 --> 00:06:02,800
you know the idea is that

139
00:06:02,800 --> 00:06:05,520
uh in this example is you know first of

140
00:06:05,520 --> 00:06:06,000
all you can

141
00:06:06,000 --> 00:06:09,120
sort of use uh spark interactively

142
00:06:09,120 --> 00:06:11,120
you consider your startup spark on your

143
00:06:11,120 --> 00:06:13,280
workstation or your laptop

144
00:06:13,280 --> 00:06:16,880
and start interacting uh with spark

145
00:06:16,880 --> 00:06:19,520
um and you know the way you know you

146
00:06:19,520 --> 00:06:21,440
could type in commands like this

147
00:06:21,440 --> 00:06:23,520
and so now what does this command do you

148
00:06:23,520 --> 00:06:24,800
know this basically

149
00:06:24,800 --> 00:06:27,759
uh creates an rdd you know called this

150
00:06:27,759 --> 00:06:29,360
is rdds

151
00:06:29,360 --> 00:06:32,720
lines is an rdd and it just represents

152
00:06:32,720 --> 00:06:35,919
the an rdd that actually stored in hdfs

153
00:06:35,919 --> 00:06:40,080
and you know the hdfs you know might be

154
00:06:40,080 --> 00:06:41,840
have money partitions you know for this

155
00:06:41,840 --> 00:06:44,000
particular file so for example

156
00:06:44,000 --> 00:06:45,280
the first you know thousands or a

157
00:06:45,280 --> 00:06:47,120
million records list on like partition

158
00:06:47,120 --> 00:06:48,639
one there's next

159
00:06:48,639 --> 00:06:50,240
million live in partition two and the

160
00:06:50,240 --> 00:06:52,479
next mill lift to impetus free

161
00:06:52,479 --> 00:06:54,880
and this rdd uh lines basically

162
00:06:54,880 --> 00:06:55,680
represents

163
00:06:55,680 --> 00:07:00,000
you know that uh that set of partitions

164
00:07:00,000 --> 00:07:02,800
uh when you uh run this line or you type

165
00:07:02,800 --> 00:07:04,000
in this line and you return

166
00:07:04,000 --> 00:07:05,759
hit return basically nothing really

167
00:07:05,759 --> 00:07:08,400
happens uh uh and so this is what the

168
00:07:08,400 --> 00:07:11,440
the paper refers to as lazy computations

169
00:07:11,440 --> 00:07:13,199
in fact the computation is executed some

170
00:07:13,199 --> 00:07:14,639
point later and we'll see

171
00:07:14,639 --> 00:07:16,400
in a second when uh but at this

172
00:07:16,400 --> 00:07:18,400
particular point the only thing that

173
00:07:18,400 --> 00:07:20,560
actually happened is that there is a

174
00:07:20,560 --> 00:07:22,800
line a lens object uh that happens to be

175
00:07:22,800 --> 00:07:24,560
an rdd

176
00:07:24,560 --> 00:07:27,599
and uh an rdd you know supports

177
00:07:27,599 --> 00:07:30,319
sort of a wide range of operations we

178
00:07:30,319 --> 00:07:31,599
can actually look at a little bit of

179
00:07:31,599 --> 00:07:34,479
some of the operations that it supports

180
00:07:34,479 --> 00:07:37,840
uh yeah

181
00:07:37,840 --> 00:07:40,960
so it's used as a an rdd has an api

182
00:07:40,960 --> 00:07:42,960
uh and it turns out that the methods on

183
00:07:42,960 --> 00:07:44,319
the api

184
00:07:44,319 --> 00:07:46,240
or the methods on an rdd fall into two

185
00:07:46,240 --> 00:07:48,000
classes

186
00:07:48,000 --> 00:07:51,520
one are the our actions and actions are

187
00:07:51,520 --> 00:07:52,080
really

188
00:07:52,080 --> 00:07:54,400
the operations that will actually cause

189
00:07:54,400 --> 00:07:56,160
computation to happen

190
00:07:56,160 --> 00:07:58,080
and so all the lazily sort of build up

191
00:07:58,080 --> 00:08:00,080
computation really happens at the point

192
00:08:00,080 --> 00:08:01,120
that you run an action

193
00:08:01,120 --> 00:08:03,919
as an example you run count or collect

194
00:08:03,919 --> 00:08:05,199
then

195
00:08:05,199 --> 00:08:07,919
the the spark computation actually is

196
00:08:07,919 --> 00:08:09,840
executed

197
00:08:09,840 --> 00:08:12,960
all the other api or methods

198
00:08:12,960 --> 00:08:15,440
are transformations and they basically

199
00:08:15,440 --> 00:08:16,639
take one rdd

200
00:08:16,639 --> 00:08:19,120
and turn it into another rdd it turns

201
00:08:19,120 --> 00:08:20,479
out that every rdd

202
00:08:20,479 --> 00:08:23,520
is actually read-only or immutable so

203
00:08:23,520 --> 00:08:25,520
you can't modify

204
00:08:25,520 --> 00:08:28,560
an rdd you can only basically

205
00:08:28,560 --> 00:08:31,759
generate new rdds from an existing one

206
00:08:31,759 --> 00:08:33,599
and so if we look at the second line

207
00:08:33,599 --> 00:08:35,519
this basically creates a second rdd

208
00:08:35,519 --> 00:08:37,519
there are the rdd errors

209
00:08:37,519 --> 00:08:40,959
and that one is created by uh running a

210
00:08:40,959 --> 00:08:41,839
filter

211
00:08:41,839 --> 00:08:44,959
on the lines rde so the lines are

212
00:08:44,959 --> 00:08:45,680
deducing

213
00:08:45,680 --> 00:08:48,959
read only is read only

214
00:08:48,959 --> 00:08:50,959
you can run this method to filter on it

215
00:08:50,959 --> 00:08:53,120
which in this case basically filters out

216
00:08:53,120 --> 00:08:55,040
all the records there that start with or

217
00:08:55,040 --> 00:08:56,800
all the lines that start with the

218
00:08:56,800 --> 00:08:58,720
message error or the string error and

219
00:08:58,720 --> 00:09:00,800
that represents a new rdd

220
00:09:00,800 --> 00:09:03,360
and again at this point nothing actually

221
00:09:03,360 --> 00:09:04,880
is really being computed it's just like

222
00:09:04,880 --> 00:09:05,600
a recipe

223
00:09:05,600 --> 00:09:07,120
or you know it builds up like almost

224
00:09:07,120 --> 00:09:08,959
like a data flow uh

225
00:09:08,959 --> 00:09:10,880
or what the people call the lineage

226
00:09:10,880 --> 00:09:20,829
graph of the computation

227
00:09:20,839 --> 00:09:22,000
um

228
00:09:22,000 --> 00:09:24,560
also when the computation actually

229
00:09:24,560 --> 00:09:25,519
starts running

230
00:09:25,519 --> 00:09:26,959
it hasn't run yet but you know when it

231
00:09:26,959 --> 00:09:29,120
will start running uh these operations

232
00:09:29,120 --> 00:09:30,560
are pipelined

233
00:09:30,560 --> 00:09:33,279
uh and with that they mean that uh so

234
00:09:33,279 --> 00:09:34,080
for example

235
00:09:34,080 --> 00:09:36,399
in stage one you know the this

236
00:09:36,399 --> 00:09:37,920
computation of the lines

237
00:09:37,920 --> 00:09:40,480
the stage one will read some set of

238
00:09:40,480 --> 00:09:41,760
records you know from

239
00:09:41,760 --> 00:09:44,959
uh for example this first partition um

240
00:09:44,959 --> 00:09:48,000
and then uh do its processing on it if

241
00:09:48,000 --> 00:09:49,200
there's anything and then hand handed

242
00:09:49,200 --> 00:09:50,320
off to stage two

243
00:09:50,320 --> 00:09:52,160
you know which is stage two is basically

244
00:09:52,160 --> 00:09:53,839
you know doing this filter

245
00:09:53,839 --> 00:09:56,959
and so in stage two you know the uh

246
00:09:56,959 --> 00:10:00,480
uh the uh this filter will run

247
00:10:00,480 --> 00:10:02,480
and sort of grab out you know the lines

248
00:10:02,480 --> 00:10:04,560
that actually are

249
00:10:04,560 --> 00:10:06,959
that match and you know basically

250
00:10:06,959 --> 00:10:09,519
produce with that a new rdd

251
00:10:09,519 --> 00:10:11,519
that just all contains you know the

252
00:10:11,519 --> 00:10:13,200
lines you know strings with that start

253
00:10:13,200 --> 00:10:13,519
with

254
00:10:13,519 --> 00:10:16,720
lines that start with the string air

255
00:10:16,720 --> 00:10:19,680
and while uh this sort of second rdd or

256
00:10:19,680 --> 00:10:21,200
the second stage runs you know the first

257
00:10:21,200 --> 00:10:22,880
stage you know grabs the next set of

258
00:10:22,880 --> 00:10:25,279
uh records from the file system and then

259
00:10:25,279 --> 00:10:27,120
you know feeds them again to stage two

260
00:10:27,120 --> 00:10:29,600
and so as you go further further you

261
00:10:29,600 --> 00:10:31,279
have more and more stages in your

262
00:10:31,279 --> 00:10:34,000
pipeline or your your

263
00:10:34,000 --> 00:10:36,240
lineage graph you know all those stages

264
00:10:36,240 --> 00:10:37,839
are going to be running in sort of

265
00:10:37,839 --> 00:10:40,160
parallel and that's what i mean with

266
00:10:40,160 --> 00:10:41,760
like sort of pipelining

267
00:10:41,760 --> 00:10:46,800
the transformations okay so

268
00:10:46,800 --> 00:10:50,240
um so here this so this you know

269
00:10:50,240 --> 00:10:52,160
this line basically describes how to

270
00:10:52,160 --> 00:10:54,560
create the rdd error

271
00:10:54,560 --> 00:10:57,680
uh the rdd errors rdd and then this line

272
00:10:57,680 --> 00:10:58,800
basically says like

273
00:10:58,800 --> 00:11:02,480
uh tell spark to basically keep a copy

274
00:11:02,480 --> 00:11:04,399
of this rdd in memory

275
00:11:04,399 --> 00:11:08,069
so a subsequent

276
00:11:08,079 --> 00:11:09,839
computation run that you know do more

277
00:11:09,839 --> 00:11:12,160
stuff with errors

278
00:11:12,160 --> 00:11:14,399
spark will actually keep the original

279
00:11:14,399 --> 00:11:16,160
rdd actually in memory

280
00:11:16,160 --> 00:11:18,800
uh so that uh it can be shared with

281
00:11:18,800 --> 00:11:20,399
later computation so for example if you

282
00:11:20,399 --> 00:11:22,720
wanted to reuse the error file

283
00:11:22,720 --> 00:11:24,720
then you know that error file will be in

284
00:11:24,720 --> 00:11:25,839
memory it doesn't have to be

285
00:11:25,839 --> 00:11:27,519
reconstructed from the files in the

286
00:11:27,519 --> 00:11:28,959
vector we represented

287
00:11:28,959 --> 00:11:33,040
hdfs and allows you to run the second

288
00:11:33,040 --> 00:11:34,720
computation

289
00:11:34,720 --> 00:11:36,560
and here for example one already you

290
00:11:36,560 --> 00:11:38,079
know even in this simple example you'll

291
00:11:38,079 --> 00:11:39,120
see there's a bit sort of a big

292
00:11:39,120 --> 00:11:40,160
difference between this

293
00:11:40,160 --> 00:11:42,240
and mapreduce where can the mapreduce

294
00:11:42,240 --> 00:11:44,320
job you know you run the computation it

295
00:11:44,320 --> 00:11:45,120
ends

296
00:11:45,120 --> 00:11:47,200
and then if you want to read you know

297
00:11:47,200 --> 00:11:48,560
redo something with the data

298
00:11:48,560 --> 00:11:50,720
you have to re-read it in from the file

299
00:11:50,720 --> 00:11:51,920
system and

300
00:11:51,920 --> 00:11:55,440
using the sort of precious method

301
00:11:55,440 --> 00:11:57,440
spark can avoid you know having to

302
00:11:57,440 --> 00:11:58,720
re-read

303
00:11:58,720 --> 00:12:00,320
that data from the disk and you know

304
00:12:00,320 --> 00:12:05,990
save a lot of time

305
00:12:06,000 --> 00:12:09,279
any questions so far um so when the air

306
00:12:09,279 --> 00:12:10,079
file

307
00:12:10,079 --> 00:12:12,399
gets extracted from p1 let's say and

308
00:12:12,399 --> 00:12:13,360
then there's

309
00:12:13,360 --> 00:12:15,440
another error file that gets extracted

310
00:12:15,440 --> 00:12:17,440
from p2 so my understanding is that

311
00:12:17,440 --> 00:12:18,880
these happen in parallel

312
00:12:18,880 --> 00:12:21,200
yes so you can think about it like you

313
00:12:21,200 --> 00:12:22,959
know they're going to be mini worker

314
00:12:22,959 --> 00:12:25,360
like in the mapreduce and the workers

315
00:12:25,360 --> 00:12:27,760
work on each petition

316
00:12:27,760 --> 00:12:29,760
so basically you know the scheduler will

317
00:12:29,760 --> 00:12:32,000
send a job you know to each of the

318
00:12:32,000 --> 00:12:33,360
workers

319
00:12:33,360 --> 00:12:36,800
and a job is or task you know pertains

320
00:12:36,800 --> 00:12:38,800
to a particular petition

321
00:12:38,800 --> 00:12:43,200
and the workers start working on

322
00:12:43,200 --> 00:12:44,560
get one of these tasks and basically

323
00:12:44,560 --> 00:12:46,639
start running so you get parallelism

324
00:12:46,639 --> 00:12:47,279
between the

325
00:12:47,279 --> 00:12:49,120
partitions and you get parallelism

326
00:12:49,120 --> 00:12:52,399
between the stages in the pipeline

327
00:12:52,399 --> 00:12:55,829
i see thank you

328
00:12:55,839 --> 00:12:58,959
what's the sorry can you hear me uh

329
00:12:58,959 --> 00:13:00,639
what's the difference between um

330
00:13:00,639 --> 00:13:03,760
the lineage and the like just the log of

331
00:13:03,760 --> 00:13:05,360
transactions that we've seen before like

332
00:13:05,360 --> 00:13:06,000
is it just the

333
00:13:06,000 --> 00:13:09,120
granularity of the operations

334
00:13:09,120 --> 00:13:11,360
as we'll see lineage uh the log is

335
00:13:11,360 --> 00:13:12,399
strictly linear

336
00:13:12,399 --> 00:13:14,079
right and the examples that we've seen

337
00:13:14,079 --> 00:13:16,320
so far the the

338
00:13:16,320 --> 00:13:18,079
the lineage is also linear but we'll see

339
00:13:18,079 --> 00:13:19,440
in later examples where

340
00:13:19,440 --> 00:13:22,560
you know we have forks where one stage

341
00:13:22,560 --> 00:13:24,079
depends on multiple

342
00:13:24,079 --> 00:13:27,519
uh different rdds and you know that's

343
00:13:27,519 --> 00:13:29,120
just not representative in the log

344
00:13:29,120 --> 00:13:32,639
right you know there share some

345
00:13:32,639 --> 00:13:34,160
similarities in the sense that like you

346
00:13:34,160 --> 00:13:35,360
start in the

347
00:13:35,360 --> 00:13:36,959
beginning state all the operations are

348
00:13:36,959 --> 00:13:38,880
deterministic and then you will end up

349
00:13:38,880 --> 00:13:39,519
in some

350
00:13:39,519 --> 00:13:40,959
and if you apply all those operations

351
00:13:40,959 --> 00:13:42,639
you will end in some you know

352
00:13:42,639 --> 00:13:45,279
deterministic end state uh so in that

353
00:13:45,279 --> 00:13:46,720
sense you know there's some similarity

354
00:13:46,720 --> 00:13:48,160
but you know i think

355
00:13:48,160 --> 00:13:51,829
they're quite different

356
00:13:51,839 --> 00:13:54,160
uh i also have a question in this case

357
00:13:54,160 --> 00:13:55,199
filter

358
00:13:55,199 --> 00:13:58,720
um it just so it works by just applying

359
00:13:58,720 --> 00:14:00,639
filter on each partition but sometimes

360
00:14:00,639 --> 00:14:02,399
like i see the transformations also

361
00:14:02,399 --> 00:14:03,440
contain join

362
00:14:03,440 --> 00:14:05,440
or sort yeah yeah let's talk about these

363
00:14:05,440 --> 00:14:06,720
a little bit i will talk about sword and

364
00:14:06,720 --> 00:14:09,750
join in a second okay

365
00:14:09,760 --> 00:14:13,680
they're clearly much more uh complicated

366
00:14:13,680 --> 00:14:16,959
so is uh do we like does persist

367
00:14:16,959 --> 00:14:20,160
is when we start like computing no

368
00:14:20,160 --> 00:14:21,680
no nothing has been completed yet so

369
00:14:21,680 --> 00:14:23,279
still all descriptions so let's talk a

370
00:14:23,279 --> 00:14:24,880
little bit further

371
00:14:24,880 --> 00:14:27,120
um so let's look at actually something

372
00:14:27,120 --> 00:14:28,160
that actually generates

373
00:14:28,160 --> 00:14:32,560
a computation so

374
00:14:32,560 --> 00:14:36,079
so here are two commands that actually

375
00:14:36,079 --> 00:14:39,839
result in computation so this command

376
00:14:39,839 --> 00:14:41,360
will result in computation because it

377
00:14:41,360 --> 00:14:44,079
contains counts which is an action

378
00:14:44,079 --> 00:14:46,399
and this command will result in a

379
00:14:46,399 --> 00:14:47,440
computation collect

380
00:14:47,440 --> 00:14:51,040
is an action and so

381
00:14:51,040 --> 00:14:54,320
uh yeah yeah and so you can

382
00:14:54,320 --> 00:14:56,560
and so we look it looks like that's a

383
00:14:56,560 --> 00:14:57,839
reason that they show two commands is

384
00:14:57,839 --> 00:14:59,199
because to demonstrate that basically

385
00:14:59,199 --> 00:15:00,880
you can reuse errors

386
00:15:00,880 --> 00:15:03,839
uh and so if you look at you know this

387
00:15:03,839 --> 00:15:04,480
uh

388
00:15:04,480 --> 00:15:07,279
computation yeah then you can draw the

389
00:15:07,279 --> 00:15:08,560
lineage graph

390
00:15:08,560 --> 00:15:12,480
right so it starts with lines

391
00:15:12,480 --> 00:15:18,470
uh there was a filter that we ran

392
00:15:18,480 --> 00:15:21,519
uh oops sorry let me write this slightly

393
00:15:21,519 --> 00:15:24,079
differently there was a filter and lines

394
00:15:24,079 --> 00:15:28,230
that we just saw that produces errors

395
00:15:28,240 --> 00:15:29,759
or that's the description how to get

396
00:15:29,759 --> 00:15:31,759
errors uh then there's an

397
00:15:31,759 --> 00:15:34,560
in this case there's another filter it's

398
00:15:34,560 --> 00:15:37,279
the filter on hdfs

399
00:15:37,279 --> 00:15:39,279
and that basically produces another rdd

400
00:15:39,279 --> 00:15:40,959
you know that rdd is not explicitly

401
00:15:40,959 --> 00:15:42,399
named here

402
00:15:42,399 --> 00:15:44,480
but you know it does produce another rdd

403
00:15:44,480 --> 00:15:45,839
so i'm just going to call it

404
00:15:45,839 --> 00:15:50,160
hdfs because it filters on hdfs

405
00:15:50,160 --> 00:15:54,160
and then we see there's a map and that

406
00:15:54,160 --> 00:15:55,920
produces yet another rdd

407
00:15:55,920 --> 00:15:58,160
uh again this rdd doesn't really have a

408
00:15:58,160 --> 00:15:59,519
name here uh but

409
00:15:59,519 --> 00:16:02,399
uh so it's anonymous uh but i'm just

410
00:16:02,399 --> 00:16:03,279
gonna give it a name

411
00:16:03,279 --> 00:16:06,880
that's time because basically

412
00:16:06,880 --> 00:16:10,800
uh splits the one in your line into

413
00:16:10,800 --> 00:16:11,680
three pieces and

414
00:16:11,680 --> 00:16:13,360
grabs the third piece out of that line

415
00:16:13,360 --> 00:16:15,680
and that happens to be the time

416
00:16:15,680 --> 00:16:18,880
and then uh there's a final you know

417
00:16:18,880 --> 00:16:22,079
operation and that's collect

418
00:16:22,079 --> 00:16:24,639
uh which then actually counts up all the

419
00:16:24,639 --> 00:16:26,000
the number times that time actually

420
00:16:26,000 --> 00:16:27,839
appears or the number entries basically

421
00:16:27,839 --> 00:16:29,199
the result of the

422
00:16:29,199 --> 00:16:33,440
in the rdd produced uh in the time rdd

423
00:16:33,440 --> 00:16:36,720
okay so this so this is actually

424
00:16:36,720 --> 00:16:39,120
at this point when you know the the

425
00:16:39,120 --> 00:16:40,959
return you hit return here in the user

426
00:16:40,959 --> 00:16:43,279
interface or in the uh

427
00:16:43,279 --> 00:16:45,920
interactive uh user interface uh it is

428
00:16:45,920 --> 00:16:46,399
and

429
00:16:46,399 --> 00:16:49,360
at that point basically uh spark you

430
00:16:49,360 --> 00:16:50,720
know we'll

431
00:16:50,720 --> 00:16:52,800
collect you know the a lot of a set of

432
00:16:52,800 --> 00:16:53,839
workers

433
00:16:53,839 --> 00:16:57,040
uh split you know send them the jobs or

434
00:16:57,040 --> 00:16:59,680
basically inform the scheduler that the

435
00:16:59,680 --> 00:17:01,360
job needs to be executed

436
00:17:01,360 --> 00:17:03,759
uh and the description of the task that

437
00:17:03,759 --> 00:17:04,559
needs to be

438
00:17:04,559 --> 00:17:13,270
executed is this lineage graph

439
00:17:13,280 --> 00:17:14,720
and so we can sort of think a little bit

440
00:17:14,720 --> 00:17:16,400
about exactly how the execution happens

441
00:17:16,400 --> 00:17:22,069
so let me draw a picture

442
00:17:22,079 --> 00:17:23,679
so the picture just as follows we got

443
00:17:23,679 --> 00:17:24,959
this what is called the driver that's

444
00:17:24,959 --> 00:17:26,640
the usual thing

445
00:17:26,640 --> 00:17:28,240
you know that's where the program that

446
00:17:28,240 --> 00:17:30,000
the user typed into

447
00:17:30,000 --> 00:17:33,039
uh it uh

448
00:17:33,039 --> 00:17:35,679
starts off collecting a bunch of workers

449
00:17:35,679 --> 00:17:38,080
you know a bunch of machines

450
00:17:38,080 --> 00:17:40,080
almost the same way like as in uh

451
00:17:40,080 --> 00:17:42,400
mapreduce

452
00:17:42,400 --> 00:17:44,799
and you know there's going to be an hd

453
00:17:44,799 --> 00:17:46,080
vest there's this

454
00:17:46,080 --> 00:17:49,039
lines file that actually has partitions

455
00:17:49,039 --> 00:17:49,679
p1

456
00:17:49,679 --> 00:17:55,669
p2 whatever

457
00:17:55,679 --> 00:17:57,200
and typically the number of partitions

458
00:17:57,200 --> 00:17:59,200
is larger than the number of workers

459
00:17:59,200 --> 00:18:01,520
uh sorry the number of yes the numerous

460
00:18:01,520 --> 00:18:02,720
partitions is larger than the number of

461
00:18:02,720 --> 00:18:04,320
workers you know you go to you know get

462
00:18:04,320 --> 00:18:05,919
some load balance

463
00:18:05,919 --> 00:18:07,600
uh if like one petition is small and the

464
00:18:07,600 --> 00:18:09,039
other one is big and

465
00:18:09,039 --> 00:18:10,240
you don't want to have workers relying

466
00:18:10,240 --> 00:18:12,240
sitting around idle

467
00:18:12,240 --> 00:18:15,679
and basically the scheduler you know the

468
00:18:15,679 --> 00:18:19,360
there's a scheduler that runs

469
00:18:19,360 --> 00:18:20,960
you know basically the computation and

470
00:18:20,960 --> 00:18:22,720
exa has the you know the information

471
00:18:22,720 --> 00:18:25,679
that has the lineage graph

472
00:18:25,679 --> 00:18:27,600
and so workers basically check in you

473
00:18:27,600 --> 00:18:31,039
know the driver exercise and the code

474
00:18:31,039 --> 00:18:33,520
the spark program and that would just

475
00:18:33,520 --> 00:18:34,320
construct it

476
00:18:34,320 --> 00:18:35,760
and then the work actually go basically

477
00:18:35,760 --> 00:18:37,280
to the schedule and say please you know

478
00:18:37,280 --> 00:18:39,200
which partition should i work

479
00:18:39,200 --> 00:18:43,600
and uh and then they run uh basically

480
00:18:43,600 --> 00:18:46,640
uh a part of the pipeline

481
00:18:46,640 --> 00:18:48,320
and so we look at this let me actually

482
00:18:48,320 --> 00:18:49,600
draw this slightly differently so i have

483
00:18:49,600 --> 00:18:51,039
a little bit more space here

484
00:18:51,039 --> 00:18:53,840
um so we saw there's a whole bunch of

485
00:18:53,840 --> 00:18:54,880
stages

486
00:18:54,880 --> 00:18:58,240
and then the last stage was the the

487
00:18:58,240 --> 00:19:02,549
last upgrade was to collect stage

488
00:19:02,559 --> 00:19:06,720
so uh in this in this

489
00:19:06,720 --> 00:19:08,720
scenario that we just looked at the

490
00:19:08,720 --> 00:19:10,720
collect stage of course needs to collect

491
00:19:10,720 --> 00:19:14,000
data from uh all the partitions

492
00:19:14,000 --> 00:19:16,559
right so we in principle you know let's

493
00:19:16,559 --> 00:19:18,400
just draw a green line

494
00:19:18,400 --> 00:19:20,400
you know basically everything of this

495
00:19:20,400 --> 00:19:22,080
you know sort of executed on an

496
00:19:22,080 --> 00:19:24,000
independent partition

497
00:19:24,000 --> 00:19:25,919
so every worker you know gets one of

498
00:19:25,919 --> 00:19:27,360
these tasks from the

499
00:19:27,360 --> 00:19:30,160
scheduler runs you know the uh that

500
00:19:30,160 --> 00:19:32,320
thing and produces in in the end

501
00:19:32,320 --> 00:19:35,600
on uh time

502
00:19:35,600 --> 00:19:40,080
rdd and when the sky scanner you know

503
00:19:40,080 --> 00:19:43,760
determines that basically all the time

504
00:19:43,760 --> 00:19:45,360
you know like all these stages this is

505
00:19:45,360 --> 00:19:48,000
called the stage

506
00:19:48,000 --> 00:19:50,240
if all the stages have completed and so

507
00:19:50,240 --> 00:19:51,280
all the time

508
00:19:51,280 --> 00:19:53,760
partitions have been produced then it

509
00:19:53,760 --> 00:19:54,960
actually will run

510
00:19:54,960 --> 00:19:57,360
you know the collect action uh to

511
00:19:57,360 --> 00:19:58,880
basically do the addition

512
00:19:58,880 --> 00:20:01,440
and you know retrieve information from

513
00:20:01,440 --> 00:20:02,640
every partition

514
00:20:02,640 --> 00:20:05,360
uh to actually compute the collect or

515
00:20:05,360 --> 00:20:05,679
the

516
00:20:05,679 --> 00:20:07,919
actually this does not collect it was a

517
00:20:07,919 --> 00:20:10,630
count

518
00:20:10,640 --> 00:20:15,430
sorry about that

519
00:20:15,440 --> 00:20:17,760
and so uh one sort of thing way to think

520
00:20:17,760 --> 00:20:20,080
about this is that this is sort of like

521
00:20:20,080 --> 00:20:21,919
uh almost like a mapreduce right where

522
00:20:21,919 --> 00:20:23,520
you have the map phase and then

523
00:20:23,520 --> 00:20:25,679
you have sort of a shuffle and then you

524
00:20:25,679 --> 00:20:26,880
run the reduced phase

525
00:20:26,880 --> 00:20:30,960
and uh the count is almost or similar

526
00:20:30,960 --> 00:20:33,360
in that fashion and in in the paper the

527
00:20:33,360 --> 00:20:34,640
way they refer to this is

528
00:20:34,640 --> 00:20:37,039
to this dependency if although there's a

529
00:20:37,039 --> 00:20:39,280
white dependency

530
00:20:39,280 --> 00:20:40,960
because the action or the transformation

531
00:20:40,960 --> 00:20:44,559
is dependent on a number of partitions

532
00:20:44,559 --> 00:20:47,440
and these are called narrow narrow

533
00:20:47,440 --> 00:20:48,799
dependencies

534
00:20:48,799 --> 00:20:52,159
because this

535
00:20:52,159 --> 00:20:55,360
uh rdd to make that rdd is only

536
00:20:55,360 --> 00:20:56,480
dependent on one

537
00:20:56,480 --> 00:20:59,280
other uh only at the only dependent on

538
00:20:59,280 --> 00:21:00,799
the parent

539
00:21:00,799 --> 00:21:03,520
partition only one parent partition to

540
00:21:03,520 --> 00:21:05,679
actually be able to compute it

541
00:21:05,679 --> 00:21:08,000
and so in general you you would prefer a

542
00:21:08,000 --> 00:21:09,280
computation that can have narrow

543
00:21:09,280 --> 00:21:10,799
dependencies because they can just run

544
00:21:10,799 --> 00:21:12,799
locally without any communication

545
00:21:12,799 --> 00:21:14,799
uh but for a white dependency you might

546
00:21:14,799 --> 00:21:16,960
have to collect uh

547
00:21:16,960 --> 00:21:18,960
uh you might have to collect um

548
00:21:18,960 --> 00:21:20,960
petitions from the parent or you may

549
00:21:20,960 --> 00:21:21,520
have to collect

550
00:21:21,520 --> 00:21:24,480
petitions from the parent rdd from all

551
00:21:24,480 --> 00:21:25,760
the machines

552
00:21:25,760 --> 00:21:30,480
um professor i had i had a question so

553
00:21:30,480 --> 00:21:33,520
um in the paper it says

554
00:21:33,520 --> 00:21:36,159
narrow dependencies um where each

555
00:21:36,159 --> 00:21:37,280
partition

556
00:21:37,280 --> 00:21:41,360
of the parent rdd is used by at most one

557
00:21:41,360 --> 00:21:42,720
partition of the child

558
00:21:42,720 --> 00:21:46,000
rdd yep um but but it doesn't say

559
00:21:46,000 --> 00:21:47,600
anything about the

560
00:21:47,600 --> 00:21:51,200
the country like the river is like like

561
00:21:51,200 --> 00:21:54,159
it doesn't say like a child partition

562
00:21:54,159 --> 00:21:56,840
needs to use it most

563
00:21:56,840 --> 00:21:59,360
one um one

564
00:21:59,360 --> 00:22:01,760
parent um yeah that's correct because

565
00:22:01,760 --> 00:22:02,640
then

566
00:22:02,640 --> 00:22:04,400
and then yeah if if a child uses

567
00:22:04,400 --> 00:22:06,000
multiple parent partitions then it's a

568
00:22:06,000 --> 00:22:08,159
white dependency

569
00:22:08,159 --> 00:22:11,600
if apparent sorry if if

570
00:22:11,600 --> 00:22:15,039
if the child meets the uh predictions of

571
00:22:15,039 --> 00:22:17,039
if the petition for mult multiple parent

572
00:22:17,039 --> 00:22:18,080
petitions

573
00:22:18,080 --> 00:22:21,600
then it's a white dependency um

574
00:22:21,600 --> 00:22:23,360
so for example in the count case correct

575
00:22:23,360 --> 00:22:25,200
you know you have

576
00:22:25,200 --> 00:22:29,120
you know time time partitions

577
00:22:29,120 --> 00:22:32,880
right and the count operation

578
00:22:32,880 --> 00:22:34,559
is going to collect in the data from all

579
00:22:34,559 --> 00:22:36,559
of them right

580
00:22:36,559 --> 00:22:39,679
and so if count were in rdd it isn't but

581
00:22:39,679 --> 00:22:40,880
like it's just an action

582
00:22:40,880 --> 00:22:42,960
but even we're in rdd then basically you

583
00:22:42,960 --> 00:22:44,400
know that would require

584
00:22:44,400 --> 00:22:47,280
uh interaction with all the parents um

585
00:22:47,280 --> 00:22:48,720
what i'm saying is i think

586
00:22:48,720 --> 00:22:52,000
i think it says the opposite right

587
00:22:52,000 --> 00:22:55,520
i i think this might like

588
00:22:55,520 --> 00:22:58,880
i mean i i i i was actually confused

589
00:22:58,880 --> 00:23:02,159
uh like with the paper

590
00:23:02,159 --> 00:23:04,880
on this very specific issue but like it

591
00:23:04,880 --> 00:23:05,679
says

592
00:23:05,679 --> 00:23:08,799
like each partition of the parent rdd is

593
00:23:08,799 --> 00:23:10,799
used by at most one partition of the

594
00:23:10,799 --> 00:23:12,640
child

595
00:23:12,640 --> 00:23:15,200
but it doesn't say one partition of the

596
00:23:15,200 --> 00:23:17,200
child uses it most

597
00:23:17,200 --> 00:23:19,760
okay i'm not who accenture you know

598
00:23:19,760 --> 00:23:20,559
exactly

599
00:23:20,559 --> 00:23:22,799
why you're confused with this so let me

600
00:23:22,799 --> 00:23:25,679
can we explain this and come back to it

601
00:23:25,679 --> 00:23:28,240
sure i think the key thing to observe is

602
00:23:28,240 --> 00:23:29,919
basically two types of dependency white

603
00:23:29,919 --> 00:23:31,360
ones the narrow ones

604
00:23:31,360 --> 00:23:33,360
and white ones basically you know

605
00:23:33,360 --> 00:23:34,880
basically involve communication because

606
00:23:34,880 --> 00:23:37,360
they have to

607
00:23:37,360 --> 00:23:40,870
talk to the

608
00:23:40,880 --> 00:23:42,720
collecting the information from the data

609
00:23:42,720 --> 00:23:46,390
from the parent petitions

610
00:23:46,400 --> 00:23:50,880
okay thanks i actually have a question

611
00:23:50,880 --> 00:23:52,320
on the interface

612
00:23:52,320 --> 00:23:54,720
um in like the previous like one or two

613
00:23:54,720 --> 00:23:56,559
slides what happens if you don't call

614
00:23:56,559 --> 00:23:57,440
error stop

615
00:23:57,440 --> 00:24:00,640
persist ah uh

616
00:24:00,640 --> 00:24:04,000
if you do not then uh the

617
00:24:04,000 --> 00:24:06,799
you would the second computation like

618
00:24:06,799 --> 00:24:08,080
this computation

619
00:24:08,080 --> 00:24:11,360
would recompute errors from the

620
00:24:11,360 --> 00:24:12,400
beginning

621
00:24:12,400 --> 00:24:15,840
so if you run this workflow

622
00:24:15,840 --> 00:24:17,440
uh this spark you know computation it

623
00:24:17,440 --> 00:24:19,120
would recompute uh

624
00:24:19,120 --> 00:24:21,520
errors you know from uh the starting

625
00:24:21,520 --> 00:24:22,960
file

626
00:24:22,960 --> 00:24:26,720
got it thank you um so i actually have a

627
00:24:26,720 --> 00:24:27,679
question about

628
00:24:27,679 --> 00:24:30,400
this point so for the partitions that we

629
00:24:30,400 --> 00:24:30,880
don't

630
00:24:30,880 --> 00:24:33,919
call persist on in the mapreduce

631
00:24:33,919 --> 00:24:36,080
case we basically uh stored them in

632
00:24:36,080 --> 00:24:37,200
intermediate files

633
00:24:37,200 --> 00:24:40,640
but we nonetheless stored them uh in

634
00:24:40,640 --> 00:24:43,200
a local file system let's say in the

635
00:24:43,200 --> 00:24:44,400
case of mapreduce

636
00:24:44,400 --> 00:24:47,039
do we actually store uh intermediate

637
00:24:47,039 --> 00:24:48,960
files here that we don't persist in some

638
00:24:48,960 --> 00:24:50,400
persistent storage or we

639
00:24:50,400 --> 00:24:53,279
just keep the whole flow uh in memory

640
00:24:53,279 --> 00:24:54,400
throughout the time

641
00:24:54,400 --> 00:24:56,720
by default the whole flow is in memory

642
00:24:56,720 --> 00:24:57,440
except

643
00:24:57,440 --> 00:25:01,279
uh you can provide to uh let me

644
00:25:01,279 --> 00:25:02,640
there's one exception i will talk about

645
00:25:02,640 --> 00:25:04,320
a little bit more in detail in a second

646
00:25:04,320 --> 00:25:05,679
hopefully

647
00:25:05,679 --> 00:25:09,120
which is um

648
00:25:09,120 --> 00:25:11,760
you see this persist here this persists

649
00:25:11,760 --> 00:25:12,640
can take another

650
00:25:12,640 --> 00:25:15,840
flag i think it's called reliable and

651
00:25:15,840 --> 00:25:16,640
then

652
00:25:16,640 --> 00:25:19,760
that set is actually stored in hdfs

653
00:25:19,760 --> 00:25:22,799
basically they call this a checkpoint i

654
00:25:22,799 --> 00:25:26,789
see thank you

655
00:25:26,799 --> 00:25:28,080
i have a quick question about the

656
00:25:28,080 --> 00:25:31,440
partitioning that um

657
00:25:31,440 --> 00:25:34,640
with uh partitioning the rdds initially

658
00:25:34,640 --> 00:25:38,080
uh is it initially hdfs who

659
00:25:38,080 --> 00:25:40,559
partitions them for each worker to

660
00:25:40,559 --> 00:25:42,240
operate on or is

661
00:25:42,240 --> 00:25:45,120
is spark handling all that the the the

662
00:25:45,120 --> 00:25:45,760
the

663
00:25:45,760 --> 00:25:48,480
this aligns rdfs you know the petition

664
00:25:48,480 --> 00:25:50,000
is defined basically by

665
00:25:50,000 --> 00:25:53,279
uh the files that are actually in hdfs

666
00:25:53,279 --> 00:25:56,240
okay you can repartition and we'll see

667
00:25:56,240 --> 00:25:57,760
in a second that actually uh might be

668
00:25:57,760 --> 00:25:59,440
advantageous to do so

669
00:25:59,440 --> 00:26:01,120
uh for example using this hash partition

670
00:26:01,120 --> 00:26:02,720
trick uh

671
00:26:02,720 --> 00:26:05,840
and uh and you can define also your own

672
00:26:05,840 --> 00:26:07,360
partitioner there's a

673
00:26:07,360 --> 00:26:09,200
partition or object or extraction that

674
00:26:09,200 --> 00:26:11,440
you can supply

675
00:26:11,440 --> 00:26:14,159
okay so it's so it's already handled by

676
00:26:14,159 --> 00:26:16,320
by hdfs but if you want to do it again

677
00:26:16,320 --> 00:26:17,679
through spark then you can

678
00:26:17,679 --> 00:26:19,520
yeah and in some sense of course these

679
00:26:19,520 --> 00:26:21,039
files are also created by

680
00:26:21,039 --> 00:26:23,840
this this file is presumably you're

681
00:26:23,840 --> 00:26:24,640
created by

682
00:26:24,640 --> 00:26:26,080
some logging system that sits on the

683
00:26:26,080 --> 00:26:27,360
side and just produces different

684
00:26:27,360 --> 00:26:28,400
partitions

685
00:26:28,400 --> 00:26:29,919
or you know you can reshuffle if you

686
00:26:29,919 --> 00:26:35,669
want to make sense thank you

687
00:26:35,679 --> 00:26:39,039
okay uh

688
00:26:39,039 --> 00:26:42,559
so um and so let me uh so so that's the

689
00:26:42,559 --> 00:26:43,600
execution model

690
00:26:43,600 --> 00:26:45,360
uh and i want to talk a little bit about

691
00:26:45,360 --> 00:26:47,840
fault tolerance um

692
00:26:47,840 --> 00:26:50,159
and so let's uh go back to uh this so

693
00:26:50,159 --> 00:26:51,440
this is sort of fault tolerance and

694
00:26:51,440 --> 00:26:53,200
the thing that we worry about in photons

695
00:26:53,200 --> 00:26:55,440
is that maybe one of these workers

696
00:26:55,440 --> 00:26:58,720
uh you know might crash right

697
00:26:58,720 --> 00:27:01,120
and you know the worker is computing

698
00:27:01,120 --> 00:27:02,240
some petition

699
00:27:02,240 --> 00:27:05,600
and so we need to re-execute that uh

700
00:27:05,600 --> 00:27:07,919
and now basically this plan is sort of

701
00:27:07,919 --> 00:27:09,279
the same as a mapreduce

702
00:27:09,279 --> 00:27:13,520
right uh where if a worker crashes

703
00:27:13,520 --> 00:27:15,760
uh we need to uh in mapreduce and then

704
00:27:15,760 --> 00:27:17,679
the map test need to be re-executed and

705
00:27:17,679 --> 00:27:19,440
perhaps maybe a reduced task has to be

706
00:27:19,440 --> 00:27:20,799
re-executed

707
00:27:20,799 --> 00:27:22,320
now in here the tasks are slightly more

708
00:27:22,320 --> 00:27:24,080
complicated because they're basically

709
00:27:24,080 --> 00:27:25,360
like these stages

710
00:27:25,360 --> 00:27:28,399
and so it means that if one worker fails

711
00:27:28,399 --> 00:27:31,360
we may have to recompute the stage

712
00:27:31,360 --> 00:27:33,440
so let's talk a little bit more about

713
00:27:33,440 --> 00:27:35,279
that so that's sort of just perspective

714
00:27:35,279 --> 00:27:36,640
from fault tolerance right that's what

715
00:27:36,640 --> 00:27:39,600
we're trying to achieve

716
00:27:39,600 --> 00:27:41,039
this is really different than like the

717
00:27:41,039 --> 00:27:42,559
fall columns that were you know you

718
00:27:42,559 --> 00:27:43,600
implemented in lab

719
00:27:43,600 --> 00:27:45,919
and two and three were taxes and you

720
00:27:45,919 --> 00:27:47,520
know stable storage and all that kind of

721
00:27:47,520 --> 00:27:48,720
stuff

722
00:27:48,720 --> 00:27:51,039
and you're really what we're we're

723
00:27:51,039 --> 00:27:51,760
worried about

724
00:27:51,760 --> 00:27:56,070
is a crash of a worker

725
00:27:56,080 --> 00:28:01,190
the worker loses its memory

726
00:28:01,200 --> 00:28:04,799
lost memory and the means of losses

727
00:28:04,799 --> 00:28:10,630
partition

728
00:28:10,640 --> 00:28:13,440
and uh you know the later parts of the

729
00:28:13,440 --> 00:28:15,120
computation are probably dependent on

730
00:28:15,120 --> 00:28:16,000
that partition

731
00:28:16,000 --> 00:28:19,039
and so we need to re-uh

732
00:28:19,039 --> 00:28:21,360
re-read or re-compute this partition and

733
00:28:21,360 --> 00:28:23,200
so the solution is like exactly like in

734
00:28:23,200 --> 00:28:24,159
mapreduce

735
00:28:24,159 --> 00:28:26,320
you know basically the scanner you

736
00:28:26,320 --> 00:28:27,760
notice at some point that doesn't get an

737
00:28:27,760 --> 00:28:29,279
answer

738
00:28:29,279 --> 00:28:32,399
and then re-runs the stage

739
00:28:32,399 --> 00:28:44,159
for the petition

740
00:28:44,159 --> 00:28:46,080
and you know and what is the cool part

741
00:28:46,080 --> 00:28:48,399
like exactly as in mapreduce

742
00:28:48,399 --> 00:28:49,440
all if you look at all these

743
00:28:49,440 --> 00:28:50,960
transformations that are sitting here in

744
00:28:50,960 --> 00:28:51,919
the api

745
00:28:51,919 --> 00:28:57,029
all these transformations are functional

746
00:28:57,039 --> 00:28:59,440
and so they basically take one input

747
00:28:59,440 --> 00:29:01,039
they take an rdd as an input they

748
00:29:01,039 --> 00:29:03,120
produce another rdd as output

749
00:29:03,120 --> 00:29:04,480
uh and it's just completely

750
00:29:04,480 --> 00:29:06,640
deterministic and so

751
00:29:06,640 --> 00:29:08,640
like you know with mapreduce you know

752
00:29:08,640 --> 00:29:10,480
these uh the maps and the reducer

753
00:29:10,480 --> 00:29:13,919
functional operations if you restart a

754
00:29:13,919 --> 00:29:16,480
stage or a sequence of transformations

755
00:29:16,480 --> 00:29:19,200
from the same input then you'll produce

756
00:29:19,200 --> 00:29:20,159
the same output

757
00:29:20,159 --> 00:29:24,840
and so you recreate you know the same

758
00:29:24,840 --> 00:29:28,549
partition

759
00:29:28,559 --> 00:29:33,909
recreates petition facebook

760
00:29:33,919 --> 00:29:37,750
okay

761
00:29:37,760 --> 00:29:40,320
sorry is this why they they're immutable

762
00:29:40,320 --> 00:29:41,440
there's also the reason i think they're

763
00:29:41,440 --> 00:29:48,070
ideally immutable yes

764
00:29:48,080 --> 00:29:50,880
okay there's one tricky case though uh

765
00:29:50,880 --> 00:29:52,399
which i want to talk about

766
00:29:52,399 --> 00:29:55,200
so this is the the the fault tolerance

767
00:29:55,200 --> 00:29:57,840
basically for the narrow case

768
00:29:57,840 --> 00:30:00,080
is the same as sort of as we saw before

769
00:30:00,080 --> 00:30:02,240
in mapreduce but you know the tricky

770
00:30:02,240 --> 00:30:04,080
case is actually the

771
00:30:04,080 --> 00:30:12,630
white dependencies

772
00:30:12,640 --> 00:30:15,520
so let's say you know we have you know

773
00:30:15,520 --> 00:30:19,830
some transformations

774
00:30:19,840 --> 00:30:22,880
and one of these transformations

775
00:30:22,880 --> 00:30:25,600
uh is dependent on a you know this is

776
00:30:25,600 --> 00:30:27,360
like sort of here we have one worker we

777
00:30:27,360 --> 00:30:28,399
have another worker we have another

778
00:30:28,399 --> 00:30:31,110
worker

779
00:30:31,120 --> 00:30:34,720
and one of these stages um

780
00:30:34,720 --> 00:30:39,190
is actually dependent on

781
00:30:39,200 --> 00:30:42,880
a number of uh parent uh partitions

782
00:30:42,880 --> 00:30:45,039
so uh let's say whatever maybe this is a

783
00:30:45,039 --> 00:30:48,080
join or we'll see later other operations

784
00:30:48,080 --> 00:30:49,840
uh we're you know we are actually

785
00:30:49,840 --> 00:30:51,039
collecting information for lots of

786
00:30:51,039 --> 00:30:52,159
partitions

787
00:30:52,159 --> 00:30:55,840
and uh and you know create a new rdd

788
00:30:55,840 --> 00:30:57,519
from that and from that rbd that might

789
00:30:57,519 --> 00:30:59,039
be used again by a map

790
00:30:59,039 --> 00:31:01,919
or whatever keep it going and so now

791
00:31:01,919 --> 00:31:02,799
let's say

792
00:31:02,799 --> 00:31:04,640
you know we're you know we're a worker

793
00:31:04,640 --> 00:31:07,519
and you know we crash here

794
00:31:07,519 --> 00:31:09,360
right then we need to reconstruct you

795
00:31:09,360 --> 00:31:15,669
know this rdd

796
00:31:15,679 --> 00:31:17,840
and you know we sort of followed uh that

797
00:31:17,840 --> 00:31:19,440
means that we have to you know we could

798
00:31:19,440 --> 00:31:21,440
also recompute this rdd

799
00:31:21,440 --> 00:31:23,919
to review this rdd on this worker that

800
00:31:23,919 --> 00:31:25,120
means we also need

801
00:31:25,120 --> 00:31:28,399
the partitions on the other workers and

802
00:31:28,399 --> 00:31:31,760
uh so uh the

803
00:31:31,760 --> 00:31:34,240
uh the reconstruction the re-execution

804
00:31:34,240 --> 00:31:34,799
of

805
00:31:34,799 --> 00:31:38,159
uh a computation on a particular

806
00:31:38,159 --> 00:31:40,320
uh worker or a particular partition may

807
00:31:40,320 --> 00:31:41,360
actually result

808
00:31:41,360 --> 00:31:44,840
that actually these ones also need to be

809
00:31:44,840 --> 00:31:47,120
recomputed

810
00:31:47,120 --> 00:31:48,480
now of course you can do this partially

811
00:31:48,480 --> 00:31:50,640
in uh parallel you can just ask you know

812
00:31:50,640 --> 00:31:51,440
please you know start

813
00:31:51,440 --> 00:31:52,720
you know re-compute this guy we could

814
00:31:52,720 --> 00:31:54,720
see that guy uh and

815
00:31:54,720 --> 00:31:57,200
you know produce then the final uh rdd

816
00:31:57,200 --> 00:31:57,919
again

817
00:31:57,919 --> 00:32:01,039
uh but you know certainly you know an uh

818
00:32:01,039 --> 00:32:03,760
failure one worker might result in the

819
00:32:03,760 --> 00:32:04,960
recompensation

820
00:32:04,960 --> 00:32:08,799
of many many uh petitions

821
00:32:08,799 --> 00:32:12,000
and that's sort of slightly uh uh

822
00:32:12,000 --> 00:32:14,960
that could be slightly costly and so the

823
00:32:14,960 --> 00:32:16,320
solution is

824
00:32:16,320 --> 00:32:19,279
that as a programmer you can say you can

825
00:32:19,279 --> 00:32:20,000
actually

826
00:32:20,000 --> 00:32:23,679
checkpoint or persist

827
00:32:23,679 --> 00:32:27,440
rdds on stable storage

828
00:32:27,440 --> 00:32:29,120
and so you might decide you know for

829
00:32:29,120 --> 00:32:31,840
example this is an rdd that

830
00:32:31,840 --> 00:32:34,960
uh that you don't want to recompute in

831
00:32:34,960 --> 00:32:36,320
the case of a failure

832
00:32:36,320 --> 00:32:38,000
because it requires you know recomputing

833
00:32:38,000 --> 00:32:39,360
all the different partitions

834
00:32:39,360 --> 00:32:41,360
you know you may want to checkpoint this

835
00:32:41,360 --> 00:32:48,870
rdd

836
00:32:48,880 --> 00:32:50,480
and then you know this you know stage

837
00:32:50,480 --> 00:32:52,399
when it actually when this

838
00:32:52,399 --> 00:32:54,640
computation needs to be re-executed now

839
00:32:54,640 --> 00:32:55,519
we can actually read

840
00:32:55,519 --> 00:32:57,760
you know the results of the partitions

841
00:32:57,760 --> 00:32:58,880
from the

842
00:32:58,880 --> 00:33:01,120
checkpoint instead of actually having to

843
00:33:01,120 --> 00:33:02,399
recompute them from

844
00:33:02,399 --> 00:33:05,519
scratch and so this is why uh spark

845
00:33:05,519 --> 00:33:06,080
supports

846
00:33:06,080 --> 00:33:10,000
checkpoints and that is sort of their uh

847
00:33:10,000 --> 00:33:12,720
uh fall down the story for white

848
00:33:12,720 --> 00:33:17,669
dependencies

849
00:33:17,679 --> 00:33:20,950
any questions about this

850
00:33:20,960 --> 00:33:25,279
um i i had one question

851
00:33:25,279 --> 00:33:28,480
so there was um

852
00:33:28,480 --> 00:33:30,880
so you can persist right just in in

853
00:33:30,880 --> 00:33:31,519
general

854
00:33:31,519 --> 00:33:33,919
but the they also mentioned the reliable

855
00:33:33,919 --> 00:33:35,440
flag

856
00:33:35,440 --> 00:33:37,919
um so i was wondering like what's the

857
00:33:37,919 --> 00:33:39,600
difference between just persisting and

858
00:33:39,600 --> 00:33:41,679
using a reliable flag

859
00:33:41,679 --> 00:33:43,120
persist just means you're going to keep

860
00:33:43,120 --> 00:33:45,200
that rdd in memory and you're not going

861
00:33:45,200 --> 00:33:47,440
to throw it away

862
00:33:47,440 --> 00:33:48,799
so that you can reuse it in later

863
00:33:48,799 --> 00:33:51,760
computations in and this is in memory

864
00:33:51,760 --> 00:33:54,000
the checkpoint or the liability flag

865
00:33:54,000 --> 00:33:56,080
basically means

866
00:33:56,080 --> 00:33:58,480
you actually write a copy of the whole

867
00:33:58,480 --> 00:33:59,440
rdd

868
00:33:59,440 --> 00:34:05,430
to hdfs

869
00:34:05,440 --> 00:34:07,440
and ac has a persistent or a stable

870
00:34:07,440 --> 00:34:11,349
storage file system

871
00:34:11,359 --> 00:34:14,320
okay is there a way to tell spark to

872
00:34:14,320 --> 00:34:16,000
unpersist something

873
00:34:16,000 --> 00:34:17,919
because otherwise like for example if

874
00:34:17,919 --> 00:34:20,720
you persist an rdd um and you do a lot

875
00:34:20,720 --> 00:34:22,240
of computations

876
00:34:22,240 --> 00:34:23,919
but like those later computations never

877
00:34:23,919 --> 00:34:26,079
use the rdd

878
00:34:26,079 --> 00:34:27,440
you might just have it sticking around

879
00:34:27,440 --> 00:34:29,760
in memory forever

880
00:34:29,760 --> 00:34:33,119
yeah so i i i presume you can

881
00:34:33,119 --> 00:34:36,320
uh there's a general strategy that

882
00:34:36,320 --> 00:34:37,839
spark uses so they talk a little bit

883
00:34:37,839 --> 00:34:39,839
about this is that they

884
00:34:39,839 --> 00:34:42,399
if there's really no space anymore they

885
00:34:42,399 --> 00:34:44,240
might

886
00:34:44,240 --> 00:34:48,320
spill some rdds to hdfs or remove them

887
00:34:48,320 --> 00:34:50,000
the paper slightly vague on exactly you

888
00:34:50,000 --> 00:34:53,829
know what their plan for that is

889
00:34:53,839 --> 00:34:56,159
thank you of course when the competition

890
00:34:56,159 --> 00:34:57,359
ends and

891
00:34:57,359 --> 00:35:01,040
uh as a user you log out

892
00:35:01,040 --> 00:35:03,839
where you stop your driver then i think

893
00:35:03,839 --> 00:35:05,200
you know those rdds are definitely gone

894
00:35:05,200 --> 00:35:11,030
from memory

895
00:35:11,040 --> 00:35:15,040
okay okay so

896
00:35:15,040 --> 00:35:17,280
that is almost you know the story of

897
00:35:17,280 --> 00:35:19,200
spark um

898
00:35:19,200 --> 00:35:22,560
uh the you know we've seen what an rdd

899
00:35:22,560 --> 00:35:22,960
is

900
00:35:22,960 --> 00:35:25,520
uh we've seen how uh so the execution

901
00:35:25,520 --> 00:35:26,320
works

902
00:35:26,320 --> 00:35:28,400
and we've seen how the paul thomas plan

903
00:35:28,400 --> 00:35:29,760
works

904
00:35:29,760 --> 00:35:31,200
the thing that i want to really want to

905
00:35:31,200 --> 00:35:32,880
talk about is another example

906
00:35:32,880 --> 00:35:35,920
to really show off where park sparks

907
00:35:35,920 --> 00:35:40,079
shines and that is an iterative example

908
00:35:40,079 --> 00:35:42,839
so in computation that has an iterative

909
00:35:42,839 --> 00:35:45,990
structure

910
00:35:46,000 --> 00:35:47,440
and the particular one i want to talk

911
00:35:47,440 --> 00:35:52,790
about is page rank

912
00:35:52,800 --> 00:35:55,599
uh i assume that most of you are

913
00:35:55,599 --> 00:35:57,040
familiar with pagerank in

914
00:35:57,040 --> 00:35:59,359
some form uh basically you know it's a

915
00:35:59,359 --> 00:36:00,560
plan to

916
00:36:00,560 --> 00:36:03,359
uh an algorithm to give weight or

917
00:36:03,359 --> 00:36:04,160
importance

918
00:36:04,160 --> 00:36:07,200
to web pages and it's dependent on the

919
00:36:07,200 --> 00:36:08,160
number of uh

920
00:36:08,160 --> 00:36:10,079
links that point to a particular webpage

921
00:36:10,079 --> 00:36:11,760
so for example if you have a you know

922
00:36:11,760 --> 00:36:13,920
webpage u1

923
00:36:13,920 --> 00:36:16,880
uh main points to itself uh you have

924
00:36:16,880 --> 00:36:17,520
maybe a

925
00:36:17,520 --> 00:36:20,160
web page you free so i'll use an example

926
00:36:20,160 --> 00:36:21,920
and my page youtube

927
00:36:21,920 --> 00:36:24,000
you know you too has a link you know to

928
00:36:24,000 --> 00:36:25,119
itself

929
00:36:25,119 --> 00:36:27,920
and to you free and maybe you know your

930
00:36:27,920 --> 00:36:30,240
free has a link to u1

931
00:36:30,240 --> 00:36:32,640
and basically page rank is an algorithm

932
00:36:32,640 --> 00:36:34,000
that you know based on these

933
00:36:34,000 --> 00:36:36,720
connectivities computes you know the

934
00:36:36,720 --> 00:36:38,960
importance of a web page

935
00:36:38,960 --> 00:36:42,160
and patreon was sort of the early one of

936
00:36:42,160 --> 00:36:42,880
the

937
00:36:42,880 --> 00:36:45,359
algorithms that really drove the google

938
00:36:45,359 --> 00:36:46,240
search machine

939
00:36:46,240 --> 00:36:49,359
uh in the sense that if you had a search

940
00:36:49,359 --> 00:36:51,040
result the way you rank search results

941
00:36:51,040 --> 00:36:52,800
is that if the search result

942
00:36:52,800 --> 00:36:55,119
appears on a more important web page you

943
00:36:55,119 --> 00:36:56,480
know that results you know it gets

944
00:36:56,480 --> 00:36:58,560
promoted to higher in the list

945
00:36:58,560 --> 00:37:00,960
uh and uh number one the reason that in

946
00:37:00,960 --> 00:37:02,640
the early days of google the google

947
00:37:02,640 --> 00:37:04,320
search machine actually produced

948
00:37:04,320 --> 00:37:06,000
a better search results where i mean the

949
00:37:06,000 --> 00:37:07,520
more important information actually or

950
00:37:07,520 --> 00:37:08,720
the more important web pages where on

951
00:37:08,720 --> 00:37:11,990
top

952
00:37:12,000 --> 00:37:15,359
and so the paper uh talks about

953
00:37:15,359 --> 00:37:19,119
uh shows off the uh

954
00:37:19,119 --> 00:37:26,630
an implementation of page rank in spark

955
00:37:26,640 --> 00:37:31,109
so here's the implementation of

956
00:37:31,119 --> 00:37:34,960
the spark implementation of page rank

957
00:37:34,960 --> 00:37:37,760
and as before you know this is just a

958
00:37:37,760 --> 00:37:39,440
description so we look at the individual

959
00:37:39,440 --> 00:37:40,320
lines

960
00:37:40,320 --> 00:37:43,359
um you know these are just the sort of

961
00:37:43,359 --> 00:37:45,760
the recipe to actually how to

962
00:37:45,760 --> 00:37:49,040
uh compute page rank and only when like

963
00:37:49,040 --> 00:37:50,320
you know in this particular case if you

964
00:37:50,320 --> 00:37:52,160
do say ranks collect

965
00:37:52,160 --> 00:37:54,560
at the very end uh then actually the

966
00:37:54,560 --> 00:37:56,240
computation would run

967
00:37:56,240 --> 00:37:59,280
uh on you know the cluster of machines

968
00:37:59,280 --> 00:38:01,280
and you know using sort of the execution

969
00:38:01,280 --> 00:38:04,240
uh pattern that we have seen so far

970
00:38:04,240 --> 00:38:07,200
and so i want to walk through uh this uh

971
00:38:07,200 --> 00:38:09,200
example a little bit more detail

972
00:38:09,200 --> 00:38:11,359
to get a sense you know what uh you know

973
00:38:11,359 --> 00:38:13,200
get a better sense why sparks or

974
00:38:13,200 --> 00:38:16,240
shines in the iterative case

975
00:38:16,240 --> 00:38:19,839
so uh so there are two rdds here

976
00:38:19,839 --> 00:38:21,040
uh i'm also going to talk about some

977
00:38:21,040 --> 00:38:23,280
optimizations that are cool in uh spark

978
00:38:23,280 --> 00:38:26,240
one is this linx rpgs and lynx is

979
00:38:26,240 --> 00:38:28,160
basically you know represents the

980
00:38:28,160 --> 00:38:30,720
uh the connection of the graphs and so

981
00:38:30,720 --> 00:38:31,359
precisely

982
00:38:31,359 --> 00:38:34,640
it probably has a line uh i'm just going

983
00:38:34,640 --> 00:38:36,320
to write it like that has a line

984
00:38:36,320 --> 00:38:39,680
uh per uh url so here you one

985
00:38:39,680 --> 00:38:43,839
and it has two outgoing links uh u1

986
00:38:43,839 --> 00:38:48,550
u3 yeah

987
00:38:48,560 --> 00:38:51,589
uh

988
00:38:51,599 --> 00:38:54,960
actually i missed one link here and then

989
00:38:54,960 --> 00:38:58,079
uh there's a u2

990
00:38:58,079 --> 00:39:00,000
entry for youtube which has now gone

991
00:39:00,000 --> 00:39:04,550
linked to u2 and u3

992
00:39:04,560 --> 00:39:08,160
and there is an entry you know q3

993
00:39:08,160 --> 00:39:12,790
uh going to u1

994
00:39:12,800 --> 00:39:14,480
so this is basically a description of

995
00:39:14,480 --> 00:39:16,960
you will the you know the world wide web

996
00:39:16,960 --> 00:39:18,880
and so of course you know in our my tiny

997
00:39:18,880 --> 00:39:21,040
little example i have free web pages

998
00:39:21,040 --> 00:39:23,040
uh but if you know we're running this at

999
00:39:23,040 --> 00:39:24,480
the scale of you know google you would

1000
00:39:24,480 --> 00:39:26,320
have a billion web pages correct

1001
00:39:26,320 --> 00:39:28,880
and so this file is gigantic and it's

1002
00:39:28,880 --> 00:39:29,920
partitioned

1003
00:39:29,920 --> 00:39:33,359
uh in predictions

1004
00:39:33,359 --> 00:39:37,040
uh so that's links uh and then ranks

1005
00:39:37,040 --> 00:39:40,079
uh is a sort of similar file uh that

1006
00:39:40,079 --> 00:39:41,920
contains the current ranks for these web

1007
00:39:41,920 --> 00:39:42,800
pages

1008
00:39:42,800 --> 00:39:44,880
and so you can think about this as a new

1009
00:39:44,880 --> 00:39:46,000
one

1010
00:39:46,000 --> 00:39:48,800
you know comma you know it's rank uh and

1011
00:39:48,800 --> 00:39:50,160
let's assume that the ranks are

1012
00:39:50,160 --> 00:39:52,320
initialized at 1.0

1013
00:39:52,320 --> 00:39:55,440
uh then you know here's 1.0

1014
00:39:55,440 --> 00:40:00,400
and you know u 2 1.0

1015
00:40:00,400 --> 00:40:04,880
u u3 1.0

1016
00:40:04,880 --> 00:40:07,920
uh and we see actually that the links

1017
00:40:07,920 --> 00:40:10,079
that the links rdds is actually

1018
00:40:10,079 --> 00:40:11,200
persistent in memory

1019
00:40:11,200 --> 00:40:12,640
uh that's presumably like in the same

1020
00:40:12,640 --> 00:40:14,560
way as the error file that we saw before

1021
00:40:14,560 --> 00:40:15,440
or the error

1022
00:40:15,440 --> 00:40:18,640
rdd um and then

1023
00:40:18,640 --> 00:40:21,680
uh ranks uh you know it's initialized

1024
00:40:21,680 --> 00:40:23,119
you know to something and then basically

1025
00:40:23,119 --> 00:40:24,240
there's this sort of

1026
00:40:24,240 --> 00:40:28,560
uh description of number of iterations

1027
00:40:28,560 --> 00:40:31,920
uh to produce a new ranks uh

1028
00:40:31,920 --> 00:40:34,720
rdd and then you can see a little bit

1029
00:40:34,720 --> 00:40:35,839
you know how this actually

1030
00:40:35,839 --> 00:40:38,560
plays out uh and one of the things to

1031
00:40:38,560 --> 00:40:39,920
notice is that

1032
00:40:39,920 --> 00:40:43,119
links gets reused in every iteration

1033
00:40:43,119 --> 00:40:46,240
and length actually gets joined with

1034
00:40:46,240 --> 00:40:48,720
ranks what means what does it mean to

1035
00:40:48,720 --> 00:40:50,880
actually do this join up this

1036
00:40:50,880 --> 00:40:53,280
this operation this operation basically

1037
00:40:53,280 --> 00:40:54,880
creates an rdd

1038
00:40:54,880 --> 00:40:56,560
right and what how does that rdd look

1039
00:40:56,560 --> 00:40:58,480
like well that rdv is going to look like

1040
00:40:58,480 --> 00:41:01,599
u1 you know the and then the join of

1041
00:41:01,599 --> 00:41:05,599
the ranks and the and the links file so

1042
00:41:05,599 --> 00:41:08,800
it's going to be u1 u1 u2 because

1043
00:41:08,800 --> 00:41:09,599
there's a d or

1044
00:41:09,599 --> 00:41:12,880
u3 the outgoing links plus the rank for

1045
00:41:12,880 --> 00:41:16,560
u1 which i'm just going to write as rank

1046
00:41:16,560 --> 00:41:19,040
one and that's sort of the idd that's

1047
00:41:19,040 --> 00:41:20,400
being produced here and so

1048
00:41:20,400 --> 00:41:22,319
same thing for you know whatever you do

1049
00:41:22,319 --> 00:41:24,560
and the one for you free

1050
00:41:24,560 --> 00:41:27,920
is whatever you want and

1051
00:41:27,920 --> 00:41:32,400
you know uh are free rank for free

1052
00:41:32,400 --> 00:41:34,960
basically merges these two it literally

1053
00:41:34,960 --> 00:41:36,160
joins the two files

1054
00:41:36,160 --> 00:41:39,760
based on key okay

1055
00:41:39,760 --> 00:41:42,160
and then you know it runs a computation

1056
00:41:42,160 --> 00:41:44,079
of flatmap on this and that flatmap

1057
00:41:44,079 --> 00:41:46,000
itself internally has

1058
00:41:46,000 --> 00:41:48,240
a map over links and so basically it's

1059
00:41:48,240 --> 00:41:49,200
going to run

1060
00:41:49,200 --> 00:41:51,359
over like the screen it's going to run

1061
00:41:51,359 --> 00:41:52,800
over these lists

1062
00:41:52,800 --> 00:41:56,319
and basically partition or divide up

1063
00:41:56,319 --> 00:41:59,920
the rank you know to the outgoing urls

1064
00:41:59,920 --> 00:42:02,160
and so it will you know create uh

1065
00:42:02,160 --> 00:42:03,280
triples

1066
00:42:03,280 --> 00:42:06,640
uh of the form uh you know let me write

1067
00:42:06,640 --> 00:42:09,119
this in green then you know u1

1068
00:42:09,119 --> 00:42:15,109
uh r1 divided by two

1069
00:42:15,119 --> 00:42:18,880
and you know u1 or dual three

1070
00:42:18,880 --> 00:42:20,720
over the outgoing link so it gave one to

1071
00:42:20,720 --> 00:42:23,760
u1 one to u3 here's u3

1072
00:42:23,760 --> 00:42:27,440
r1 divided over two et cetera so it

1073
00:42:27,440 --> 00:42:28,960
creates triples of this kind of form so

1074
00:42:28,960 --> 00:42:30,240
basically you know it computes the

1075
00:42:30,240 --> 00:42:32,240
weight you know it divides the rank

1076
00:42:32,240 --> 00:42:33,920
across the outgoing edges

1077
00:42:33,920 --> 00:42:36,640
and gives you know the values of these

1078
00:42:36,640 --> 00:42:37,760
uh

1079
00:42:37,760 --> 00:42:40,000
rings you know to the outgoing edges so

1080
00:42:40,000 --> 00:42:41,599
the outgoing edges so we're going to get

1081
00:42:41,599 --> 00:42:43,200
a big you know rdd

1082
00:42:43,200 --> 00:42:46,480
that has you know disc four

1083
00:42:46,480 --> 00:42:48,560
and that's produced basically that is

1084
00:42:48,560 --> 00:42:50,240
the contributions

1085
00:42:50,240 --> 00:42:54,640
rdd then there's a final step

1086
00:42:54,640 --> 00:42:56,800
in the uh i think at first it doesn't

1087
00:42:56,800 --> 00:42:58,079
reduce by key

1088
00:42:58,079 --> 00:43:00,319
so basically grabs all the u1's you know

1089
00:43:00,319 --> 00:43:01,119
together

1090
00:43:01,119 --> 00:43:02,800
and then sums them up so basically this

1091
00:43:02,800 --> 00:43:05,119
will result is that you know all the

1092
00:43:05,119 --> 00:43:08,720
uh weights or the the fractional weights

1093
00:43:08,720 --> 00:43:09,440
you know that

1094
00:43:09,440 --> 00:43:10,960
you want is going to receive are being

1095
00:43:10,960 --> 00:43:12,640
added up so

1096
00:43:12,640 --> 00:43:15,359
u1 of course is going to receive weights

1097
00:43:15,359 --> 00:43:16,000
from

1098
00:43:16,000 --> 00:43:18,880
itself this one is going to relate from

1099
00:43:18,880 --> 00:43:20,240
u3

1100
00:43:20,240 --> 00:43:22,240
and so we'll sum them up it's going to

1101
00:43:22,240 --> 00:43:24,319
be r1

1102
00:43:24,319 --> 00:43:29,359
divided by two and r3 divided by one

1103
00:43:29,359 --> 00:43:31,760
and that is basically the sum that's

1104
00:43:31,760 --> 00:43:33,359
being computed

1105
00:43:33,359 --> 00:43:35,599
and so that gives us a list of sums and

1106
00:43:35,599 --> 00:43:37,440
then actually they are added up

1107
00:43:37,440 --> 00:43:42,790
and computed into a final value

1108
00:43:42,800 --> 00:43:45,280
and that produces the new ranks rdd

1109
00:43:45,280 --> 00:43:46,880
which has the same shape

1110
00:43:46,880 --> 00:43:49,520
as the one that we saw before namely

1111
00:43:49,520 --> 00:43:51,040
this shape

1112
00:43:51,040 --> 00:43:57,430
for every web page there is a number

1113
00:43:57,440 --> 00:44:00,710
does that make sense

1114
00:44:00,720 --> 00:44:03,920
and so uh it's interesting to you know

1115
00:44:03,920 --> 00:44:05,680
that's sort of the description so first

1116
00:44:05,680 --> 00:44:07,040
of all you can see that

1117
00:44:07,040 --> 00:44:09,040
actually the description of patreon is

1118
00:44:09,040 --> 00:44:10,319
quite precise

1119
00:44:10,319 --> 00:44:12,160
and this is one of the examples this is

1120
00:44:12,160 --> 00:44:14,400
like if you had to run this in a

1121
00:44:14,400 --> 00:44:15,680
mapreduce style then

1122
00:44:15,680 --> 00:44:17,520
that would mean that every iteration you

1123
00:44:17,520 --> 00:44:19,200
know of this loop

1124
00:44:19,200 --> 00:44:20,800
at the end of the iteration you would

1125
00:44:20,800 --> 00:44:22,960
store the results in the file system

1126
00:44:22,960 --> 00:44:26,240
and then you re-read you know the

1127
00:44:26,240 --> 00:44:28,160
iteration from the result for the next

1128
00:44:28,160 --> 00:44:29,520
iteration and

1129
00:44:29,520 --> 00:44:32,640
in uh in this

1130
00:44:32,640 --> 00:44:34,880
spark system you just every iteration

1131
00:44:34,880 --> 00:44:36,160
just runs straight out of memory it

1132
00:44:36,160 --> 00:44:36,480
leaves

1133
00:44:36,480 --> 00:44:37,839
results in memory so that the next

1134
00:44:37,839 --> 00:44:39,839
iteration can pick it up right there

1135
00:44:39,839 --> 00:44:42,960
and furthermore you know this links file

1136
00:44:42,960 --> 00:44:46,319
is shared among all the iterations

1137
00:44:46,319 --> 00:44:48,000
okay so to get a little bit more sense

1138
00:44:48,000 --> 00:44:50,560
of like you know why is also very cool

1139
00:44:50,560 --> 00:44:53,760
uh the way to look at it is to actually

1140
00:44:53,760 --> 00:44:54,480
look at the

1141
00:44:54,480 --> 00:44:56,960
lineage graph for this particular

1142
00:44:56,960 --> 00:44:59,670
computation

1143
00:44:59,680 --> 00:45:02,839
so let's look at the lineage graph for

1144
00:45:02,839 --> 00:45:06,390
uh

1145
00:45:06,400 --> 00:45:09,520
so here's the manage graph for

1146
00:45:09,520 --> 00:45:12,960
um the the

1147
00:45:12,960 --> 00:45:16,240
um for patrick

1148
00:45:16,240 --> 00:45:18,400
and so the couple things i want to uh

1149
00:45:18,400 --> 00:45:20,079
point out but first of all so the

1150
00:45:20,079 --> 00:45:22,400
lineage graph is like dynamic almost

1151
00:45:22,400 --> 00:45:26,870
looks uh

1152
00:45:26,880 --> 00:45:29,040
you know just keeps growing you know

1153
00:45:29,040 --> 00:45:30,960
with the number iterations

1154
00:45:30,960 --> 00:45:33,359
uh and so as the scheduler you know

1155
00:45:33,359 --> 00:45:35,119
basically computes and does new

1156
00:45:35,119 --> 00:45:38,880
uh stages then uh it uh keeps going and

1157
00:45:38,880 --> 00:45:40,240
you know we can see what the stages are

1158
00:45:40,240 --> 00:45:42,160
going to be correct because uh

1159
00:45:42,160 --> 00:45:44,839
you know sort of you know every

1160
00:45:44,839 --> 00:45:46,319
iteration uh

1161
00:45:46,319 --> 00:45:49,280
of a loop uh is uh one stage and will

1162
00:45:49,280 --> 00:45:50,800
basically append you know

1163
00:45:50,800 --> 00:45:54,800
uh uh parts uh transformations to the

1164
00:45:54,800 --> 00:45:57,040
to the lineage graph so we see here's

1165
00:45:57,040 --> 00:45:58,319
the input file

1166
00:45:58,319 --> 00:46:01,359
uh here's the links and as we saw like

1167
00:46:01,359 --> 00:46:02,880
links is actually created once

1168
00:46:02,880 --> 00:46:04,960
then persisted in memory right now not

1169
00:46:04,960 --> 00:46:06,480
on this persistent memory

1170
00:46:06,480 --> 00:46:08,319
and it's being reused many many times

1171
00:46:08,319 --> 00:46:10,480
like in every loop iteration

1172
00:46:10,480 --> 00:46:12,800
basically ranks is being reused and so

1173
00:46:12,800 --> 00:46:14,640
again you know compared to a mapreduce

1174
00:46:14,640 --> 00:46:15,599
if you have to write this in the

1175
00:46:15,599 --> 00:46:17,839
mapreduce style you don't get that reuse

1176
00:46:17,839 --> 00:46:19,280
and so that's of course going to like

1177
00:46:19,280 --> 00:46:21,599
tremendously perform performance because

1178
00:46:21,599 --> 00:46:23,599
links you know as we said before it's

1179
00:46:23,599 --> 00:46:25,920
like this gigantic file that basically

1180
00:46:25,920 --> 00:46:26,880
corresponds to

1181
00:46:26,880 --> 00:46:29,599
one line for every webpage in the

1182
00:46:29,599 --> 00:46:32,309
universe

1183
00:46:32,319 --> 00:46:33,920
another interesting thing to observe

1184
00:46:33,920 --> 00:46:35,760
here is that

1185
00:46:35,760 --> 00:46:37,440
we see a white dependencies here right

1186
00:46:37,440 --> 00:46:41,349
this is a white dependency

1187
00:46:41,359 --> 00:46:43,440
who could be a white dependency i will

1188
00:46:43,440 --> 00:46:44,720
be a little bit more sophisticated about

1189
00:46:44,720 --> 00:46:46,240
this in a second

1190
00:46:46,240 --> 00:46:48,560
um because basically these contributions

1191
00:46:48,560 --> 00:46:50,160
you know when we contribute

1192
00:46:50,160 --> 00:46:53,440
the intermediate result it is a join

1193
00:46:53,440 --> 00:46:54,000
across

1194
00:46:54,000 --> 00:46:57,760
ranks and links and so it needs the

1195
00:46:57,760 --> 00:47:00,960
partitions uh from links

1196
00:47:00,960 --> 00:47:03,359
and it needs partitions from ranks to

1197
00:47:03,359 --> 00:47:04,960
basically compute

1198
00:47:04,960 --> 00:47:07,280
a contribute a partition for contribute

1199
00:47:07,280 --> 00:47:08,319
for the

1200
00:47:08,319 --> 00:47:12,400
contrib contrap rdd sorry

1201
00:47:12,400 --> 00:47:14,400
so that might require you know some

1202
00:47:14,400 --> 00:47:16,640
never communication

1203
00:47:16,640 --> 00:47:19,440
but it turns out they have sort of a

1204
00:47:19,440 --> 00:47:24,240
clever optimization

1205
00:47:24,240 --> 00:47:25,839
this is in response to an earlier

1206
00:47:25,839 --> 00:47:28,160
question about like uh partitioning

1207
00:47:28,160 --> 00:47:30,960
uh the uh you can specify that you want

1208
00:47:30,960 --> 00:47:31,440
to

1209
00:47:31,440 --> 00:47:34,960
partition uh an rdd

1210
00:47:34,960 --> 00:47:38,880
using a hash partition

1211
00:47:38,880 --> 00:47:41,200
and what does what does that mean is

1212
00:47:41,200 --> 00:47:42,720
that the

1213
00:47:42,720 --> 00:47:46,640
links and ranks file these two rdds

1214
00:47:46,640 --> 00:47:47,920
are going to be petitioned in the same

1215
00:47:47,920 --> 00:47:49,200
way they're going to actually partition

1216
00:47:49,200 --> 00:47:50,079
by key

1217
00:47:50,079 --> 00:47:52,079
or by the hash of a key so if we go look

1218
00:47:52,079 --> 00:47:53,359
back you know

1219
00:47:53,359 --> 00:47:56,640
in our previous picture yeah the keys

1220
00:47:56,640 --> 00:47:59,680
for uh ranks you know are

1221
00:47:59,680 --> 00:48:03,359
you on youtube the keys for

1222
00:48:03,359 --> 00:48:07,680
um the uh links okay sorry the keys for

1223
00:48:07,680 --> 00:48:09,359
ranks are you want youtube free

1224
00:48:09,359 --> 00:48:12,400
the keys for uh links are also you want

1225
00:48:12,400 --> 00:48:14,000
youtube really free

1226
00:48:14,000 --> 00:48:16,240
and basically uh the client for

1227
00:48:16,240 --> 00:48:17,599
optimization this is a standard

1228
00:48:17,599 --> 00:48:18,480
optimization

1229
00:48:18,480 --> 00:48:20,800
from the database literature is that if

1230
00:48:20,800 --> 00:48:21,839
you partition

1231
00:48:21,839 --> 00:48:24,319
links and ranks by key then you know

1232
00:48:24,319 --> 00:48:26,160
this one is going to have u1 on one

1233
00:48:26,160 --> 00:48:28,160
machine

1234
00:48:28,160 --> 00:48:32,549
now maybe this is u2 and one machine

1235
00:48:32,559 --> 00:48:34,400
then ranks is going to have the same

1236
00:48:34,400 --> 00:48:35,839
thing it's going to have u1

1237
00:48:35,839 --> 00:48:37,440
on one machine and in fact you know it's

1238
00:48:37,440 --> 00:48:39,680
going to have u1 on the same machine

1239
00:48:39,680 --> 00:48:45,599
as the lynx one saying for u2 and u3

1240
00:48:45,599 --> 00:48:48,880
so even though this join

1241
00:48:48,880 --> 00:48:51,680
is exceptionally white dependency it can

1242
00:48:51,680 --> 00:48:53,599
be executed like a narrow dependency

1243
00:48:53,599 --> 00:48:54,720
because basically

1244
00:48:54,720 --> 00:48:56,559
to compute you know the join of these

1245
00:48:56,559 --> 00:48:58,640
two for the u1

1246
00:48:58,640 --> 00:49:01,760
you know for the first partition for pt

1247
00:49:01,760 --> 00:49:03,359
p1 you only have to look at the

1248
00:49:03,359 --> 00:49:05,920
partition p1 of links and p1

1249
00:49:05,920 --> 00:49:09,200
p1 of ranks uh because you know the keys

1250
00:49:09,200 --> 00:49:10,880
are hacked in the same way

1251
00:49:10,880 --> 00:49:14,000
to the same machine uh and so the

1252
00:49:14,000 --> 00:49:14,640
scheduler

1253
00:49:14,640 --> 00:49:16,559
uh or the programmer can actually

1254
00:49:16,559 --> 00:49:18,160
specify these hash petitions

1255
00:49:18,160 --> 00:49:21,200
the scheduler sees ah you know the join

1256
00:49:21,200 --> 00:49:21,680
actually

1257
00:49:21,680 --> 00:49:23,839
uses these hashtags the hash partitions

1258
00:49:23,839 --> 00:49:25,200
that are the same

1259
00:49:25,200 --> 00:49:27,119
and therefore and actually don't have to

1260
00:49:27,119 --> 00:49:29,200
do this white dependency i don't have to

1261
00:49:29,200 --> 00:49:30,000
do sort of a complete

1262
00:49:30,000 --> 00:49:32,720
barrier as the map reduce but i can just

1263
00:49:32,720 --> 00:49:35,440
treat this as a narrow dependency

1264
00:49:35,440 --> 00:49:38,480
so that's pretty cool

1265
00:49:38,480 --> 00:49:41,520
um then you know again uh

1266
00:49:41,520 --> 00:49:43,280
if a machine fails correct we talked a

1267
00:49:43,280 --> 00:49:44,960
little bit about this earlier

1268
00:49:44,960 --> 00:49:48,000
that might be uh painful because

1269
00:49:48,000 --> 00:49:50,319
you may have to re-execute many loops or

1270
00:49:50,319 --> 00:49:51,839
one iterations

1271
00:49:51,839 --> 00:49:54,160
and so uh you know if you would probably

1272
00:49:54,160 --> 00:49:55,760
write this for real then

1273
00:49:55,760 --> 00:49:57,440
uh the programmer would probably say

1274
00:49:57,440 --> 00:50:02,069
like maybe every you know 10 iterations

1275
00:50:02,079 --> 00:50:11,109
you know basically checkpoint

1276
00:50:11,119 --> 00:50:13,599
so then you don't have to re-compute the

1277
00:50:13,599 --> 00:50:15,119
computation all the way from uh

1278
00:50:15,119 --> 00:50:17,119
from the beginning oh so we don't

1279
00:50:17,119 --> 00:50:18,960
actually recompute links or anything

1280
00:50:18,960 --> 00:50:21,520
each time right like we don't persist it

1281
00:50:21,520 --> 00:50:22,720
sorry

1282
00:50:22,720 --> 00:50:25,200
we don't persist it no not at all the

1283
00:50:25,200 --> 00:50:26,240
only thing that was existed

1284
00:50:26,240 --> 00:50:27,839
was only once was links correct this is

1285
00:50:27,839 --> 00:50:29,839
the only thing that was had a persist

1286
00:50:29,839 --> 00:50:30,960
call

1287
00:50:30,960 --> 00:50:35,760
oh we do persist it links we do okay

1288
00:50:35,760 --> 00:50:38,480
but not the intermediate rdds because

1289
00:50:38,480 --> 00:50:40,240
they're basically new rdds every time

1290
00:50:40,240 --> 00:50:43,200
like range one is a new rdd ranks two is

1291
00:50:43,200 --> 00:50:46,000
a new rdd correct

1292
00:50:46,000 --> 00:50:48,480
uh but you may want to persist them you

1293
00:50:48,480 --> 00:50:49,280
know occasionally

1294
00:50:49,280 --> 00:50:52,319
and store them in hdfs so that uh if you

1295
00:50:52,319 --> 00:50:52,880
have to

1296
00:50:52,880 --> 00:50:54,480
a failure you don't have to go back to

1297
00:50:54,480 --> 00:50:56,480
the iteration loop iteration zero to

1298
00:50:56,480 --> 00:51:01,190
re-compute everything

1299
00:51:01,200 --> 00:51:04,000
okay does this make sense i'm sorry the

1300
00:51:04,000 --> 00:51:04,880
different

1301
00:51:04,880 --> 00:51:06,960
contribs can they be computed in

1302
00:51:06,960 --> 00:51:08,640
parallel

1303
00:51:08,640 --> 00:51:12,160
on different partitions yes um

1304
00:51:12,160 --> 00:51:13,920
because there there's like this line

1305
00:51:13,920 --> 00:51:16,319
that goes vertically down

1306
00:51:16,319 --> 00:51:19,920
down uh it's pipelined correct and then

1307
00:51:19,920 --> 00:51:20,480
there's

1308
00:51:20,480 --> 00:51:22,079
so there's two types of parallelism

1309
00:51:22,079 --> 00:51:23,839
there's stage parallelism and there is

1310
00:51:23,839 --> 00:51:24,319
sort of

1311
00:51:24,319 --> 00:51:26,240
panelism between different partitions

1312
00:51:26,240 --> 00:51:27,839
and we can think about this thing

1313
00:51:27,839 --> 00:51:30,400
you know this whole thing like running

1314
00:51:30,400 --> 00:51:31,440
many many times

1315
00:51:31,440 --> 00:51:45,270
on different partitions

1316
00:51:45,280 --> 00:51:47,920
so in this case the collect at the very

1317
00:51:47,920 --> 00:51:49,599
end will be the only place where we have

1318
00:51:49,599 --> 00:51:50,880
a wide

1319
00:51:50,880 --> 00:51:53,040
yeah exactly exactly the collect is the

1320
00:51:53,040 --> 00:51:54,880
only one that's gonna

1321
00:51:54,880 --> 00:51:56,240
you know whatever year we have no

1322
00:51:56,240 --> 00:51:58,480
repetitions correct

1323
00:51:58,480 --> 00:51:59,920
i'll make a mess of this picture but

1324
00:51:59,920 --> 00:52:02,000
that is gonna has to

1325
00:52:02,000 --> 00:52:09,030
get them from everyone

1326
00:52:09,040 --> 00:52:11,280
okay i hope the driver's overseas this

1327
00:52:11,280 --> 00:52:12,160
is actually pretty cool

1328
00:52:12,160 --> 00:52:15,040
right uh you know just by expressing

1329
00:52:15,040 --> 00:52:16,000
these computations

1330
00:52:16,000 --> 00:52:17,920
uh sort of a lineage graph or a data

1331
00:52:17,920 --> 00:52:19,119
flow computation

1332
00:52:19,119 --> 00:52:20,880
uh the scheduler has a bit of room for

1333
00:52:20,880 --> 00:52:23,359
optimizations uh like these uh

1334
00:52:23,359 --> 00:52:26,480
it's exploding hash partitions uh

1335
00:52:26,480 --> 00:52:28,640
the you know we get a lot of parallelism

1336
00:52:28,640 --> 00:52:30,480
uh we get also a lot of reuse

1337
00:52:30,480 --> 00:52:32,640
you know we can keep the results of one

1338
00:52:32,640 --> 00:52:34,319
rdd in memory so that we can reuse it

1339
00:52:34,319 --> 00:52:35,760
for the next iteration

1340
00:52:35,760 --> 00:52:37,760
and you can sort of see that you know

1341
00:52:37,760 --> 00:52:39,040
these techniques combined you know are

1342
00:52:39,040 --> 00:52:40,319
going to give you a

1343
00:52:40,319 --> 00:52:44,549
significant performance optimization

1344
00:52:44,559 --> 00:52:46,400
and allows you to expand and express

1345
00:52:46,400 --> 00:52:48,559
more uh powerful or more

1346
00:52:48,559 --> 00:52:52,230
interesting computations

1347
00:52:52,240 --> 00:52:54,000
so maybe with that i will like summarize

1348
00:52:54,000 --> 00:53:02,069
this lecture

1349
00:53:02,079 --> 00:53:06,559
um so a couple things uh

1350
00:53:06,559 --> 00:53:10,079
you know so rdds are made by

1351
00:53:10,079 --> 00:53:19,030
functional transformations

1352
00:53:19,040 --> 00:53:21,520
uh they're grouped together in sort of a

1353
00:53:21,520 --> 00:53:23,280
lineage graph

1354
00:53:23,280 --> 00:53:24,800
which you can think about as a data flow

1355
00:53:24,800 --> 00:53:26,720
graph

1356
00:53:26,720 --> 00:53:29,920
and this gives the you know this allows

1357
00:53:29,920 --> 00:53:32,870
reuse

1358
00:53:32,880 --> 00:53:34,319
uh it also allows some clever

1359
00:53:34,319 --> 00:53:41,510
optimizations by the scheduler

1360
00:53:41,520 --> 00:53:44,720
uh and basically allows also more extra

1361
00:53:44,720 --> 00:53:52,069
there's more expressiveness

1362
00:53:52,079 --> 00:53:58,230
yes then you know mapreduce by itself

1363
00:53:58,240 --> 00:54:01,359
and which results basically in good

1364
00:54:01,359 --> 00:54:02,640
performance because like a lot of the

1365
00:54:02,640 --> 00:54:14,549
data just stays in memory

1366
00:54:14,559 --> 00:54:16,319
and so if you actually if you're excited

1367
00:54:16,319 --> 00:54:18,079
about this uh you can try it out

1368
00:54:18,079 --> 00:54:20,640
uh you should download you know spark

1369
00:54:20,640 --> 00:54:22,640
play around and write some programs or

1370
00:54:22,640 --> 00:54:24,880
you know go to databricks.com and you

1371
00:54:24,880 --> 00:54:26,319
know create an account and then you can

1372
00:54:26,319 --> 00:54:26,880
run

1373
00:54:26,880 --> 00:54:29,200
spark computations on there on their

1374
00:54:29,200 --> 00:54:30,960
clusters

1375
00:54:30,960 --> 00:54:32,800
so if you're excited about this and want

1376
00:54:32,800 --> 00:54:34,480
to try it out you know it's pretty easy

1377
00:54:34,480 --> 00:54:35,280
to do so

1378
00:54:35,280 --> 00:54:37,200
like unlike farm you can just like not

1379
00:54:37,200 --> 00:54:38,640
play with

1380
00:54:38,640 --> 00:54:40,240
but this actually you can actually go

1381
00:54:40,240 --> 00:54:42,720
out and try out okay with that i want to

1382
00:54:42,720 --> 00:54:43,520
stop

1383
00:54:43,520 --> 00:54:46,799
uh for today and uh and

1384
00:54:46,799 --> 00:54:48,480
the people that want to hang around and

1385
00:54:48,480 --> 00:54:50,240
ask more questions please feel free to

1386
00:54:50,240 --> 00:54:51,119
do so

1387
00:54:51,119 --> 00:54:52,720
the other thing i want to remind people

1388
00:54:52,720 --> 00:54:54,960
of is that you know the

1389
00:54:54,960 --> 00:54:58,559
deadline of for 4b uh is a little bit

1390
00:54:58,559 --> 00:54:59,599
away but

1391
00:54:59,599 --> 00:55:01,359
i just want to remind people that 4b is

1392
00:55:01,359 --> 00:55:03,040
pretty tricky uh requires a bit of

1393
00:55:03,040 --> 00:55:04,160
design

1394
00:55:04,160 --> 00:55:07,040
so don't don't start too late and uh

1395
00:55:07,040 --> 00:55:07,599
with that

1396
00:55:07,599 --> 00:55:14,309
i'll see you on tuesday

1397
00:55:14,319 --> 00:55:16,880
and again questions i'm happy to answer

1398
00:55:16,880 --> 00:55:18,480
that

1399
00:55:18,480 --> 00:55:22,720
thank you uh i had a question about

1400
00:55:22,720 --> 00:55:26,319
the checkpoints i think they mentioned

1401
00:55:26,319 --> 00:55:29,520
automatic checkpoints um using

1402
00:55:29,520 --> 00:55:31,680
data about how long each computation

1403
00:55:31,680 --> 00:55:32,559
took

1404
00:55:32,559 --> 00:55:35,680
um and i wasn't really sure what

1405
00:55:35,680 --> 00:55:38,480
they mean by this like what what are

1406
00:55:38,480 --> 00:55:39,200
they going to be

1407
00:55:39,200 --> 00:55:42,160
optimizing for i think you know the

1408
00:55:42,160 --> 00:55:43,520
whole checkpoint is correct is an

1409
00:55:43,520 --> 00:55:45,040
optimization between

1410
00:55:45,040 --> 00:55:46,880
uh you know taking checkpoint is

1411
00:55:46,880 --> 00:55:48,160
expensive

1412
00:55:48,160 --> 00:55:51,599
uh and so that you know takes time uh

1413
00:55:51,599 --> 00:55:53,440
and but you know re-execution if there's

1414
00:55:53,440 --> 00:55:54,640
a machine failure

1415
00:55:54,640 --> 00:55:57,599
also takes a lot of time uh and so for

1416
00:55:57,599 --> 00:55:58,559
example if you take never

1417
00:55:58,559 --> 00:55:59,839
checkpoint then you basically have to

1418
00:55:59,839 --> 00:56:01,119
re-execute the computation from the

1419
00:56:01,119 --> 00:56:02,720
beginning right

1420
00:56:02,720 --> 00:56:04,400
but if you take periodically checkpoints

1421
00:56:04,400 --> 00:56:06,240
you know you don't have to repeat

1422
00:56:06,240 --> 00:56:07,839
you know the computation that you did

1423
00:56:07,839 --> 00:56:09,200
before the checkpoint but the check

1424
00:56:09,200 --> 00:56:11,680
taking the checkpoint takes time

1425
00:56:11,680 --> 00:56:13,599
so if you take very frequent checkpoints

1426
00:56:13,599 --> 00:56:15,040
you don't have to recompute a lot but

1427
00:56:15,040 --> 00:56:16,079
you spend all your time taking

1428
00:56:16,079 --> 00:56:18,000
checkpoints

1429
00:56:18,000 --> 00:56:20,880
and so there's sort of a you know an

1430
00:56:20,880 --> 00:56:22,480
optimization problem here

1431
00:56:22,480 --> 00:56:23,520
you know you want to take the

1432
00:56:23,520 --> 00:56:26,240
checkpoints and some regular interval

1433
00:56:26,240 --> 00:56:28,960
uh that you're willing to take to

1434
00:56:28,960 --> 00:56:31,910
recompute

1435
00:56:31,920 --> 00:56:33,680
okay so maybe like compute checkpoints

1436
00:56:33,680 --> 00:56:36,640
only for very large computations

1437
00:56:36,640 --> 00:56:38,160
yeah or like for example in the cases

1438
00:56:38,160 --> 00:56:40,079
pagerank you know maybe you know

1439
00:56:40,079 --> 00:56:44,720
do it every 10 iterations

1440
00:56:44,720 --> 00:56:46,799
thank you it depends of course if the

1441
00:56:46,799 --> 00:56:48,079
size of your checkpoints

1442
00:56:48,079 --> 00:56:49,359
decides to check when you're small you

1443
00:56:49,359 --> 00:56:52,240
can checkpoint more frequently

1444
00:56:52,240 --> 00:56:54,079
but in the case of this pagerank you

1445
00:56:54,079 --> 00:56:55,040
know that checkpoint is going to be

1446
00:56:55,040 --> 00:56:56,640
pretty big

1447
00:56:56,640 --> 00:56:59,119
there's going to be a line or a record

1448
00:56:59,119 --> 00:57:00,160
you know per

1449
00:57:00,160 --> 00:57:03,190
uh webpage

1450
00:57:03,200 --> 00:57:07,270
that makes sense thank you

1451
00:57:07,280 --> 00:57:10,000
i have a question about the driver

1452
00:57:10,000 --> 00:57:11,359
because the application

1453
00:57:11,359 --> 00:57:14,559
is like does the driver is the driver on

1454
00:57:14,559 --> 00:57:15,440
the client side

1455
00:57:15,440 --> 00:57:17,920
or is it yes okay yeah the driver is on

1456
00:57:17,920 --> 00:57:19,040
the car

1457
00:57:19,040 --> 00:57:21,200
if it crashes we lose like the the whole

1458
00:57:21,200 --> 00:57:23,440
graph and that's fine because that's the

1459
00:57:23,440 --> 00:57:25,920
application right yeah i don't exactly

1460
00:57:25,920 --> 00:57:27,599
know what happens because the

1461
00:57:27,599 --> 00:57:31,119
the uh the scanner has a tube

1462
00:57:31,119 --> 00:57:35,280
and uh and the scanner is full tolerant

1463
00:57:35,280 --> 00:57:36,480
so i don't know exactly when you know

1464
00:57:36,480 --> 00:57:38,400
what happens maybe you can reconnect i i

1465
00:57:38,400 --> 00:57:43,829
don't know

1466
00:57:43,839 --> 00:57:46,000
i had a question about the why

1467
00:57:46,000 --> 00:57:47,760
dependency optimization

1468
00:57:47,760 --> 00:57:49,680
you mentioned that they do like the hash

1469
00:57:49,680 --> 00:57:50,799
partitioning um

1470
00:57:50,799 --> 00:57:53,200
how does that work okay i can say a

1471
00:57:53,200 --> 00:57:54,799
little bit more so this is not a

1472
00:57:54,799 --> 00:57:56,160
hashtag is not something that they

1473
00:57:56,160 --> 00:57:57,200
invented i mean this is actually

1474
00:57:57,200 --> 00:57:57,920
something that is

1475
00:57:57,920 --> 00:58:01,440
uh uh is a standard database

1476
00:58:01,440 --> 00:58:05,200
uh partitioning scheme and it is cool

1477
00:58:05,200 --> 00:58:07,440
because if you need to compute a join

1478
00:58:07,440 --> 00:58:09,200
you don't have to do a lot of

1479
00:58:09,200 --> 00:58:10,720
communication so let me actually i can

1480
00:58:10,720 --> 00:58:11,200
maybe

1481
00:58:11,200 --> 00:58:12,880
start a new slide because it's a little

1482
00:58:12,880 --> 00:58:14,319
bit hard to

1483
00:58:14,319 --> 00:58:19,990
read so hash partitioning

1484
00:58:20,000 --> 00:58:22,640
so if you have two data sets and here's

1485
00:58:22,640 --> 00:58:23,680
data set one

1486
00:58:23,680 --> 00:58:26,240
your data set two they have keys right

1487
00:58:26,240 --> 00:58:27,440
like you know e1

1488
00:58:27,440 --> 00:58:30,480
p2 so they have the same

1489
00:58:30,480 --> 00:58:33,839
set of keys then what you do

1490
00:58:33,839 --> 00:58:35,680
by hash partitioning you partition the

1491
00:58:35,680 --> 00:58:37,440
data set in number of partitions

1492
00:58:37,440 --> 00:58:40,880
so boom boom boom and

1493
00:58:40,880 --> 00:58:44,630
uh you has the key

1494
00:58:44,640 --> 00:58:47,200
so you have k1 you has k2 and that

1495
00:58:47,200 --> 00:58:48,720
actually determines the partition it

1496
00:58:48,720 --> 00:58:51,920
ends up in and so all the

1497
00:58:51,920 --> 00:58:53,760
keys that actually have the same hash

1498
00:58:53,760 --> 00:58:55,359
will end up in the same place so like

1499
00:58:55,359 --> 00:58:56,720
this is machine one

1500
00:58:56,720 --> 00:58:59,680
this is machine two it's machine three

1501
00:58:59,680 --> 00:59:01,359
so you take you know whatever

1502
00:59:01,359 --> 00:59:04,160
you hash k1 that goes in here you know

1503
00:59:04,160 --> 00:59:06,240
you hash you know whatever k2 maybe

1504
00:59:06,240 --> 00:59:07,680
somewhere else in the file who knows

1505
00:59:07,680 --> 00:59:08,480
where it is

1506
00:59:08,480 --> 00:59:10,319
and you hash it to that partition you do

1507
00:59:10,319 --> 00:59:12,319
the same thing here

1508
00:59:12,319 --> 00:59:14,319
for the other data set so here's data

1509
00:59:14,319 --> 00:59:17,430
set one

1510
00:59:17,440 --> 00:59:20,950
this data set too

1511
00:59:20,960 --> 00:59:23,359
like links and ranks and you know what

1512
00:59:23,359 --> 00:59:24,160
will happen

1513
00:59:24,160 --> 00:59:26,880
is that all the records in this data set

1514
00:59:26,880 --> 00:59:28,079
that have

1515
00:59:28,079 --> 00:59:31,040
the same keys as records in the other

1516
00:59:31,040 --> 00:59:33,200
data set those keys or those records

1517
00:59:33,200 --> 00:59:34,960
will end up in the same machine

1518
00:59:34,960 --> 00:59:37,280
right so here you're going to partition

1519
00:59:37,280 --> 00:59:39,359
this guy and basically k1 will end up

1520
00:59:39,359 --> 00:59:40,240
here too

1521
00:59:40,240 --> 00:59:43,990
on the same machine

1522
00:59:44,000 --> 00:59:45,839
correct and same for you know the other

1523
00:59:45,839 --> 00:59:47,520
lp's because you basically use the same

1524
00:59:47,520 --> 00:59:49,040
hash function and you have the same set

1525
00:59:49,040 --> 00:59:49,839
of keys

1526
00:59:49,839 --> 00:59:51,520
and so this allows you to take a data

1527
00:59:51,520 --> 00:59:53,040
set you know partition them both in the

1528
00:59:53,040 --> 00:59:53,680
same way

1529
00:59:53,680 --> 00:59:56,240
uh using this uh hashing trick and this

1530
00:59:56,240 --> 00:59:58,000
is cool because now if you need to do a

1531
00:59:58,000 --> 00:59:59,280
join

1532
00:59:59,280 --> 01:00:00,880
over these two data sets like if you

1533
01:00:00,880 --> 01:00:02,799
need to do a join of these two data sets

1534
01:00:02,799 --> 01:00:04,160
then basically you can just join the

1535
01:00:04,160 --> 01:00:06,319
partitions

1536
01:00:06,319 --> 01:00:08,720
and you don't have to communicate you

1537
01:00:08,720 --> 01:00:10,079
know each of these machines doesn't have

1538
01:00:10,079 --> 01:00:12,000
to communicate with any other

1539
01:00:12,000 --> 01:00:14,079
machine because it knows it has all the

1540
01:00:14,079 --> 01:00:15,200
keys that you know

1541
01:00:15,200 --> 01:00:17,280
uh that the other data set has and

1542
01:00:17,280 --> 01:00:20,160
they're all on the same machine

1543
01:00:20,160 --> 01:00:22,480
gotcha so basically it's just trying to

1544
01:00:22,480 --> 01:00:23,680
uh sort

1545
01:00:23,680 --> 01:00:25,520
not sort but like bucket the different

1546
01:00:25,520 --> 01:00:31,990
yeah in the same machine

1547
01:00:32,000 --> 01:00:35,119
okay great thank you so much

1548
01:00:35,119 --> 01:00:37,040
so this means the hash function has to

1549
01:00:37,040 --> 01:00:38,240
make sure that

1550
01:00:38,240 --> 01:00:41,280
there are no links that would have to be

1551
01:00:41,280 --> 01:00:42,240
like

1552
01:00:42,240 --> 01:00:43,839
using the computation on another machine

1553
01:00:43,839 --> 01:00:45,839
right like for instance yeah well

1554
01:00:45,839 --> 01:00:47,520
since they use the same hash function

1555
01:00:47,520 --> 01:00:48,799
and they have the same keys you know

1556
01:00:48,799 --> 01:00:54,829
that will happen

1557
01:00:54,839 --> 01:00:58,160
um i oh i had a question i

1558
01:00:58,160 --> 01:00:59,520
actually wanted to come back to the

1559
01:00:59,520 --> 01:01:01,920
question i asked before yeah yeah yeah

1560
01:01:01,920 --> 01:01:05,030
good good good

1561
01:01:05,040 --> 01:01:08,400
um so let me open up yeah let me also

1562
01:01:08,400 --> 01:01:10,480
open the paper i guess

1563
01:01:10,480 --> 01:01:11,920
is there any other people that have

1564
01:01:11,920 --> 01:01:13,520
questions because maybe this will take a

1565
01:01:13,520 --> 01:01:13,920
little bit

1566
01:01:13,920 --> 01:01:16,319
of time yeah actually i had a quick

1567
01:01:16,319 --> 01:01:17,359
question on the uh

1568
01:01:17,359 --> 01:01:20,400
thought holler thought tolerance of farm

1569
01:01:20,400 --> 01:01:24,000
um so so just to clarify

1570
01:01:24,000 --> 01:01:27,440
what happens so if a failure occurs

1571
01:01:27,440 --> 01:01:30,960
before the decision point um

1572
01:01:30,960 --> 01:01:32,880
then the entire thing is aborted but if

1573
01:01:32,880 --> 01:01:34,720
it occurs after the decision point then

1574
01:01:34,720 --> 01:01:35,200
after

1575
01:01:35,200 --> 01:01:37,920
the failed computers come back up they

1576
01:01:37,920 --> 01:01:39,040
have to

1577
01:01:39,040 --> 01:01:40,640
re-ask the coordinator for whether or

1578
01:01:40,640 --> 01:01:42,240
not they should come in

1579
01:01:42,240 --> 01:01:44,559
uh they don't really re-ask correct like

1580
01:01:44,559 --> 01:01:46,400
the what what happens is they're

1581
01:01:46,400 --> 01:01:48,400
after failure there's a recovery process

1582
01:01:48,400 --> 01:01:50,720
runs and the repulsion process

1583
01:01:50,720 --> 01:01:53,040
looks uh basically and all the logs are

1584
01:01:53,040 --> 01:01:53,760
drained

1585
01:01:53,760 --> 01:01:56,000
and and then the recovery process looks

1586
01:01:56,000 --> 01:01:57,680
at the state of the system

1587
01:01:57,680 --> 01:01:59,359
and based on the state of the system it

1588
01:01:59,359 --> 01:02:01,119
decides what to do with the transaction

1589
01:02:01,119 --> 01:02:04,480
either it aborted or commits it and

1590
01:02:04,480 --> 01:02:07,359
the key uh aspect here in this protocol

1591
01:02:07,359 --> 01:02:08,640
is to ensure

1592
01:02:08,640 --> 01:02:12,079
that at the point when the

1593
01:02:12,079 --> 01:02:13,839
uh transaction coordinator actually has

1594
01:02:13,839 --> 01:02:15,119
reported to the application that the

1595
01:02:15,119 --> 01:02:16,559
transaction succeeded

1596
01:02:16,559 --> 01:02:18,799
uh committed uh it has to be the case

1597
01:02:18,799 --> 01:02:20,640
that there are enough pieces of evidence

1598
01:02:20,640 --> 01:02:21,839
left around

1599
01:02:21,839 --> 01:02:23,119
in the system so that during the

1600
01:02:23,119 --> 01:02:24,799
recovery process that transaction is

1601
01:02:24,799 --> 01:02:26,720
definitely committed

1602
01:02:26,720 --> 01:02:30,640
and uh and that's not sort of the plan

1603
01:02:30,640 --> 01:02:32,000
and the reason that there's enough

1604
01:02:32,000 --> 01:02:33,760
evidence is because there's these

1605
01:02:33,760 --> 01:02:36,079
log records lying around uh there's this

1606
01:02:36,079 --> 01:02:37,680
command backup records

1607
01:02:37,680 --> 01:02:39,839
lying around and there's this this one

1608
01:02:39,839 --> 01:02:43,029
commit record

1609
01:02:43,039 --> 01:02:46,000
i see so if something for example if a

1610
01:02:46,000 --> 01:02:46,559
failure

1611
01:02:46,559 --> 01:02:49,680
occurs on a primary before it gets a

1612
01:02:49,680 --> 01:02:51,119
commit primary

1613
01:02:51,119 --> 01:02:53,839
um what happens there ah so that's

1614
01:02:53,839 --> 01:02:55,119
interesting so there's enough backup

1615
01:02:55,119 --> 01:02:56,480
records correct

1616
01:02:56,480 --> 01:02:58,240
to basically decide you know that every

1617
01:02:58,240 --> 01:02:59,920
backup that every shard actually has

1618
01:02:59,920 --> 01:03:01,359
committed

1619
01:03:01,359 --> 01:03:03,839
and uh and so that's enough information

1620
01:03:03,839 --> 01:03:05,680
for the recovery process to say oh yeah

1621
01:03:05,680 --> 01:03:07,520
i'm gonna run for that transaction

1622
01:03:07,520 --> 01:03:10,559
because it could have committed

1623
01:03:10,559 --> 01:03:12,319
got it so it doesn't need the primary in

1624
01:03:12,319 --> 01:03:14,480
that case it can use the backups because

1625
01:03:14,480 --> 01:03:17,200
the backups have

1626
01:03:17,200 --> 01:03:21,280
exactly got it thank you

1627
01:03:21,280 --> 01:03:24,559
and what happens if the backup fails

1628
01:03:24,559 --> 01:03:26,640
uh well if one of the backup fails and

1629
01:03:26,640 --> 01:03:28,319
presumably that means that the commit

1630
01:03:28,319 --> 01:03:30,640
record is still there and then again

1631
01:03:30,640 --> 01:03:32,160
that is enough information to decide

1632
01:03:32,160 --> 01:03:33,359
that actually the transaction needs to

1633
01:03:33,359 --> 01:03:34,480
commit

1634
01:03:34,480 --> 01:03:36,240
and there's enough backups around to

1635
01:03:36,240 --> 01:03:37,760
actually know what the new values or

1636
01:03:37,760 --> 01:03:38,960
there's also the log

1637
01:03:38,960 --> 01:03:41,359
entries which actually contain so if a

1638
01:03:41,359 --> 01:03:42,400
primary is up

1639
01:03:42,400 --> 01:03:44,319
it will have a log entry it will be a

1640
01:03:44,319 --> 01:03:46,000
commit entry plus and there's enough

1641
01:03:46,000 --> 01:03:47,599
backups to actually uh finish the

1642
01:03:47,599 --> 01:03:50,630
transaction

1643
01:03:50,640 --> 01:03:53,839
thank you i'm sorry to follow up on that

1644
01:03:53,839 --> 01:03:55,119
if you said that

1645
01:03:55,119 --> 01:03:58,480
if the primary failed then you could use

1646
01:03:58,480 --> 01:03:59,920
the backups to

1647
01:03:59,920 --> 01:04:01,359
complete the production if there's

1648
01:04:01,359 --> 01:04:03,119
enough of them

1649
01:04:03,119 --> 01:04:06,880
you need need to elect a new primary

1650
01:04:06,880 --> 01:04:09,119
i think this has all happened during the

1651
01:04:09,119 --> 01:04:09,920
uh

1652
01:04:09,920 --> 01:04:11,599
basically you can think of the recovery

1653
01:04:11,599 --> 01:04:13,280
process as a primary

1654
01:04:13,280 --> 01:04:16,480
and that just finishes everything off

1655
01:04:16,480 --> 01:04:18,799
oh so whoever does the recovery is the

1656
01:04:18,799 --> 01:04:20,000
primary

1657
01:04:20,000 --> 01:04:23,680
yeah yeah okay makes sense

1658
01:04:23,680 --> 01:04:25,680
thank you i don't think explicitly they

1659
01:04:25,680 --> 01:04:27,680
you know promote a primary you just like

1660
01:04:27,680 --> 01:04:29,920
go ahead and do it

1661
01:04:29,920 --> 01:04:33,119
and what is like enough backups well

1662
01:04:33,119 --> 01:04:36,400
we have f plus one right

1663
01:04:36,400 --> 01:04:38,960
uh and so it means that you know as long

1664
01:04:38,960 --> 01:04:40,799
as one is left

1665
01:04:40,799 --> 01:04:42,960
you know we we're good so we can't have

1666
01:04:42,960 --> 01:04:44,559
more than f plus one failures

1667
01:04:44,559 --> 01:04:46,559
and we can only have f failures in this

1668
01:04:46,559 --> 01:04:47,760
particular drawing

1669
01:04:47,760 --> 01:04:50,950
f is one

1670
01:04:50,960 --> 01:04:52,880
so there has to be per shard you know

1671
01:04:52,880 --> 01:04:55,990
one machine left

1672
01:04:56,000 --> 01:04:58,720
okay that makes sense thank you you're

1673
01:04:58,720 --> 01:05:02,309
welcome

1674
01:05:02,319 --> 01:05:06,630
okay philippians just me and you

1675
01:05:06,640 --> 01:05:08,960
well unless anyone else has questions i

1676
01:05:08,960 --> 01:05:16,150
think

1677
01:05:16,160 --> 01:05:19,440
so it's it's page they explain it in

1678
01:05:19,440 --> 01:05:20,799
page six

1679
01:05:20,799 --> 01:05:24,400
yeah right below

1680
01:05:24,400 --> 01:05:27,599
table three yep

1681
01:05:27,599 --> 01:05:30,400
um so the bear got it starts the most

1682
01:05:30,400 --> 01:05:31,760
interesting question right

1683
01:05:31,760 --> 01:05:34,960
it goes on to define yeah we found the

1684
01:05:34,960 --> 01:05:36,319
most sufficient usual to classify

1685
01:05:36,319 --> 01:05:38,319
dependencies in two types narrow

1686
01:05:38,319 --> 01:05:40,319
where parent petition the parent id is

1687
01:05:40,319 --> 01:05:42,480
used by most one partition of the child

1688
01:05:42,480 --> 01:05:44,079
rt correct so this is

1689
01:05:44,079 --> 01:05:46,240
right so let me so let's let's draw this

1690
01:05:46,240 --> 01:05:48,319
correct so here we have

1691
01:05:48,319 --> 01:05:52,960
a parent petition

1692
01:05:52,960 --> 01:05:57,760
right and let's say there's a map

1693
01:05:57,760 --> 01:06:04,559
and here we have the child petition

1694
01:06:04,559 --> 01:06:08,559
okay right okay so that's the narrow

1695
01:06:08,559 --> 01:06:12,630
this is the narrow case

1696
01:06:12,640 --> 01:06:16,470
right but um

1697
01:06:16,480 --> 01:06:20,319
i think like the the the example i was

1698
01:06:20,319 --> 01:06:21,280
thinking of is

1699
01:06:21,280 --> 01:06:24,319
you know each um

1700
01:06:24,319 --> 01:06:27,599
parent is used by at most

1701
01:06:27,599 --> 01:06:32,400
um one partition of the child but the

1702
01:06:32,400 --> 01:06:33,200
child

1703
01:06:33,200 --> 01:06:35,119
right that doesn't say anything about

1704
01:06:35,119 --> 01:06:37,200
like it says

1705
01:06:37,200 --> 01:06:39,119
right like it doesn't necessarily mean

1706
01:06:39,119 --> 01:06:41,280
it's a one-to-one relationship

1707
01:06:41,280 --> 01:06:43,760
right well because more or less have to

1708
01:06:43,760 --> 01:06:44,880
correctly just let me

1709
01:06:44,880 --> 01:06:47,359
uh let's say we have a white one correct

1710
01:06:47,359 --> 01:06:49,039
so then

1711
01:06:49,039 --> 01:06:52,160
here we have parent

1712
01:06:52,160 --> 01:06:56,309
partition one

1713
01:06:56,319 --> 01:06:58,240
here we have you know maybe it has end

1714
01:06:58,240 --> 01:06:59,599
of them correct you know here we're

1715
01:06:59,599 --> 01:07:00,319
right here

1716
01:07:00,319 --> 01:07:06,549
and here's n and in the white one

1717
01:07:06,559 --> 01:07:09,910
the child is

1718
01:07:09,920 --> 01:07:13,520
right um what what what i was saying is

1719
01:07:13,520 --> 01:07:15,280
i i think you know based on the

1720
01:07:15,280 --> 01:07:17,839
definition given the paper

1721
01:07:17,839 --> 01:07:23,990
this could be a narrow partition

1722
01:07:24,000 --> 01:07:27,200
and in fact i mean like if you look at

1723
01:07:27,200 --> 01:07:30,720
um like a join with inputs

1724
01:07:30,720 --> 01:07:33,119
co-partitioned like you have yeah the

1725
01:07:33,119 --> 01:07:34,559
hash partition that actually the white

1726
01:07:34,559 --> 01:07:36,480
one turns into a narrow one

1727
01:07:36,480 --> 01:07:39,680
right right but but you still have like

1728
01:07:39,680 --> 01:07:42,640
a partition a child partition getting

1729
01:07:42,640 --> 01:07:43,200
like

1730
01:07:43,200 --> 01:07:47,200
and like being completed from several

1731
01:07:47,200 --> 01:07:49,200
yeah parent partitions yeah right i

1732
01:07:49,200 --> 01:07:51,119
think

1733
01:07:51,119 --> 01:07:52,640
explicitly mentioned that correct i

1734
01:07:52,640 --> 01:07:54,960
think that's the only example

1735
01:07:54,960 --> 01:07:58,160
yeah but like i think there's a typo in

1736
01:07:58,160 --> 01:08:02,150
that sentence correct

1737
01:08:02,160 --> 01:08:04,000
i i mean i'm not i'm not sure if it's

1738
01:08:04,000 --> 01:08:06,480
yeah i i'm not sure like if it's

1739
01:08:06,480 --> 01:08:11,029
what they meant to like write or

1740
01:08:11,039 --> 01:08:12,799
what we know we conclude this is the two

1741
01:08:12,799 --> 01:08:15,039
cases like i got it yeah

1742
01:08:15,039 --> 01:08:18,880
okay so there are no other cases well

1743
01:08:18,880 --> 01:08:21,199
we can go for every operation correct

1744
01:08:21,199 --> 01:08:22,480
and then we can see whether it's an

1745
01:08:22,480 --> 01:08:25,679
error or a white one right right

1746
01:08:25,679 --> 01:08:27,839
and the ones that are and it's the job

1747
01:08:27,839 --> 01:08:28,719
of the programmer

1748
01:08:28,719 --> 01:08:30,319
that defines these operations to

1749
01:08:30,319 --> 01:08:31,759
actually indicate whether it's a

1750
01:08:31,759 --> 01:08:34,000
white partition or an arrow but a white

1751
01:08:34,000 --> 01:08:35,520
dependency or a narrow dependency

1752
01:08:35,520 --> 01:08:36,239
correct

1753
01:08:36,239 --> 01:08:38,880
that's what the figure table three is

1754
01:08:38,880 --> 01:08:39,759
about

1755
01:08:39,759 --> 01:08:42,960
uh-huh yeah uh like what i'm saying is

1756
01:08:42,960 --> 01:08:44,080
like usually like

1757
01:08:44,080 --> 01:08:45,520
like the way like i saw it through the

1758
01:08:45,520 --> 01:08:48,080
paper like uh like

1759
01:08:48,080 --> 01:08:50,719
your example on the right would be a

1760
01:08:50,719 --> 01:08:52,480
could would be a narrow dependency

1761
01:08:52,480 --> 01:08:53,279
unless

1762
01:08:53,279 --> 01:08:55,440
right like you have several child and

1763
01:08:55,440 --> 01:08:56,719
the parent partitions

1764
01:08:56,719 --> 01:08:59,759
are like ah okay so in general okay it

1765
01:08:59,759 --> 01:09:01,120
is the case support correct if there's

1766
01:09:01,120 --> 01:09:02,880
another child petition here okay so

1767
01:09:02,880 --> 01:09:04,480
maybe this is why we're trying to get it

1768
01:09:04,480 --> 01:09:06,000
so let's separate

1769
01:09:06,000 --> 01:09:07,359
the real picture that i should draw is

1770
01:09:07,359 --> 01:09:09,199
this okay that's another channel

1771
01:09:09,199 --> 01:09:10,000
partition

1772
01:09:10,000 --> 01:09:13,279
and basically operations uh

1773
01:09:13,279 --> 01:09:17,189
the transformations are

1774
01:09:17,199 --> 01:09:20,560
exactly yeah and that's narrow for sure

1775
01:09:20,560 --> 01:09:23,520
right like this on the right side this

1776
01:09:23,520 --> 01:09:23,920
is why

1777
01:09:23,920 --> 01:09:25,520
sorry that's why yeah yeah that's what i

1778
01:09:25,520 --> 01:09:27,520
meant that that for sure is white

1779
01:09:27,520 --> 01:09:29,120
okay and the other one is white you know

1780
01:09:29,120 --> 01:09:31,679
the one i drew is also white i believe

1781
01:09:31,679 --> 01:09:34,239
like okay do you join if you do an

1782
01:09:34,239 --> 01:09:35,279
action like

1783
01:09:35,279 --> 01:09:37,920
uh collect at the very end yeah it's a

1784
01:09:37,920 --> 01:09:39,679
white dependency yeah

1785
01:09:39,679 --> 01:09:41,199
okay there's a does it say that it

1786
01:09:41,199 --> 01:09:42,319
doesn't say it has to come for different

1787
01:09:42,319 --> 01:09:44,000
rdds it just says like it has to come

1788
01:09:44,000 --> 01:09:44,319
from

1789
01:09:44,319 --> 01:09:46,000
different partitions and so this is

1790
01:09:46,000 --> 01:09:48,000
narrow so i think narrow

1791
01:09:48,000 --> 01:09:49,600
is only the case where there's one to

1792
01:09:49,600 --> 01:09:51,359
one okay

1793
01:09:51,359 --> 01:09:53,359
there is like narrow means no

1794
01:09:53,359 --> 01:09:55,840
communication

1795
01:09:55,840 --> 01:10:07,590
right okay

1796
01:10:07,600 --> 01:10:10,080
yeah i think i think the case where i

1797
01:10:10,080 --> 01:10:12,000
was like confused was like the

1798
01:10:12,000 --> 01:10:15,360
like men many the one

1799
01:10:15,360 --> 01:10:17,840
like i think based on the definition of

1800
01:10:17,840 --> 01:10:19,280
the like of the of the

1801
01:10:19,280 --> 01:10:21,199
paper like the many-to-one relation is

1802
01:10:21,199 --> 01:10:22,880
still um

1803
01:10:22,880 --> 01:10:25,280
it's still narrow no i think they i

1804
01:10:25,280 --> 01:10:26,239
think they

1805
01:10:26,239 --> 01:10:29,040
mean it to be uh i mean yeah yeah yeah

1806
01:10:29,040 --> 01:10:30,000
but like strictly

1807
01:10:30,000 --> 01:10:32,800
like if you re like yeah i i think maybe

1808
01:10:32,800 --> 01:10:34,960
like an implementation you'll see like

1809
01:10:34,960 --> 01:10:36,640
yeah what what are you saying right like

1810
01:10:36,640 --> 01:10:38,560
it's white i was just like i think if

1811
01:10:38,560 --> 01:10:39,679
you read like the

1812
01:10:39,679 --> 01:10:41,280
the yeah you could be confusing paper

1813
01:10:41,280 --> 01:10:43,040
textually yeah like you

1814
01:10:43,040 --> 01:10:44,560
you can get confused so like the menu

1815
01:10:44,560 --> 01:10:46,719
one relationship but the one too many

1816
01:10:46,719 --> 01:10:50,159
is clearly wide yeah but yeah okay

1817
01:10:50,159 --> 01:10:54,239
sounds good okay yeah that'll just makes

1818
01:10:54,239 --> 01:10:56,239
i can't make the paper easier to

1819
01:10:56,239 --> 01:10:57,679
understand in general

1820
01:10:57,679 --> 01:11:00,800
okay good good okay yeah there's

1821
01:11:00,800 --> 01:11:02,320
an easier to understand paper than the

1822
01:11:02,320 --> 01:11:05,199
form oh yeah

1823
01:11:05,199 --> 01:11:08,400
yeah for sure yeah i think

1824
01:11:08,400 --> 01:11:10,480
well i think those were probably the

1825
01:11:10,480 --> 01:11:12,239
most too heavy-duty papers that will uh

1826
01:11:12,239 --> 01:11:14,800
that we'll see in this term

1827
01:11:14,800 --> 01:11:16,880
okay nice environment yeah farmer's

1828
01:11:16,880 --> 01:11:18,880
banner i think the remaining ones are

1829
01:11:18,880 --> 01:11:22,320
a little bit more you know more uh i

1830
01:11:22,320 --> 01:11:24,880
wouldn't say straightforward but

1831
01:11:24,880 --> 01:11:29,920
perhaps fewer moving pieces nice

1832
01:11:29,920 --> 01:11:33,199
okay awesome thanks professor can i ask

1833
01:11:33,199 --> 01:11:33,600
one

1834
01:11:33,600 --> 01:11:35,679
last question i just realized that i

1835
01:11:35,679 --> 01:11:37,360
have um

1836
01:11:37,360 --> 01:11:39,760
it was about the computation you can

1837
01:11:39,760 --> 01:11:41,360
paralyze it if

1838
01:11:41,360 --> 01:11:43,040
if it's on different partitions but if

1839
01:11:43,040 --> 01:11:44,719
it's also

1840
01:11:44,719 --> 01:11:46,960
um you said if it's in yeah there's a

1841
01:11:46,960 --> 01:11:49,120
stage the stages correct you know

1842
01:11:49,120 --> 01:11:51,360
they're sort of uh like streaming

1843
01:11:51,360 --> 01:11:52,719
parallelism if you will or

1844
01:11:52,719 --> 01:11:55,600
pipeline parallelism uh let me see if i

1845
01:11:55,600 --> 01:11:57,920
can find a picture

1846
01:11:57,920 --> 01:12:01,960
that's one of them

1847
01:12:01,970 --> 01:12:04,640
[Music]

1848
01:12:04,640 --> 01:12:07,760
i gotta find a lineage graph uh

1849
01:12:07,760 --> 01:12:11,040
no no

1850
01:12:11,040 --> 01:12:12,560
right here it's really in this graph

1851
01:12:12,560 --> 01:12:14,480
great now maybe here's a

1852
01:12:14,480 --> 01:12:17,360
picture that we can modify uh do you see

1853
01:12:17,360 --> 01:12:18,880
it

1854
01:12:18,880 --> 01:12:22,239
yes okay so basically

1855
01:12:22,239 --> 01:12:23,600
this is the lineage graph and he was to

1856
01:12:23,600 --> 01:12:25,280
collect and

1857
01:12:25,280 --> 01:12:30,630
so this is like one stage right

1858
01:12:30,640 --> 01:12:32,159
and the schedule runs one of these

1859
01:12:32,159 --> 01:12:33,840
stages on each worker

1860
01:12:33,840 --> 01:12:38,560
for each partition right so each

1861
01:12:38,560 --> 01:12:42,070
worker

1862
01:12:42,080 --> 01:12:48,630
runs a stage on a partition

1863
01:12:48,640 --> 01:12:50,800
so basically all these partitions are

1864
01:12:50,800 --> 01:12:51,760
all these uh

1865
01:12:51,760 --> 01:12:53,440
stages running parallel on different

1866
01:12:53,440 --> 01:12:55,199
workers

1867
01:12:55,199 --> 01:12:57,600
then within a stage there's also

1868
01:12:57,600 --> 01:12:59,040
parallelism

1869
01:12:59,040 --> 01:13:02,480
uh because you know every filter uh

1870
01:13:02,480 --> 01:13:05,360
is pipelined uh with that you know they

1871
01:13:05,360 --> 01:13:05,760
mean

1872
01:13:05,760 --> 01:13:07,920
like you read maybe like the first n

1873
01:13:07,920 --> 01:13:09,440
records

1874
01:13:09,440 --> 01:13:12,719
and then you apply the filter operation

1875
01:13:12,719 --> 01:13:15,360
and then uh you know that produces you

1876
01:13:15,360 --> 01:13:16,840
know whatever

1877
01:13:16,840 --> 01:13:20,000
few m records

1878
01:13:20,000 --> 01:13:22,080
and you know then the next you know

1879
01:13:22,080 --> 01:13:24,000
filter you know process those

1880
01:13:24,000 --> 01:13:25,840
and records but while it's processing

1881
01:13:25,840 --> 01:13:27,040
those m records

1882
01:13:27,040 --> 01:13:30,480
the first filter reads the next end

1883
01:13:30,480 --> 01:13:32,400
and you know produces them and then

1884
01:13:32,400 --> 01:13:34,320
passes them on and then you know maybe

1885
01:13:34,320 --> 01:13:35,920
that results in some number of records

1886
01:13:35,920 --> 01:13:38,080
and again and you know it goes on and on

1887
01:13:38,080 --> 01:13:40,320
and so basically all these you know all

1888
01:13:40,320 --> 01:13:41,840
these oops not these

1889
01:13:41,840 --> 01:13:44,840
all these uh transformations are

1890
01:13:44,840 --> 01:13:48,070
pipelined

1891
01:13:48,080 --> 01:13:49,360
and so they're almost running

1892
01:13:49,360 --> 01:13:51,040
concurrently or they're running not

1893
01:13:51,040 --> 01:13:52,239
truly concurrently but they're running

1894
01:13:52,239 --> 01:13:54,400
in a pipeline fashion

1895
01:13:54,400 --> 01:13:57,040
oh i just said this is the batch thing

1896
01:13:57,040 --> 01:13:58,320
that they were talking about yeah

1897
01:13:58,320 --> 01:14:01,040
exactly so things are passed in veggies

1898
01:14:01,040 --> 01:14:02,320
and basically every stage of the bike

1899
01:14:02,320 --> 01:14:02,880
will improve

1900
01:14:02,880 --> 01:14:06,480
uh processes batch okay okay yeah

1901
01:14:06,480 --> 01:14:07,920
that makes it very clear yeah thank you

1902
01:14:07,920 --> 01:14:10,080
so much that was that was an interesting

1903
01:14:10,080 --> 01:14:11,199
lecture thank you

1904
01:14:11,199 --> 01:14:14,400
okay you're welcome glad you enjoyed it

1905
01:14:14,400 --> 01:14:23,990
it's a cool system okay

1906
01:14:24,000 --> 01:14:25,840
sorry sorry can you hear me now yeah

1907
01:14:25,840 --> 01:14:31,590
yeah i didn't hear

1908
01:14:31,600 --> 01:14:33,679
i'm sorry about that i yeah sorry i was

1909
01:14:33,679 --> 01:14:34,800
listening i um

1910
01:14:34,800 --> 01:14:37,920
um i i realized we'd be going light i'm

1911
01:14:37,920 --> 01:14:39,280
going to try to make this question very

1912
01:14:39,280 --> 01:14:39,760
quick

1913
01:14:39,760 --> 01:14:41,679
um i've gotten to smart because you're

1914
01:14:41,679 --> 01:14:43,920
addressing my schedule

1915
01:14:43,920 --> 01:14:47,280
thank you thank you so much

1916
01:14:47,280 --> 01:14:49,360
so yeah uh yeah i really really

1917
01:14:49,360 --> 01:14:50,560
appreciate this lecture

1918
01:14:50,560 --> 01:14:52,320
uh spark is actually something that i'm

1919
01:14:52,320 --> 01:14:54,239
gonna use in my future job so i

1920
01:14:54,239 --> 01:14:56,560
appreciate you teaching this to me um

1921
01:14:56,560 --> 01:14:59,040
just one problem i'm not sure this

1922
01:14:59,040 --> 01:14:59,679
lecture will

1923
01:14:59,679 --> 01:15:03,040
really help you writing spark programs

1924
01:15:03,040 --> 01:15:05,280
no i i mean i did it as an intern really

1925
01:15:05,280 --> 01:15:06,800
not knowing what i was doing

1926
01:15:06,800 --> 01:15:09,280
but like this has helped me give like

1927
01:15:09,280 --> 01:15:12,080
get more context with it

1928
01:15:12,080 --> 01:15:13,920
so uh i guess the the quick question

1929
01:15:13,920 --> 01:15:16,159
with uh spark programs that i

1930
01:15:16,159 --> 01:15:18,400
um the way that i've understood spark

1931
01:15:18,400 --> 01:15:19,440
jobs is

1932
01:15:19,440 --> 01:15:22,960
how spark constructs a directed a

1933
01:15:22,960 --> 01:15:27,270
directed acyclic graph of all the tasks

1934
01:15:27,280 --> 01:15:29,120
yeah yep and this is what you talked

1935
01:15:29,120 --> 01:15:30,719
about with the wide partitions and

1936
01:15:30,719 --> 01:15:32,400
narrow partitions how like

1937
01:15:32,400 --> 01:15:34,560
yeah so i guess they're called

1938
01:15:34,560 --> 01:15:36,480
dependencies not partitions but

1939
01:15:36,480 --> 01:15:39,520
oh yep yep sorry okay okay

1940
01:15:39,520 --> 01:15:42,640
uh okay sorry um that

1941
01:15:42,640 --> 01:15:45,840
with rdds then i guess like okay this is

1942
01:15:45,840 --> 01:15:48,640
different terminology between

1943
01:15:48,640 --> 01:15:51,040
the dependencies are like these tasks in

1944
01:15:51,040 --> 01:15:52,640
the directed acyclic graph

1945
01:15:52,640 --> 01:15:56,000
of all the tasks but the rdds that

1946
01:15:56,000 --> 01:15:59,040
like each task in this graph is not

1947
01:15:59,040 --> 01:16:00,080
represented by this

1948
01:16:00,080 --> 01:16:03,199
rdd let me actually go back so maybe

1949
01:16:03,199 --> 01:16:06,880
this picture is the right and oh gosh i

1950
01:16:06,880 --> 01:16:08,159
appreciate you staying here please let

1951
01:16:08,159 --> 01:16:09,440
me know if you have to go

1952
01:16:09,440 --> 01:16:11,040
no no no no no no that's fine i got some

1953
01:16:11,040 --> 01:16:13,280
more time oh yeah yeah all right so

1954
01:16:13,280 --> 01:16:16,400
um uh so okay

1955
01:16:16,400 --> 01:16:19,679
so this is sort of an rdd correct let me

1956
01:16:19,679 --> 01:16:23,120
draw another color so that we can agree

1957
01:16:23,120 --> 01:16:26,480
this is an rdd okay an rdd has a bunch

1958
01:16:26,480 --> 01:16:28,320
of partitions

1959
01:16:28,320 --> 01:16:33,030
and here's another rdd

1960
01:16:33,040 --> 01:16:36,080
all right okay yep and

1961
01:16:36,080 --> 01:16:38,800
uh the arrows are basically sort of like

1962
01:16:38,800 --> 01:16:39,840
here's the same story

1963
01:16:39,840 --> 01:16:41,840
let me actually uh finish this picture

1964
01:16:41,840 --> 01:16:43,440
too on the side here's some more

1965
01:16:43,440 --> 01:16:45,040
partitions

1966
01:16:45,040 --> 01:16:48,239
here's the rdds boom boom

1967
01:16:48,239 --> 01:16:51,440
rdd boom boom rdd and then there are

1968
01:16:51,440 --> 01:16:53,040
transformations basically between

1969
01:16:53,040 --> 01:16:56,960
rdds correct and so in the arrows

1970
01:16:56,960 --> 01:16:59,040
when you pick another color you know

1971
01:16:59,040 --> 01:17:00,960
these errors

1972
01:17:00,960 --> 01:17:04,159
there's our transformations

1973
01:17:04,159 --> 01:17:07,199
got you and this is this is like part of

1974
01:17:07,199 --> 01:17:09,360
the directed acyclic graph of the spark

1975
01:17:09,360 --> 01:17:10,000
job

1976
01:17:10,000 --> 01:17:12,000
every single transformation leads to

1977
01:17:12,000 --> 01:17:14,239
like creating another rdd

1978
01:17:14,239 --> 01:17:16,159
that makes sense that that makes sense

1979
01:17:16,159 --> 01:17:17,600
okay and then the only thing is like

1980
01:17:17,600 --> 01:17:19,280
some of these arrows

1981
01:17:19,280 --> 01:17:22,000
are white and some of them are narrow

1982
01:17:22,000 --> 01:17:23,040
and the graph

1983
01:17:23,040 --> 01:17:25,199
from the graph you can't really tell

1984
01:17:25,199 --> 01:17:27,199
whether which ones are narrower or which

1985
01:17:27,199 --> 01:17:28,400
ones are

1986
01:17:28,400 --> 01:17:29,920
uh which transformations are sort of

1987
01:17:29,920 --> 01:17:32,159
narrow transformations and

1988
01:17:32,159 --> 01:17:34,159
white transformations you're talking

1989
01:17:34,159 --> 01:17:36,000
about the graph that's the spark program

1990
01:17:36,000 --> 01:17:37,360
actually like shows you

1991
01:17:37,360 --> 01:17:40,000
in there uh yeah this lineage graph you

1992
01:17:40,000 --> 01:17:40,560
can't tell

1993
01:17:40,560 --> 01:17:43,760
she got you okay okay that's uh here so

1994
01:17:43,760 --> 01:17:44,000
here

1995
01:17:44,000 --> 01:17:46,719
if you look at this lineage graph uh

1996
01:17:46,719 --> 01:17:47,280
like

1997
01:17:47,280 --> 01:17:48,719
this transformation that transformation

1998
01:17:48,719 --> 01:17:49,679
that transformation all that

1999
01:17:49,679 --> 01:17:50,719
transformation they're all looking like

2000
01:17:50,719 --> 01:17:52,480
narrow because it's a single arrow

2001
01:17:52,480 --> 01:17:53,920
but it's not really true right like the

2002
01:17:53,920 --> 01:17:56,000
last one for example must be uh

2003
01:17:56,000 --> 01:17:59,199
white one because it will collect

2004
01:17:59,199 --> 01:18:00,400
information from all

2005
01:18:00,400 --> 01:18:03,520
right okay

2006
01:18:03,520 --> 01:18:05,920
uh then i i guess i was wondering like

2007
01:18:05,920 --> 01:18:06,719
do you have

2008
01:18:06,719 --> 01:18:10,480
uh recommendations on resources for

2009
01:18:10,480 --> 01:18:13,679
things that can show me how spark

2010
01:18:13,679 --> 01:18:16,000
figures out how to construct the

2011
01:18:16,000 --> 01:18:17,840
directed acyclic graph to

2012
01:18:17,840 --> 01:18:21,840
to do all these tasks yeah look at the

2013
01:18:21,840 --> 01:18:23,920
excess is really the scanner that does

2014
01:18:23,920 --> 01:18:25,199
all this right and

2015
01:18:25,199 --> 01:18:28,400
yeah reading the paper like i i you know

2016
01:18:28,400 --> 01:18:30,080
i was trying to comprehend the paper as

2017
01:18:30,080 --> 01:18:31,840
best as i could but it's you know it's

2018
01:18:31,840 --> 01:18:33,199
it it's difficult but

2019
01:18:33,199 --> 01:18:34,400
yeah i know all these papers are

2020
01:18:34,400 --> 01:18:36,400
difficult to read so the scheduler uh

2021
01:18:36,400 --> 01:18:38,480
i think i would go back first to uh

2022
01:18:38,480 --> 01:18:40,719
mate's visas these doctoral thesis i'm

2023
01:18:40,719 --> 01:18:42,000
sure he has a capital on the

2024
01:18:42,000 --> 01:18:44,640
schedule gotcha gotcha all right i'll

2025
01:18:44,640 --> 01:18:46,640
okay and that'll show me just how spark

2026
01:18:46,640 --> 01:18:50,000
figures out how to make this graph

