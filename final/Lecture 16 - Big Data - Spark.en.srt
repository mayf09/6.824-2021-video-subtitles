1
00:00:00,000 --> 00:00:03,450
We'll talking about is Spark,

2
00:00:03,840 --> 00:00:07,200
and so this goes back to almost the beginning of the semester,

3
00:00:07,620 --> 00:00:11,130
where you know we talked quite a bit about mapreduce,

4
00:00:11,130 --> 00:00:14,130
in fact you know lab 1 that you implemented mapreduce.

5
00:00:14,940 --> 00:00:19,650
So and really what's you know sort of informally,

6
00:00:19,650 --> 00:00:26,800
you know the Spark is basically the successor you know to Hadoop,

7
00:00:28,830 --> 00:00:35,930
and Hadoop is the open-source version of mapreduce.

8
00:00:38,610 --> 00:00:40,890
So I think today,

9
00:00:40,890 --> 00:00:45,960
you know people typically will use Spark instead of Hadoop.

10
00:00:46,640 --> 00:00:48,620
And so it's really widely used,

11
00:00:52,860 --> 00:00:55,950
and it's widely used for data science computation,

12
00:00:55,950 --> 00:00:58,650
so people that you know have lots of data,

13
00:00:58,680 --> 00:01:00,660
that need to run some computation over it,

14
00:01:00,840 --> 00:01:02,370
require a ton of machines,

15
00:01:02,550 --> 00:01:06,870
you know Spark is designed for that particular case,

16
00:01:07,230 --> 00:01:10,320
it is commercialized by a company called Databricks,

17
00:01:12,250 --> 00:01:15,910
you know Matei Zaharia who is the author,

18
00:01:15,910 --> 00:01:19,060
of the main author of this paper, his PhD thesis,

19
00:01:19,180 --> 00:01:20,980
started with a number of other people,

20
00:01:20,980 --> 00:01:25,510
this company Databricks which commercializes Spark,

21
00:01:25,540 --> 00:01:30,760
but it also supports the Apache open-source Spark version,

22
00:01:33,010 --> 00:01:35,590
it's a pretty popular open-source project

23
00:01:36,310 --> 00:01:39,220
or very popular open-source project.

24
00:01:39,400 --> 00:01:44,230
It is one reason as sort of replaced you know the use of Hadoop is,

25
00:01:44,230 --> 00:01:46,480
because it actually supports wider range,

26
00:01:47,610 --> 00:01:50,370
wider range of applications than mapreduce can,

27
00:01:52,410 --> 00:01:55,590
in particular you know very good at these iterative.

28
00:01:57,070 --> 00:01:57,910
Oops, what happened there,

29
00:02:00,800 --> 00:02:04,280
let me see, something maybe external crash,

30
00:02:04,310 --> 00:02:05,180
hold on a second.

31
00:02:36,270 --> 00:02:41,220
Okay, fortunately having good shape.

32
00:02:42,340 --> 00:02:45,580
Okay, so it supports a wide range of applications,

33
00:02:45,700 --> 00:02:48,010
in particular is good at these iterative applications,

34
00:02:48,010 --> 00:02:51,610
so applications where multiple rounds of mapreduce operations,

35
00:02:51,610 --> 00:02:52,990
so if you have an application,

36
00:02:52,990 --> 00:02:54,850
that requires one set of mapreduce

37
00:02:54,850 --> 00:02:58,480
followed by another set of mapreduce, followed by another mapreduce computation,

38
00:02:58,900 --> 00:03:00,130
Spark is really good at it,

39
00:03:00,400 --> 00:03:01,600
and the reason is so good at it is,

40
00:03:01,600 --> 00:03:04,510
because basically it keeps the intermediate results in memory,

41
00:03:04,510 --> 00:03:08,200
and that's really good support, programming support for doing so.

42
00:03:09,140 --> 00:03:13,550
In some ways, you know,

43
00:03:13,550 --> 00:03:17,180
so if there's any connection at all between the previous paper and this paper,

44
00:03:17,180 --> 00:03:18,350
which basically is not,

45
00:03:18,500 --> 00:03:23,420
but you know they're all both targeted to sort of in-memory computations,

46
00:03:28,000 --> 00:03:30,280
you know for datasets that basically fit in-memory,

47
00:03:30,850 --> 00:03:32,710
in the previous paper and FaRM paper

48
00:03:32,710 --> 00:03:35,260
is all about you know the database fit in-memory,

49
00:03:35,290 --> 00:03:38,410
here's the dataset the data science computation,

50
00:03:38,410 --> 00:03:40,150
for data science computation that you want to do.

51
00:03:42,860 --> 00:03:45,800
Of course, you know since 2012, when this paper was published,

52
00:03:45,800 --> 00:03:48,200
a lot of things have happened,

53
00:03:48,230 --> 00:03:53,340
the Spark is not really tied to Scala,

54
00:03:53,340 --> 00:03:54,780
sort of described in the paper,

55
00:03:54,780 --> 00:03:56,760
but there are other language front ends, for example,

56
00:03:57,240 --> 00:03:58,860
but probably more importantly,

57
00:03:58,860 --> 00:04:05,280
you know the RDD has defined in this paper slightly deprecated,

58
00:04:05,280 --> 00:04:10,320
and replace by dataframes,

59
00:04:10,440 --> 00:04:12,600
but dataframes the way to think about it,

60
00:04:12,600 --> 00:04:13,830
the way I think about it,

61
00:04:13,830 --> 00:04:18,180
this is basically column, is RDD with explicit columns,

62
00:04:18,540 --> 00:04:23,700
and all the good ideas of RDDs are also true for dataframes.

63
00:04:24,480 --> 00:04:26,310
And so for the rest of the lecture,

64
00:04:26,310 --> 00:04:28,380
I'm just gonna talk about RDDs,

65
00:04:28,380 --> 00:04:34,410
and think about them as equivalently as to dataframes.

66
00:04:36,540 --> 00:04:38,460
Any questions before I proceed?

67
00:04:46,200 --> 00:04:48,060
And then quick other point,

68
00:04:48,060 --> 00:04:51,180
maybe this research is really quite successful,

69
00:04:52,380 --> 00:04:53,340
widely used,

70
00:04:53,580 --> 00:04:59,550
also Matei you know received the ACM doctoral thesis award,

71
00:04:59,550 --> 00:05:03,060
for this for this thesis, that basically is all about Spark.

72
00:05:04,300 --> 00:05:07,660
So it's quite unusual actually for a doctoral thesis,

73
00:05:07,660 --> 00:05:09,040
that to have that kind of impact.

74
00:05:16,470 --> 00:05:17,040
Okay?

75
00:05:18,580 --> 00:05:21,940
So the way I want to talk about Spark

76
00:05:21,940 --> 00:05:24,910
by just looking at the some of examples,

77
00:05:24,940 --> 00:05:27,370
because I think you get the best,

78
00:05:27,370 --> 00:05:30,700
you, you understand the programming model,

79
00:05:31,960 --> 00:05:33,760
but that is based on RDDs,

80
00:05:34,240 --> 00:05:36,910
best by this I think looking at examples,

81
00:05:40,330 --> 00:05:43,600
and you get the best idea of what actually in RDD is.

82
00:05:44,050 --> 00:05:46,150
And so let me pull up some of the examples,

83
00:05:46,150 --> 00:05:47,200
that were in the paper,

84
00:05:48,070 --> 00:05:49,870
then we'll walk through those.

85
00:05:51,260 --> 00:05:53,360
Take this one,

86
00:05:53,960 --> 00:05:58,090
so let's start here, a very simple example.

87
00:05:58,600 --> 00:06:02,110
And so you know the idea is that,

88
00:06:02,720 --> 00:06:04,670
in this example is that,

89
00:06:04,670 --> 00:06:09,140
you know first of all, you can sort of use spark interactively,

90
00:06:09,170 --> 00:06:12,980
so you can see that start of Spark at the workstation or laptop,

91
00:06:13,280 --> 00:06:16,670
and start interacting with Spark,

92
00:06:16,940 --> 00:06:20,990
you know the way you know you type in commands like this.

93
00:06:21,480 --> 00:06:23,550
And so now what does this command do,

94
00:06:23,580 --> 00:06:26,610
you know this basically creates an RDD,

95
00:06:26,610 --> 00:06:30,690
you know called this RDDs, lines is an RDD,

96
00:06:30,990 --> 00:06:35,700
and I just represents you know the RDD that actually stored in HDFS,

97
00:06:35,940 --> 00:06:41,410
and you know the HDFS you know might be have many partitions

98
00:06:41,410 --> 00:06:42,850
you know for this particular file,

99
00:06:42,880 --> 00:06:47,560
so for example the first thousand or million records live on like partition 1,

100
00:06:47,560 --> 00:06:50,170
there's next million live in partition 2,

101
00:06:50,170 --> 00:06:52,090
and the next million live to partition 3,

102
00:06:52,630 --> 00:06:58,630
then this RDD lines basically represents you know that that set of partitions,

103
00:07:00,070 --> 00:07:04,960
when you run this line or you type in this line in return, hit return,

104
00:07:04,990 --> 00:07:06,460
basically nothing really happens,

105
00:07:06,580 --> 00:07:11,500
and so this is what the paper refers to as lazy computations,

106
00:07:12,010 --> 00:07:13,960
in fact, computations executed some point later,

107
00:07:13,960 --> 00:07:15,550
and we'll see in a second when,

108
00:07:15,880 --> 00:07:17,650
but at this particular point,

109
00:07:17,740 --> 00:07:19,660
the only thing that's actually happened is that,

110
00:07:19,660 --> 00:07:23,650
there is a lines lines object that happens to be in RDD.

111
00:07:24,640 --> 00:07:29,860
And RDD you know supports a wide range of operations,

112
00:07:30,070 --> 00:07:33,460
we can actually look a little bit with some operations that it supports.

113
00:07:37,440 --> 00:07:41,610
Yeah, so it's has, an RDD has an API,

114
00:07:41,610 --> 00:07:44,070
and it turns out that the methods of the API

115
00:07:44,100 --> 00:07:46,980
or the methods on RDD fail in two classes,

116
00:07:47,340 --> 00:07:50,340
one are the, are Actions,

117
00:07:50,770 --> 00:07:55,540
and Actions are really the operations that will actually cause computation to happen,

118
00:07:56,380 --> 00:07:59,050
so all the lazily sort of build up computation

119
00:07:59,140 --> 00:08:01,060
really happens at the point that you're run an Action,

120
00:08:01,210 --> 00:08:03,640
and so for example, you run count or collect,

121
00:08:03,850 --> 00:08:08,590
then, the Spark computation actually is executed.

122
00:08:09,780 --> 00:08:14,250
All the other API or methods are Transformations,

123
00:08:14,580 --> 00:08:18,630
and they basically take one RDD and turn it into another RDD,

124
00:08:18,780 --> 00:08:22,980
it turns out that every RDD is actually read-only or immutable,

125
00:08:23,470 --> 00:08:26,480
so you can't modify an RDD,

126
00:08:26,600 --> 00:08:31,520
can only basically generate new RDDs form an existing one.

127
00:08:31,910 --> 00:08:33,440
And so if we look at the second line,

128
00:08:33,530 --> 00:08:37,070
this basically creates a second RDD, RDD errors,

129
00:08:37,550 --> 00:08:43,430
and that one is created by running a filter on the lines RDD,

130
00:08:44,240 --> 00:08:48,200
so the lines RDD read-only, read-only,

131
00:08:49,220 --> 00:08:51,080
you can run this method the filter on it,

132
00:08:51,080 --> 00:08:51,830
which in this case,

133
00:08:51,830 --> 00:08:54,920
basically filters out all the records start with,

134
00:08:54,920 --> 00:08:58,490
all the lines that start with the message ERROR or string ERROR,

135
00:08:58,730 --> 00:09:00,560
and that represents a new RDD,

136
00:09:00,800 --> 00:09:04,490
and again at this point, nothing actually really being computed,

137
00:09:04,490 --> 00:09:08,300
is just like a recipe or you know built some almost a dataflow

138
00:09:08,390 --> 00:09:12,470
or what the paper calls iterative graph of the computation.

139
00:09:22,040 --> 00:09:25,460
Also, when the computation actually starts running,

140
00:09:25,550 --> 00:09:26,420
it hasn't run yet,

141
00:09:26,420 --> 00:09:27,980
but when it will start running,

142
00:09:28,160 --> 00:09:31,100
these operations are pipelined,

143
00:09:31,100 --> 00:09:33,260
with that, they mean,

144
00:09:33,260 --> 00:09:37,730
so for example in stage one you know the this computation of the lines,

145
00:09:37,880 --> 00:09:41,150
the stage one will read some set of records,

146
00:09:41,150 --> 00:09:43,940
you know from for example this first partition,

147
00:09:44,410 --> 00:09:47,920
and then, do its processing on it,

148
00:09:47,920 --> 00:09:50,410
if there's anything and then handed off to stage two,

149
00:09:50,530 --> 00:09:53,350
you know what stage two basically you know doing this filter,

150
00:09:53,890 --> 00:10:00,550
and so in stage two, you know the, the, this filter will run,

151
00:10:00,670 --> 00:10:05,440
and sort of grab out the lines that actually are match,

152
00:10:05,770 --> 00:10:09,520
and you know basically produce with that new RDD,

153
00:10:09,610 --> 00:10:13,540
that just contains you know the lines you know strings that start with,

154
00:10:13,600 --> 00:10:15,160
lines that start with the string ERROR.

155
00:10:16,670 --> 00:10:20,720
And while this sort of second RDD, second stage runs,

156
00:10:20,720 --> 00:10:24,620
you know the first stage you know grab the next set of records from the file system,

157
00:10:24,950 --> 00:10:27,140
and then you know fetch them again to stage two,

158
00:10:27,170 --> 00:10:29,660
and so as you go further and further,

159
00:10:29,660 --> 00:10:31,970
you have more and more stages in your pipeline,

160
00:10:31,970 --> 00:10:34,970
or your iterative graph,

161
00:10:35,060 --> 00:10:38,480
and all those stages are going to be running sort of in parallel,

162
00:10:38,850 --> 00:10:43,030
and that's what I mean with [] pipelining the transformations.

163
00:10:45,990 --> 00:10:51,830
Okay, so, so here this this you know this line basically describes,

164
00:10:51,830 --> 00:10:56,720
how to create RDD error, RDD, errors RDD,

165
00:10:56,870 --> 00:10:59,540
and then this lines basically [],

166
00:10:59,570 --> 00:11:04,010
tells Spark to basically keep a copy of this RDD in memory,

167
00:11:04,500 --> 00:11:11,190
so if subsequent computation run that you do more stuff with errors,

168
00:11:11,430 --> 00:11:17,070
Spark will actually keep the original RDD actually in memory,

169
00:11:17,070 --> 00:11:19,830
so that, it can be shared with later computation,

170
00:11:19,830 --> 00:11:22,230
so for example if you wanted to reuse the error file,

171
00:11:22,530 --> 00:11:25,230
then that error file will be in memory,

172
00:11:25,230 --> 00:11:27,600
doesn't have to be reconstructed from the files,

173
00:11:27,600 --> 00:11:30,030
actually represent the maybe HDFS

174
00:11:30,120 --> 00:11:33,690
and allows you to run the second computation.

175
00:11:34,920 --> 00:11:38,100
And here for example one already, you know even in this simple example,

176
00:11:38,100 --> 00:11:41,130
you'll see there's a big difference between this and mapreduce,

177
00:11:41,400 --> 00:11:44,940
where at the mapreduce job and you run the computation, it ends,

178
00:11:45,150 --> 00:11:48,630
and then, if you want to redo something with the data,

179
00:11:48,720 --> 00:11:51,210
you have to reread it in from the file system,

180
00:11:51,210 --> 00:11:54,540
and using this sort of persist method,

181
00:11:54,750 --> 00:12:00,090
Spark can avoid you know having to reread that data from the disk,

182
00:12:00,090 --> 00:12:01,230
and you know save a lot of time.

183
00:12:06,110 --> 00:12:07,190
Any questions so far?

184
00:12:08,480 --> 00:12:12,410
And so when the error file gets extracted from P1, let's say,

185
00:12:12,410 --> 00:12:16,310
and then there's another error file that gets extracted from P2,

186
00:12:16,400 --> 00:12:18,890
so my understanding is that, this happened in parallel?

187
00:12:18,920 --> 00:12:20,600
Yes, so you can think about it,

188
00:12:20,600 --> 00:12:24,290
like you know they're going to be many worker, like in mapreduce,

189
00:12:24,620 --> 00:12:26,750
and the workers work on each partition,

190
00:12:27,940 --> 00:12:31,450
basically, you know the scheduler will sent a job

191
00:12:31,450 --> 00:12:32,770
you know to each of the workers,

192
00:12:32,860 --> 00:12:38,050
and a job is a task you know pertains to a particular partition,

193
00:12:38,230 --> 00:12:44,290
and workers start working on get one of these tasks

194
00:12:44,290 --> 00:12:45,100
and basically start running.

195
00:12:45,710 --> 00:12:48,110
So you get parallelism between the partitions,

196
00:12:48,320 --> 00:12:50,750
and you get parallelism between the stages in the pipeline.

197
00:12:52,580 --> 00:12:53,540
I see, thank you.

198
00:12:57,220 --> 00:12:57,580
What's the.

199
00:12:57,640 --> 00:12:58,030
Sorry.

200
00:12:58,030 --> 00:12:58,750
Can you hear me.

201
00:12:58,900 --> 00:13:04,480
What's the difference between lineage and the like just the log of transactions,

202
00:13:04,480 --> 00:13:05,350
that we've seen before,

203
00:13:05,350 --> 00:13:08,260
like is it just the granularity of the operations?

204
00:13:09,000 --> 00:13:12,930
As we see, lineage, log is strictly linear, right,

205
00:13:12,930 --> 00:13:14,670
and the examples that we've seen so far,

206
00:13:14,670 --> 00:13:17,820
the lineage is also linear,

207
00:13:17,820 --> 00:13:20,610
but we'll see later examples where with forks,

208
00:13:21,200 --> 00:13:26,870
where one stage depends on multiple different RDDs,

209
00:13:26,870 --> 00:13:29,180
and you know that's not representative in the log.

210
00:13:31,350 --> 00:13:33,810
You know there share some similarities in the sense,

211
00:13:33,810 --> 00:13:36,270
that like you're starting the beginning state,

212
00:13:36,270 --> 00:13:38,040
all the operations are deterministic,

213
00:13:38,040 --> 00:13:39,540
and then you'll end up in some,

214
00:13:39,660 --> 00:13:41,100
if you apply all those operations,

215
00:13:41,100 --> 00:13:44,040
will end in some deterministic end state,

216
00:13:44,370 --> 00:13:46,800
so in that sense, there's some similarity,

217
00:13:46,800 --> 00:13:49,230
but you know I think they're quite different.

218
00:13:51,870 --> 00:13:53,400
I also have a question,

219
00:13:53,400 --> 00:13:55,860
in this case filter,

220
00:13:56,910 --> 00:14:00,030
so it works by just applying filter on each partition,

221
00:14:00,030 --> 00:14:04,170
but sometimes, like I see the transformation also contain join or sort.

222
00:14:04,170 --> 00:14:05,940
Yeah, let's talk about a little bit, I will talk about sort and join in a second,

223
00:14:09,700 --> 00:14:12,520
they're clearly much more complicated.

224
00:14:13,620 --> 00:14:19,290
So is, do we, but like this persist is when we start computing?

225
00:14:19,530 --> 00:14:22,830
No, nothing has been computed yet, still all descriptions,

226
00:14:22,830 --> 00:14:24,000
so let's talk a little bit further.

227
00:14:24,880 --> 00:14:29,170
Let's look at actually something that actually generates the computation.

228
00:14:32,730 --> 00:14:38,730
So, here are two commands that actually result in computation,

229
00:14:38,730 --> 00:14:41,100
so this command will result in computation,

230
00:14:41,100 --> 00:14:43,920
because it contains count, which is an action,

231
00:14:44,070 --> 00:14:48,240
and this command will result in a computation, collect is an action.

232
00:14:50,210 --> 00:14:54,140
And so yeah and so you can,

233
00:14:54,560 --> 00:14:56,120
so we look like,

234
00:14:56,120 --> 00:14:57,920
so the reason that they show two commands,

235
00:14:57,920 --> 00:15:00,500
because they demonstrate that basically you can reuse errors,

236
00:15:00,800 --> 00:15:05,390
and so if you look at you know this computation,

237
00:15:05,720 --> 00:15:08,930
you know then you can draw the lineage graph, right,

238
00:15:08,930 --> 00:15:10,940
so start with lines,

239
00:15:12,510 --> 00:15:15,870
there was a filter that we ran,

240
00:15:18,450 --> 00:15:20,010
oops, sorry,

241
00:15:20,040 --> 00:15:22,050
let me write slightly differently,

242
00:15:22,050 --> 00:15:23,460
there was a filter on lines,

243
00:15:24,260 --> 00:15:26,030
that we just saw that produces errors,

244
00:15:28,630 --> 00:15:30,400
or that's a description, how to get errors,

245
00:15:30,700 --> 00:15:33,670
then there's in this case, there's another filter,

246
00:15:34,580 --> 00:15:36,380
it's the filter of HDFS,

247
00:15:37,400 --> 00:15:39,440
and that basically produces another RDD,

248
00:15:39,470 --> 00:15:41,810
you know that RDD is not explicitly named here,

249
00:15:42,020 --> 00:15:44,630
but it does produce another RDD,

250
00:15:44,630 --> 00:15:47,000
so I was going to call it hdfs,

251
00:15:47,750 --> 00:15:49,640
because the filter on HDFS,

252
00:15:50,210 --> 00:15:53,180
and then we see there's a map,

253
00:15:53,820 --> 00:15:55,950
and that produces yet another RDD,

254
00:15:55,980 --> 00:16:02,040
again this RDD doesn't really have a name here, but so it's anonymous,

255
00:16:02,070 --> 00:16:04,260
I'm just gonna give it a name, that's time,

256
00:16:05,760 --> 00:16:11,520
because basically splits the one either line into three pieces

257
00:16:11,520 --> 00:16:13,470
and grabs the third piece out of that line

258
00:16:13,470 --> 00:16:15,150
and that happens to be the time,

259
00:16:15,740 --> 00:16:21,920
and then, there's a final you know operation and that's collect,

260
00:16:22,190 --> 00:16:26,630
which actually counts up all the, the number of times, that time actually appears

261
00:16:26,660 --> 00:16:32,600
or the number of entries basically result of the in the RDD produced, in the time RDD.

262
00:16:33,470 --> 00:16:34,070
Okay?

263
00:16:34,400 --> 00:16:37,550
So this, so this is actually this point,

264
00:16:37,550 --> 00:16:40,550
when you know the return return here,

265
00:16:40,550 --> 00:16:45,980
in the user interface or in the interactive user interface it is,

266
00:16:45,980 --> 00:16:47,240
and at that point,

267
00:16:47,240 --> 00:16:53,570
basically Spark you know will collect you know a lot of a set of workers,

268
00:16:53,870 --> 00:16:56,840
split you know send them the jobs,

269
00:16:56,840 --> 00:17:01,160
or basically inform the scheduler that job needs to be executed,

270
00:17:01,430 --> 00:17:06,200
and the description of the task that needs to be executed this lineage graph.

271
00:17:13,400 --> 00:17:16,550
And so we can sort of think a little bit about exactly how the execution happens,

272
00:17:16,550 --> 00:17:17,720
so let me draw a picture,

273
00:17:22,190 --> 00:17:23,450
so the picture as follows,

274
00:17:23,450 --> 00:17:25,820
we got there's what's called the driver, that's the usual thing,

275
00:17:26,790 --> 00:17:29,610
you know where the program that the user typed into,

276
00:17:30,000 --> 00:17:37,080
it start of collecting a bunch of workers, you know a bunch of machines,

277
00:17:37,800 --> 00:17:41,160
almost the same way like as in mapreduce.

278
00:17:42,460 --> 00:17:45,340
And you know there's gonna be a HDFS,

279
00:17:45,490 --> 00:17:47,410
there's lines file,

280
00:17:47,410 --> 00:17:55,760
that actually has partitions are P1, P2 whatever, blah blah blah.

281
00:17:55,790 --> 00:17:59,660
And typically the number of partitions is larger than the number of workers,

282
00:17:59,660 --> 00:18:00,590
sorry, number of,

283
00:18:01,190 --> 00:18:03,230
yes, number of partitions is larger than the number of workers,

284
00:18:03,230 --> 00:18:05,300
you know get load balance,

285
00:18:06,020 --> 00:18:08,600
is like one partition is small and the other one is big,

286
00:18:08,630 --> 00:18:11,330
and you don't want to have workers are relying sitting around idle.

287
00:18:11,940 --> 00:18:13,920
And basically the scheduler,

288
00:18:14,400 --> 00:18:20,910
you know there's a scheduler runs you know basically the computation,

289
00:18:20,910 --> 00:18:24,210
and it has you know the information that has the lineage graphs.

290
00:18:25,820 --> 00:18:27,500
And so worker is basically check in,

291
00:18:27,590 --> 00:18:32,370
you know the driver [exercising] the code, Spark program,

292
00:18:32,550 --> 00:18:34,260
and we just constructed,

293
00:18:34,500 --> 00:18:36,450
and the worker should go basically to the scheduler,

294
00:18:36,450 --> 00:18:38,610
say, please you know which partition should I work.

295
00:18:39,240 --> 00:18:45,860
And, then they run basically part of the pipeline,

296
00:18:46,780 --> 00:18:47,890
and so we look at this,

297
00:18:47,890 --> 00:18:49,360
let me actually draw this slightly different,

298
00:18:49,360 --> 00:18:50,830
so I have a little bit more space here.

299
00:18:52,600 --> 00:18:54,580
So we saw there was a whole bunch of stages,

300
00:18:54,970 --> 00:18:56,800
and then the last stage was,

301
00:18:57,250 --> 00:19:00,310
the last operate was to collect stage.

302
00:19:02,560 --> 00:19:08,350
So in this, in this scenario that we just looked at,

303
00:19:08,440 --> 00:19:14,410
the collect stage, of course, needs to collect data from all the partitions, right,

304
00:19:14,410 --> 00:19:15,940
so in principle,

305
00:19:16,120 --> 00:19:18,310
you know let's draw a green line,

306
00:19:18,520 --> 00:19:20,140
basically everything of this,

307
00:19:20,600 --> 00:19:23,510
it was executed on an independent partition,

308
00:19:24,100 --> 00:19:28,150
so every worker gets one of these tasks from the scheduler,

309
00:19:28,180 --> 00:19:30,580
runs you know the thing

310
00:19:30,580 --> 00:19:36,400
and produces in the end a time RDD,

311
00:19:37,270 --> 00:19:43,420
and when the scheduler you know determines that basically all the time,

312
00:19:43,660 --> 00:19:45,160
you know like all these stages,

313
00:19:45,160 --> 00:19:46,270
this is called the stage,

314
00:19:48,080 --> 00:19:49,880
if all the stages have completed,

315
00:19:49,910 --> 00:19:52,850
and so all the time partitions have been produced,

316
00:19:52,970 --> 00:19:58,700
then it actually will run you know the collect action to basically do the addition

317
00:19:58,940 --> 00:20:02,480
and retrieve information from every partition,

318
00:20:02,690 --> 00:20:05,330
to actually compute the collect,

319
00:20:05,330 --> 00:20:07,370
or actually this is not collect,

320
00:20:07,370 --> 00:20:11,520
it was a count, sorry about that.

321
00:20:15,500 --> 00:20:18,440
And so one sort of way to think about this is,

322
00:20:18,440 --> 00:20:21,680
that this is sort of like almost like a mapreduce,

323
00:20:21,680 --> 00:20:22,970
where you have the map phase,

324
00:20:22,970 --> 00:20:25,070
and then you have a shuffle,

325
00:20:25,220 --> 00:20:26,960
and then you run the reduce phase,

326
00:20:26,960 --> 00:20:32,060
and the count is almost similar in that fashion,

327
00:20:32,180 --> 00:20:34,670
and in the paper, the way they refer to this is,

328
00:20:34,790 --> 00:20:38,330
this dependency, they call this a wide dependency,

329
00:20:39,230 --> 00:20:44,090
because the action or the transformation is dependent on a number of partitions,

330
00:20:44,540 --> 00:20:48,470
and these are called narrow, narrow dependencies,

331
00:20:48,740 --> 00:20:53,990
because, this RDD,

332
00:20:53,990 --> 00:20:57,020
to make that RDD is only dependent on one another,

333
00:20:57,320 --> 00:21:01,580
only at the, only dependent on the parent partition,

334
00:21:01,670 --> 00:21:04,730
only one parent partition to actually be able to compute it.

335
00:21:05,970 --> 00:21:10,020
So generally you you would prefer computation have narrow dependencies,

336
00:21:10,020 --> 00:21:12,600
because they can just run locally without any communication,

337
00:21:12,960 --> 00:21:17,010
before wide dependency, you might have to collect,

338
00:21:17,010 --> 00:21:20,670
you might have to collect partitions from the parent,

339
00:21:20,670 --> 00:21:25,350
or you may have to collect partitions from the parent RDD from all the machines.

340
00:21:25,840 --> 00:21:27,370
Professor.

341
00:21:27,400 --> 00:21:28,210
Yeah.

342
00:21:28,240 --> 00:21:29,620
I had a question,

343
00:21:29,620 --> 00:21:34,900
so in the paper, it says narrow dependencies,

344
00:21:35,470 --> 00:21:39,130
where each partition of the parent RDD

345
00:21:39,490 --> 00:21:43,450
is used by at most one partition of the child RDD,

346
00:21:43,870 --> 00:21:48,730
but it doesn't say anything about the the control,

347
00:21:49,300 --> 00:21:50,800
the [] is like,

348
00:21:51,250 --> 00:22:00,280
it doesn't say like a child partition needs to use at most one, one parent.

349
00:22:00,550 --> 00:22:02,290
Yeah, that's correct, because then.

350
00:22:02,290 --> 00:22:05,680
Yeah, if a child uses multiple parent partition,

351
00:22:05,680 --> 00:22:07,060
then it's a wide dependency.

352
00:22:08,310 --> 00:22:09,720
If a parent, sorry.

353
00:22:10,200 --> 00:22:14,790
If, if the child meets the partition,

354
00:22:14,790 --> 00:22:17,820
if the partition for multiple parent partitions,

355
00:22:18,030 --> 00:22:19,980
then it's a wide dependency.

356
00:22:20,750 --> 00:22:23,480
So, for example in the count case, correct,

357
00:22:23,510 --> 00:22:28,310
you know you have time, time partitions.

358
00:22:29,100 --> 00:22:29,580
Right.

359
00:22:30,770 --> 00:22:35,030
And the count operation is going to collect you know data from all of them.

360
00:22:35,720 --> 00:22:36,380
Right.

361
00:22:36,800 --> 00:22:38,570
So if count where at RDD,

362
00:22:39,110 --> 00:22:40,940
it isn't like, it's just an action,

363
00:22:41,090 --> 00:22:42,170
but even we're in RDD,

364
00:22:42,170 --> 00:22:46,220
then basically you know that would require interaction with all the parents.

365
00:22:46,780 --> 00:22:48,220
What I'm saying is,

366
00:22:48,220 --> 00:22:51,520
I think I think it's just the opposite, right,

367
00:22:52,020 --> 00:22:54,600
I I think this might like,

368
00:22:55,600 --> 00:22:59,560
I mean I I I I was actually confused,

369
00:23:00,700 --> 00:23:03,880
like with the paper, on this specific issue,

370
00:23:03,880 --> 00:23:05,620
but as it says,

371
00:23:05,680 --> 00:23:11,470
like each partition of the parent RDD is used by at most one partition of the child,

372
00:23:12,760 --> 00:23:16,570
but it doesn't say one partition of the child uses it most.

373
00:23:17,920 --> 00:23:21,790
I'm not actually sure you know exactly why you're confused with this,

374
00:23:21,790 --> 00:23:25,090
so let me know when we can postpone this and come back to it.

375
00:23:25,740 --> 00:23:26,430
Sure.

376
00:23:26,430 --> 00:23:29,670
I think the key thing observes basically two types of dependency,

377
00:23:29,670 --> 00:23:31,140
wide ones and narrow ones,

378
00:23:31,410 --> 00:23:34,770
and wide ones basically you know basically involve communication,

379
00:23:34,770 --> 00:23:40,620
because they have to talk to the,

380
00:23:41,010 --> 00:23:44,310
collect the information from the from the parent partitions.

381
00:23:45,990 --> 00:23:47,040
Okay?

382
00:23:47,430 --> 00:23:47,910
Thanks.

383
00:23:50,100 --> 00:23:52,020
I actually have a question on interface,

384
00:23:53,280 --> 00:23:55,260
like the previous one or two slides,

385
00:23:55,260 --> 00:23:58,200
what happens, if we don't call errors.persist?

386
00:23:59,610 --> 00:24:02,100
If you do not, then,

387
00:24:02,100 --> 00:24:05,880
the you were the second computation,

388
00:24:06,090 --> 00:24:11,900
like this computation would re-compute errors from the beginning,

389
00:24:12,530 --> 00:24:15,020
so if you run this workload,

390
00:24:15,850 --> 00:24:19,690
the Spark you know computation would re-compute errors,

391
00:24:19,690 --> 00:24:21,970
you know from a the starting file.

392
00:24:23,030 --> 00:24:23,990
Got it, thank you.

393
00:24:25,360 --> 00:24:28,510
So I actually have a question about this point,

394
00:24:28,570 --> 00:24:32,290
so for the partitions that we don't call persist on,

395
00:24:32,590 --> 00:24:34,450
in the mapreduce case,

396
00:24:34,450 --> 00:24:37,210
we basically stored them in intermediate files,

397
00:24:37,240 --> 00:24:42,610
but we nonetheless stored them in a local file system,

398
00:24:42,640 --> 00:24:44,170
let's say in the case of mapreduce,

399
00:24:44,350 --> 00:24:47,650
do we actually store intermediate files here,

400
00:24:47,650 --> 00:24:50,050
that we don't persist in some persistent storage,

401
00:24:50,050 --> 00:24:54,070
or we just keep the whole flow in memory?

402
00:24:54,220 --> 00:24:56,770
By default, the whole flow is in memory,

403
00:24:56,770 --> 00:24:59,920
except you can provide to,

404
00:24:59,920 --> 00:25:02,200
there's one exception,

405
00:25:02,200 --> 00:25:04,870
that we'll talk about a little bit more in detail in a second, hopefully,

406
00:25:05,260 --> 00:25:10,640
which is you see this persist here,

407
00:25:11,090 --> 00:25:14,480
this persist here take another flag, I think it's called reliable,

408
00:25:15,570 --> 00:25:21,390
and then, that set is actually stored in HDFS, basically called this a checkpoint.

409
00:25:22,760 --> 00:25:23,570
I see, thank you.

410
00:25:26,930 --> 00:25:28,940
I have a quick question about the partitioning,

411
00:25:29,090 --> 00:25:34,670
that with partitioning, the RDDs initially,

412
00:25:34,670 --> 00:25:39,350
is it initially HDFS who partitions them

413
00:25:39,500 --> 00:25:43,460
for each worker to operate on is Spark handling.

414
00:25:43,460 --> 00:25:47,780
This lines are in HDFS,

415
00:25:47,780 --> 00:25:53,150
you know the partition is defined basically by the files directly in hdfs,

416
00:25:53,750 --> 00:25:55,340
you can repetition,

417
00:25:55,400 --> 00:25:59,390
and you'll see in a second that actually it might be the stages to do so,

418
00:25:59,540 --> 00:26:01,640
for example using this hash partition trick,

419
00:26:02,090 --> 00:26:06,530
and you can define also your own partitioner,

420
00:26:06,530 --> 00:26:10,400
there's a partitioner object or abstraction that you can supply.

421
00:26:12,880 --> 00:26:15,250
So, it's already handled by HDFS,

422
00:26:15,250 --> 00:26:16,960
but if you want to do it again to Spark,

423
00:26:16,960 --> 00:26:17,740
then you can.

424
00:26:19,180 --> 00:26:20,860
Of course, files are also created by,

425
00:26:21,010 --> 00:26:26,500
this files presumably here created by logging system that sits on the side,

426
00:26:26,500 --> 00:26:28,180
and just produce different partitions,

427
00:26:28,330 --> 00:26:30,340
or you know you can reshuffle if you want to.

428
00:26:31,060 --> 00:26:31,960
Makes sense, thank you.

429
00:26:35,280 --> 00:26:40,330
Okay, so,

430
00:26:41,130 --> 00:26:41,970
and so let me,

431
00:26:41,970 --> 00:26:44,160
so that's the execution model,

432
00:26:44,160 --> 00:26:47,400
and I want to talk a little bit about fault tolerance.

433
00:26:47,980 --> 00:26:49,690
And so let's go back to,

434
00:26:49,690 --> 00:26:51,280
this, this sort of fault tolerance

435
00:26:51,280 --> 00:26:53,650
and the thing that we worry about fault tolerance is that,

436
00:26:53,710 --> 00:26:58,740
maybe one of these workers you know might crash, right,

437
00:26:58,740 --> 00:27:02,070
then you know that worker had is computing some partition,

438
00:27:02,310 --> 00:27:04,170
and so we need to re-execute that,

439
00:27:04,680 --> 00:27:10,200
and that sort of basically this plans are the same as mapreduce, right,

440
00:27:10,440 --> 00:27:14,880
were if a worker crashes, we need to,

441
00:27:15,000 --> 00:27:17,730
in mapreduce, you know the map task need to be re-executed,

442
00:27:17,730 --> 00:27:20,370
and perhaps maybe reduce task has to be re-executed,

443
00:27:20,850 --> 00:27:23,100
now in here, the task is slightly more complicated,

444
00:27:23,130 --> 00:27:25,170
because basically like these stages,

445
00:27:25,440 --> 00:27:28,740
and so it means that, if one worker fails,

446
00:27:28,740 --> 00:27:30,240
we may have to re-compute the stage.

447
00:27:31,230 --> 00:27:33,960
So, let's talk a little bit more about that,

448
00:27:34,200 --> 00:27:36,360
that sort of perspective from fault tolerance, right,

449
00:27:36,360 --> 00:27:37,980
that's what we're trying to achieve.

450
00:27:39,600 --> 00:27:40,740
This is really different than,

451
00:27:40,740 --> 00:27:45,360
like the fail tolerance that were you implemented in lab 2 or 3 or Paxos,

452
00:27:45,360 --> 00:27:48,000
you know stable storage and all that kind of stuff,

453
00:27:48,300 --> 00:27:53,190
and here really what we're we're worried about is crash of worker,

454
00:27:56,270 --> 00:28:02,350
the worker loses its memory, lost memory,

455
00:28:04,040 --> 00:28:05,630
and that means loses partition,

456
00:28:10,720 --> 00:28:15,970
and you know later part of the computation are probably dependent on that partition

457
00:28:16,150 --> 00:28:21,400
and so we need to reread or re-compute this partition.

458
00:28:21,400 --> 00:28:24,130
And so the solution is exactly like in mapreduce,

459
00:28:24,310 --> 00:28:28,420
you know basically the scheduler notice at some point that doesn't get an answer,

460
00:28:29,360 --> 00:28:33,340
and then reruns the stage for that partition.

461
00:28:44,200 --> 00:28:46,240
And you know and what is the cool part,

462
00:28:46,270 --> 00:28:48,220
like exactly as it in mapreduce,

463
00:28:48,460 --> 00:28:51,880
all, if we look at all these Transformations that are sitting here in the API,

464
00:28:52,000 --> 00:28:54,100
all these transformations are functional,

465
00:28:57,400 --> 00:28:59,290
so they basically take one input,

466
00:28:59,380 --> 00:29:00,910
they take an RDD as an input,

467
00:29:00,910 --> 00:29:02,770
they produce another RDD as output,

468
00:29:03,040 --> 00:29:05,350
and just completely deterministic.

469
00:29:05,840 --> 00:29:08,450
And so like you know in mapreduce,

470
00:29:08,450 --> 00:29:12,380
you know these map and reduce are functional operations,

471
00:29:12,380 --> 00:29:18,140
if you restart a stage where a sequence of transformations from the same input,

472
00:29:18,230 --> 00:29:20,000
then you'll produce the same output,

473
00:29:20,210 --> 00:29:24,550
and so you recreate the same partition,

474
00:29:28,520 --> 00:29:30,320
recreate partitions sufficiently.

475
00:29:34,020 --> 00:29:34,590
Okay?

476
00:29:37,600 --> 00:29:39,970
Sorry, is this why they are immutable?

477
00:29:40,300 --> 00:29:42,700
This also the reason I think they're immutable is.

478
00:29:48,240 --> 00:29:50,250
Okay, there's one tricky case though,

479
00:29:50,430 --> 00:29:52,050
which I want to talk about,

480
00:29:52,500 --> 00:29:56,730
so this the fault tolerance is basically for the narrow case,

481
00:29:57,890 --> 00:30:01,160
it's the same as sort of as we saw before in mapreduce,

482
00:30:01,160 --> 00:30:05,270
but you know the tricky case is actually the wide dependencies.

483
00:30:12,670 --> 00:30:17,020
So let's say you know we have you know some transformations,

484
00:30:19,850 --> 00:30:25,040
and one of these transformations is dependent on the,

485
00:30:25,130 --> 00:30:26,060
you know this is like sort of,

486
00:30:26,240 --> 00:30:29,090
here is one worker, here is another worker and another worker,

487
00:30:31,240 --> 00:30:42,380
and one of these stages is actually dependent on a number of parent partitions,

488
00:30:42,890 --> 00:30:45,530
so let's say whatever maybe this is a join

489
00:30:45,530 --> 00:30:47,480
or we'll see later other operations,

490
00:30:48,170 --> 00:30:51,920
where you know we're actually collecting information for lots of partitions,

491
00:30:52,130 --> 00:30:56,330
and you know create a RDD from that,

492
00:30:56,540 --> 00:31:00,620
from that RDD that might be used again by a map or whatever keep going,

493
00:31:01,250 --> 00:31:06,080
and so now let's say you know we're we're worker and we crash here,

494
00:31:07,590 --> 00:31:10,530
then we need to reconstruct this RDD,

495
00:31:15,640 --> 00:31:17,500
and you know we sort of follow,

496
00:31:17,500 --> 00:31:21,160
that means that we have to, we could also recompute this RDD,

497
00:31:21,520 --> 00:31:23,830
to recompute this RDD on this worker,

498
00:31:23,890 --> 00:31:27,130
that means we also need the partitions on the other workers.

499
00:31:27,820 --> 00:31:37,210
And, so the and reconstruction the reexecution of a computation

500
00:31:37,210 --> 00:31:39,970
on a particular worker, a particular partition

501
00:31:40,000 --> 00:31:45,610
may actually result that actually these ones also need to be recomputed.

502
00:31:47,290 --> 00:31:49,780
Of course, you can do this partially in parallel,

503
00:31:49,780 --> 00:31:50,710
you can just ask you know,

504
00:31:50,710 --> 00:31:53,590
please you know start to recompute this guy, recompute that guy,

505
00:31:53,890 --> 00:31:58,270
and you know produce then the final RDD again,

506
00:31:58,270 --> 00:31:59,560
but you know certainly you know

507
00:31:59,560 --> 00:32:07,360
failure one worker might be result in the recomputation of many many partitions,

508
00:32:08,960 --> 00:32:13,580
and that sort of slightly, that could be slightly costly.

509
00:32:14,060 --> 00:32:16,040
And so the solution is,

510
00:32:16,100 --> 00:32:17,930
that as a programmer,

511
00:32:18,140 --> 00:32:26,690
you can say, you can actually checkpoint or persist RDDs on stable storage,

512
00:32:27,520 --> 00:32:29,020
and so you might decide,

513
00:32:29,020 --> 00:32:32,710
you know for example, this is an RDD that,

514
00:32:33,460 --> 00:32:36,400
then you don't want to recompute in the case of a failure,

515
00:32:36,400 --> 00:32:39,310
because it requires you know recomputing all the different partitions,

516
00:32:39,520 --> 00:32:42,070
you may want to checkpoint this RDD.

517
00:32:48,960 --> 00:32:50,610
And then you know this stage,

518
00:32:50,610 --> 00:32:54,390
when actually when this you know computation needs to be reexecuted,

519
00:32:54,600 --> 00:32:59,640
gonna actually read you know the results of the partitions from the checkpoint

520
00:32:59,760 --> 00:33:03,090
instead of actually having to recompute them from scratch.

521
00:33:03,450 --> 00:33:06,810
And so this is why spark supports checkpoints,

522
00:33:06,810 --> 00:33:13,720
and that's sort of their fault tolerance story for wide dependencies.

523
00:33:17,800 --> 00:33:18,850
Any questions about this?

524
00:33:21,000 --> 00:33:22,740
I I had one question,

525
00:33:25,290 --> 00:33:31,520
so there was, so you persist right just in general,

526
00:33:31,520 --> 00:33:34,580
but they, they also mentioned a RELIABLE flag.

527
00:33:34,700 --> 00:33:35,180
Hmm.

528
00:33:35,510 --> 00:33:36,680
So I was wondering,

529
00:33:37,320 --> 00:33:41,040
like what's the difference between just persisting and using a RELIABLE flag?

530
00:33:41,770 --> 00:33:44,500
Persist just means you're gonna keep that RDD in memory,

531
00:33:44,710 --> 00:33:46,090
and you're not going to throw it away,

532
00:33:47,200 --> 00:33:51,340
so you can reuse it in later computations in and use in memory,

533
00:33:51,730 --> 00:33:55,090
the checkpoint or RELIABLE flag basically means,

534
00:33:56,210 --> 00:34:00,920
you actually write a copy of the whole RDD to HDFS,

535
00:34:05,560 --> 00:34:08,410
and HDFS is a persistent or a stable storage file system.

536
00:34:12,170 --> 00:34:15,860
Is there a way to tell Spark to unpersist something,

537
00:34:16,130 --> 00:34:19,490
because otherwise like for example if you persist in RDD,

538
00:34:19,760 --> 00:34:21,920
and you do a lot of computations,

539
00:34:22,400 --> 00:34:25,010
but like those later computations never use the RDD,

540
00:34:26,080 --> 00:34:29,050
you might just have it sticking around in memory forever.

541
00:34:29,780 --> 00:34:33,020
Yeah, I get, I I presume you can,

542
00:34:33,140 --> 00:34:37,190
or there's a general strategy that Spark uses,

543
00:34:37,190 --> 00:34:38,630
they talk a little bit about this,

544
00:34:38,900 --> 00:34:41,810
is that they have there's really no space anymore,

545
00:34:42,020 --> 00:34:48,200
they might spill some RDDs to HDFS or remove them,

546
00:34:48,230 --> 00:34:51,650
the the paper slightly vague on exactly you know what their plan for that is.

547
00:34:54,010 --> 00:34:54,520
Thank you.

548
00:34:55,000 --> 00:34:56,710
Of course, when the computation ends

549
00:34:56,740 --> 00:35:02,350
and user you logout, or you stop your driver,

550
00:35:02,350 --> 00:35:05,770
then I think you know those RDDs definitely gone from memory.

551
00:35:11,060 --> 00:35:11,660
Okay?

552
00:35:13,940 --> 00:35:17,990
Okay, so that is almost you know the story of Spark,

553
00:35:18,020 --> 00:35:22,970
the you know we've seen what a RDD is,

554
00:35:23,000 --> 00:35:26,120
we've seen how the execution works,

555
00:35:26,360 --> 00:35:28,910
and we've seen how the fault tolerance plan works.

556
00:35:29,210 --> 00:35:32,900
The thing that I want to talk about is another example,

557
00:35:32,960 --> 00:35:36,680
to really show off where Spark shines

558
00:35:36,890 --> 00:35:38,780
and that is an iterative example.

559
00:35:40,240 --> 00:35:42,310
So in computation that has an iterative structure,

560
00:35:46,060 --> 00:35:48,400
and the particular one I want to talk about the PageRank.

561
00:35:52,900 --> 00:35:57,820
I assume most of you are familiar with PageRank in some form,

562
00:35:58,060 --> 00:36:05,230
basically it's a plan to algorithm to give weight or important to web pages,

563
00:36:05,620 --> 00:36:10,210
and this dependent on the number of links that point to a particular web page,

564
00:36:10,210 --> 00:36:15,660
so for example if you have a web page U1, make points to itself,

565
00:36:16,110 --> 00:36:18,780
you have maybe a web page U3,

566
00:36:18,990 --> 00:36:20,310
so I'll use an example

567
00:36:20,310 --> 00:36:21,690
and web page U2,

568
00:36:22,050 --> 00:36:26,580
U2 has a link to itself and to U3,

569
00:36:26,820 --> 00:36:29,460
and maybe you know U3 has a link to U1.

570
00:36:30,310 --> 00:36:35,050
And basically PageRank is an algorithm based on these connectivities,

571
00:36:35,230 --> 00:36:38,170
computes you know the importance of a web page,

572
00:36:38,380 --> 00:36:43,510
and PageRank sort of the early one of the algorithms,

573
00:36:43,510 --> 00:36:46,270
that really drove the Google search machine,

574
00:36:46,330 --> 00:36:49,870
in the sense that, if you had a search result,

575
00:36:49,870 --> 00:36:51,520
the way you rank search result is that,

576
00:36:51,520 --> 00:36:55,150
if search result appears on a more important web page,

577
00:36:55,150 --> 00:36:58,150
you know that results gets promoted to higher in the list,

578
00:36:58,570 --> 00:37:01,810
and that was one of the reasons early days,

579
00:37:01,810 --> 00:37:05,560
Google search machine actually produce better search results,

580
00:37:05,560 --> 00:37:07,510
where the more important information actually

581
00:37:07,510 --> 00:37:09,130
or the more important web pages were on top.

582
00:37:11,150 --> 00:37:22,730
And so the paper talks about shows off the implementation of PageRank in Spark.

583
00:37:26,780 --> 00:37:34,340
So here's the implementation of the Spark implementation of the PageRank,

584
00:37:34,430 --> 00:37:38,390
and as before you know this is just a description,

585
00:37:38,390 --> 00:37:41,120
so we look at the individual lines,

586
00:37:42,200 --> 00:37:47,600
you know, these are just the recipe to actually how do compute PageRank,

587
00:37:47,810 --> 00:37:50,270
and only when like you know in this particular case,

588
00:37:50,270 --> 00:37:53,250
if you do ranks.collect at the very end,

589
00:37:53,730 --> 00:37:59,040
and then actually computation would run on you know the cluster of machines,

590
00:37:59,310 --> 00:38:03,480
and using sort of execution pattern that we've seen so far.

591
00:38:04,320 --> 00:38:08,880
And so I want to walk through this example a little bit more detail,

592
00:38:09,090 --> 00:38:10,740
to get a sense you know what,

593
00:38:10,740 --> 00:38:12,240
you know to get a better sense,

594
00:38:12,240 --> 00:38:14,700
why Spark are shines in the iterative case.

595
00:38:15,530 --> 00:38:20,150
So, there are two RDDs here,

596
00:38:20,150 --> 00:38:23,360
I'm also going to talk about some optimizations that are cool in Spark,

597
00:38:23,390 --> 00:38:25,730
one of these links RDDs,

598
00:38:25,730 --> 00:38:30,500
and links is basically represents the connection of the graphs,

599
00:38:30,500 --> 00:38:33,830
and so precisely, it probably has a line,

600
00:38:33,860 --> 00:38:38,300
I'm going to write it like that, as a line per URL,

601
00:38:38,300 --> 00:38:39,680
so here U1,

602
00:38:39,860 --> 00:38:44,540
it has two outgoing links, U1 U3,

603
00:38:51,620 --> 00:38:53,210
actually I missed one link here.

604
00:38:54,240 --> 00:38:59,140
And then you know the U2, entry for U2,

605
00:38:59,140 --> 00:39:01,930
which has going to U2 and U3.

606
00:39:04,660 --> 00:39:10,040
And there is an entry you know U3, going to U1.

607
00:39:13,030 --> 00:39:16,630
So this is basically a description of, you will, the world wide web,

608
00:39:17,080 --> 00:39:19,630
and of course you know my tiny little example,

609
00:39:19,630 --> 00:39:20,920
I have three web pages,

610
00:39:21,130 --> 00:39:24,370
but if you know were running this at the scale of Google,

611
00:39:24,370 --> 00:39:25,990
you would have a billion web pages, right,

612
00:39:26,410 --> 00:39:31,900
and so this file is gigantic and it's partitioned in partitions.

613
00:39:33,340 --> 00:39:34,720
So that's links.

614
00:39:35,240 --> 00:39:39,950
And then ranks is a similar file,

615
00:39:39,950 --> 00:39:42,620
that contains the current ranks for these web pages,

616
00:39:42,920 --> 00:39:44,540
and so you can think about these as,

617
00:39:44,540 --> 00:39:47,910
U1, you know comma you know it's rank,

618
00:39:48,150 --> 00:39:51,990
and let's assume the ranks initialized 1.0,

619
00:39:52,320 --> 00:39:54,840
and then you know here's 1.0,

620
00:39:55,510 --> 00:39:59,800
then U2 you know 1.0,

621
00:40:00,420 --> 00:40:03,510
U3 1.0.

622
00:40:04,970 --> 00:40:07,730
And we see actually that the links,

623
00:40:08,060 --> 00:40:11,240
the links are these is actually persisted in memory,

624
00:40:11,300 --> 00:40:13,940
that presumably in the same way as the error file,

625
00:40:13,940 --> 00:40:16,100
that we saw before, the error RDD.

626
00:40:17,840 --> 00:40:22,550
And then ranks, it's initialized to something,

627
00:40:22,550 --> 00:40:28,970
and then basically there's sort of description of number of iterations

628
00:40:28,970 --> 00:40:32,600
to produce new ranks RDD.

629
00:40:33,490 --> 00:40:36,700
You can see a little bit how this actually plays out,

630
00:40:37,240 --> 00:40:39,520
and one of the things we notice is that,

631
00:40:39,670 --> 00:40:42,070
links gets reused in every iteration,

632
00:40:43,190 --> 00:40:46,940
and links actually gets joined with ranks,

633
00:40:47,030 --> 00:40:50,210
what means, what does it mean to actually do this join up this,

634
00:40:50,210 --> 00:40:54,590
this, this operation, this operation basically creates an RDD,

635
00:40:55,010 --> 00:40:56,960
and we wonder how does the RDD look like,

636
00:40:56,960 --> 00:40:58,310
well that RDD is going to look like,

637
00:40:58,400 --> 00:41:05,260
U1, you know, and then the joining of the ranks in the, and the links file,

638
00:41:05,260 --> 00:41:10,390
so it's gonna be U1 U1 U2, because those are U3,

639
00:41:10,390 --> 00:41:13,700
outgoing links plus the rank for U1,

640
00:41:13,730 --> 00:41:17,150
which I'm going to go write as R1.

641
00:41:17,880 --> 00:41:20,010
And that's sort of the RDD that's being produced here,

642
00:41:20,070 --> 00:41:22,410
and so same thing for you know whatever U2,

643
00:41:22,410 --> 00:41:31,530
and the one for U3 is whatever U1 and you know R3 rank for 3,

644
00:41:32,540 --> 00:41:34,190
basically merges these two,

645
00:41:34,190 --> 00:41:37,130
literally joins the two files based on key.

646
00:41:38,120 --> 00:41:38,750
Okay?

647
00:41:39,860 --> 00:41:43,460
And then you know it runs a computation of flatMap on this,

648
00:41:43,460 --> 00:41:47,540
and that flatMap itself internally has a map over links,

649
00:41:47,750 --> 00:41:50,030
so basically it's going to run over,

650
00:41:50,060 --> 00:41:52,490
like the [] is going to run over these lists,

651
00:41:52,820 --> 00:41:59,180
and basically partition or divide up the rank you know to the outgoing URLs.

652
00:41:59,960 --> 00:42:04,820
And so it will you know create triples of the form,

653
00:42:05,090 --> 00:42:07,550
let me write this in green then,

654
00:42:07,580 --> 00:42:12,980
U1, R1 divided by 2,

655
00:42:15,120 --> 00:42:20,080
and U1 or U3 over the outgoing links,

656
00:42:20,080 --> 00:42:22,360
so give 1 to U1, 1 to U3,

657
00:42:22,360 --> 00:42:27,310
here's U3, R1, divided over 2, etc,

658
00:42:27,310 --> 00:42:28,990
it creates triples of this kind of form,

659
00:42:28,990 --> 00:42:30,610
so basically you know computes the way,

660
00:42:30,610 --> 00:42:33,880
divides that rank across the outgoing edges,

661
00:42:34,030 --> 00:42:39,850
and gives you know the values of these ranks you know to the outgoing edges,

662
00:42:39,850 --> 00:42:40,900
so the outgoing edges,

663
00:42:40,900 --> 00:42:44,770
so we got a big RDD, that has you know this form,

664
00:42:46,600 --> 00:42:50,890
that's produced basically, that is the contributions RDD.

665
00:42:53,130 --> 00:42:55,890
Then there's a final step in the,

666
00:42:55,920 --> 00:42:57,960
I think it first has to be reduceByKey,

667
00:42:58,200 --> 00:43:01,050
so basically grabs all the U1's you know together

668
00:43:01,230 --> 00:43:02,190
and then sums them up,

669
00:43:02,190 --> 00:43:03,990
so basically this will result is that,

670
00:43:03,990 --> 00:43:08,820
you know all the weights or the fractional weights,

671
00:43:08,820 --> 00:43:11,610
you know that U1 is going to receive are being added up,

672
00:43:12,030 --> 00:43:16,800
so, U1 of course is going to receive weight from itself,

673
00:43:16,920 --> 00:43:19,650
this one, is going to relate from U3,

674
00:43:20,380 --> 00:43:22,000
and so you know will sum up,

675
00:43:22,000 --> 00:43:25,610
there's going to be R1 divided by 2,

676
00:43:25,790 --> 00:43:28,160
and R3 divided by 1,

677
00:43:29,480 --> 00:43:32,720
and that was basically the sum, that's been created.

678
00:43:33,510 --> 00:43:35,670
And so that gives us a list of sums,

679
00:43:35,670 --> 00:43:37,050
and then that's when they are added up,

680
00:43:37,560 --> 00:43:39,240
and computed into a final value.

681
00:43:43,000 --> 00:43:45,340
It produces the new ranks RDD,

682
00:43:45,340 --> 00:43:46,960
which has the same shape,

683
00:43:46,990 --> 00:43:50,500
as the one that we saw before, namely this shape,

684
00:43:51,130 --> 00:43:54,070
for every web page, there is a number.

685
00:43:57,670 --> 00:43:58,390
Does this makes sense?

686
00:44:00,840 --> 00:44:05,070
And so, it's interesting to know that sort of description,

687
00:44:05,070 --> 00:44:07,170
so first of all, you can see that,

688
00:44:07,170 --> 00:44:10,020
actually the description of PageRank quite precise,

689
00:44:10,440 --> 00:44:11,940
and there's one of examples,

690
00:44:11,940 --> 00:44:15,450
this is like, if you had to run this in a mapreduce style,

691
00:44:15,450 --> 00:44:19,320
then that would mean that every iteration of this loop,

692
00:44:19,320 --> 00:44:20,310
at the end of the iteration,

693
00:44:20,310 --> 00:44:22,680
you would store the results in the file system,

694
00:44:23,010 --> 00:44:28,770
and then you reread you know the iteration from the result for the next iteration,

695
00:44:28,830 --> 00:44:33,570
and in in this Spark system,

696
00:44:33,660 --> 00:44:37,410
uses every iteration runs straight at the memory, leaves results in memory,

697
00:44:37,410 --> 00:44:39,390
so that the next iteration could pick it up right there,

698
00:44:40,050 --> 00:44:45,060
and furthermore you know these links file is shared among all the iterations.

699
00:44:46,380 --> 00:44:48,240
Okay, so to get a little bit more sense,

700
00:44:48,240 --> 00:44:49,110
like you know why,

701
00:44:49,110 --> 00:44:51,480
there's also a cool,

702
00:44:52,120 --> 00:44:53,470
the way to look at is

703
00:44:53,470 --> 00:44:57,750
to actually look at the lineage graph for this particular computation.

704
00:44:59,770 --> 00:45:02,620
So let's look at the lineage graph for,

705
00:45:06,440 --> 00:45:14,840
so here's the lineage graph for the for PageRank.

706
00:45:16,380 --> 00:45:19,080
And so a couple things I want to point out,

707
00:45:19,380 --> 00:45:21,960
first of all, so the lineage graph is like dynamic, almost [],

708
00:45:22,410 --> 00:45:23,010
oops,

709
00:45:27,010 --> 00:45:30,500
you know just keep growing you know with the number of iterations,

710
00:45:30,950 --> 00:45:36,140
and so as the scheduler you know basically computes does new stages,

711
00:45:36,140 --> 00:45:38,510
then, it keeps going,

712
00:45:38,510 --> 00:45:41,030
and you know we can see what the stages are gonna be, correct,

713
00:45:41,030 --> 00:45:48,930
because, you know sort of, you know every iteration of a loop is one stage,

714
00:45:48,930 --> 00:45:55,890
and will basically append you know parts of transformations to the lineage graph.

715
00:45:56,500 --> 00:45:59,890
So we see here's the input file, here's links,

716
00:46:00,220 --> 00:46:02,860
and as we saw the links actually created ones,

717
00:46:02,920 --> 00:46:04,450
then persisted in memory,

718
00:46:04,480 --> 00:46:06,430
where not in disk, persist in memory

719
00:46:06,610 --> 00:46:08,500
and is being reused many many times,

720
00:46:08,500 --> 00:46:12,250
like every loop iteration, basically the ranks being reused,

721
00:46:12,550 --> 00:46:14,650
and so again, you know compare it to mapreduce,

722
00:46:14,650 --> 00:46:16,720
if you have to write this to mapreduce style,

723
00:46:16,750 --> 00:46:17,920
you don't get that reuse,

724
00:46:17,950 --> 00:46:20,950
and so that's of course going to tremendously performance,

725
00:46:20,950 --> 00:46:23,470
because links, you know as we said before,

726
00:46:23,560 --> 00:46:25,510
it's like just gigantic file,

727
00:46:25,510 --> 00:46:30,250
that basically corresponds to one line for every web page in the in the universe.

728
00:46:32,100 --> 00:46:35,700
Another interesting thing to observe here is that,

729
00:46:35,850 --> 00:46:37,590
we see a wide dependencies here, right,

730
00:46:37,590 --> 00:46:38,910
this is a wide dependency,

731
00:46:41,470 --> 00:46:43,180
could be the wide dependency,

732
00:46:43,270 --> 00:46:45,610
will be a little bit more sophisticated about this in a second,

733
00:46:46,260 --> 00:46:48,750
because basically these contributions,

734
00:46:48,750 --> 00:46:52,560
you know when we contribute to the intermediate result,

735
00:46:52,560 --> 00:46:55,260
it is a join across ranks and links,

736
00:46:55,800 --> 00:47:01,050
and so it needs the partitions from links,

737
00:47:01,110 --> 00:47:03,300
and it needs partitions from ranks,

738
00:47:03,300 --> 00:47:10,920
to basically to compute partition for contribute, contrib RDD, sorry.

739
00:47:11,760 --> 00:47:15,390
So that might require you know from from a network communication,

740
00:47:15,990 --> 00:47:20,940
but it turns out they have sort of a clever optimization,

741
00:47:23,840 --> 00:47:28,010
and this is in response to an earlier question about partitioning,

742
00:47:28,220 --> 00:47:36,290
the you can specify you want to partition RDD, using a hash-partition.

743
00:47:38,960 --> 00:47:41,780
And what does that mean is that,

744
00:47:41,780 --> 00:47:44,720
the links and ranks file,

745
00:47:45,050 --> 00:47:48,290
these two RDDs are going to be partitioned in the same way,

746
00:47:48,290 --> 00:47:51,440
and they're actually partition by key or by the hash key.

747
00:47:51,470 --> 00:47:54,680
So we look back at our previous picture,

748
00:47:55,210 --> 00:48:01,300
the keys for ranks, you know, are U1 U2 U3,

749
00:48:01,570 --> 00:48:06,750
the keys for links,

750
00:48:06,750 --> 00:48:09,300
okay, sorry, the keys for ranks U1 U2 U3,

751
00:48:09,450 --> 00:48:13,530
keys for links are also U1 U2 U3.

752
00:48:14,140 --> 00:48:16,960
And basically, the client for optimization,

753
00:48:16,960 --> 00:48:20,140
this is a standard optimization from the database literature,

754
00:48:20,320 --> 00:48:23,560
is that if you partition links and ranks by key,

755
00:48:23,650 --> 00:48:26,860
then you know this one is going to have U1 on one machine,

756
00:48:28,310 --> 00:48:30,260
and maybe this is U2 on one machine,

757
00:48:32,680 --> 00:48:34,870
and ranks just going to have the same thing,

758
00:48:34,870 --> 00:48:36,730
just gonna have U1 on one machine,

759
00:48:36,730 --> 00:48:39,340
and in fact you know it's going to have U1 on the same machine,

760
00:48:39,700 --> 00:48:43,750
as links saying for U2 and U3.

761
00:48:45,710 --> 00:48:51,050
So even though this join is perceptually wide dependency,

762
00:48:51,110 --> 00:48:53,750
it can be executed like a narrow dependency,

763
00:48:53,750 --> 00:48:57,140
because basically to compute you know the join of these two,

764
00:48:57,290 --> 00:49:02,450
for the U1 for the first partition, for P1,

765
00:49:02,600 --> 00:49:07,280
you only have to look at the partition P1 of links and P1 P1 of ranks,

766
00:49:07,490 --> 00:49:12,000
because you know the keys are hashed in the same way to the same machine.

767
00:49:12,640 --> 00:49:18,070
And so the scheduler or the programmer can actually specify these hash partitions,

768
00:49:18,370 --> 00:49:19,360
scheduler sees,

769
00:49:19,390 --> 00:49:22,960
ha, you know the join actually uses hash partitions,

770
00:49:22,960 --> 00:49:25,150
the hash partitions that are the same,

771
00:49:25,300 --> 00:49:28,720
and therefore actually don't have to do this wide dependency,

772
00:49:28,720 --> 00:49:31,720
don't have to do a complete barrier as mapreduce,

773
00:49:31,840 --> 00:49:34,150
but I can just treat this as an narrow dependency.

774
00:49:35,120 --> 00:49:37,760
So, that's pretty cool,

775
00:49:38,540 --> 00:49:41,000
then you know again,

776
00:49:41,000 --> 00:49:43,040
if a machine fails, right,

777
00:49:43,040 --> 00:49:45,080
we talked a little bit about it earlier,

778
00:49:45,110 --> 00:49:47,540
that might be painful,

779
00:49:47,540 --> 00:49:51,590
because you may have to reexecute many loops or one iterations,

780
00:49:51,920 --> 00:49:55,100
and so you know if you would probably write this for real,

781
00:49:55,100 --> 00:49:57,560
then the programmer would probably say,

782
00:49:57,560 --> 00:50:03,630
like maybe every you know 10 iterations, you know basically checkpoint,

783
00:50:11,260 --> 00:50:15,880
so that you don't have to recompute computation all the way from from the beginning.

784
00:50:16,540 --> 00:50:20,290
Oh, so we don't actually recompute links or anything each time, right,

785
00:50:20,500 --> 00:50:21,940
like we don't persist, sorry.

786
00:50:22,870 --> 00:50:24,340
We don't persist, no, not at all,

787
00:50:25,030 --> 00:50:27,700
the only thing that was persist with only one was links, right,

788
00:50:27,700 --> 00:50:30,310
this is the only thing that was kind of persist call.

789
00:50:31,100 --> 00:50:32,210
Oh, we do persist.

790
00:50:32,600 --> 00:50:33,470
Links, we do.

791
00:50:33,800 --> 00:50:34,280
Okay.

792
00:50:35,760 --> 00:50:37,470
But not the intermediate RDDs,

793
00:50:38,470 --> 00:50:40,270
because they're basically new RDD every time,

794
00:50:40,270 --> 00:50:42,100
like ranks 1 is a new RDD,

795
00:50:42,100 --> 00:50:44,050
ranks 2 is a new RDD.

796
00:50:45,990 --> 00:50:51,210
But you may want to persist them you know occasionally and stored in HDFS,

797
00:50:51,210 --> 00:50:53,640
so that if you have to a failure,

798
00:50:53,640 --> 00:50:57,510
you don't have to go back to iteration loop, iteration 0 to compute everything.

799
00:51:01,370 --> 00:51:02,420
Okay, does this make sense?

800
00:51:03,350 --> 00:51:07,790
Oh, sorry, the different contribs, can they be computed in parallel?

801
00:51:08,490 --> 00:51:10,170
On different partitions, yes.

802
00:51:11,500 --> 00:51:15,610
Because there's like this line that goes vertically down.

803
00:51:15,700 --> 00:51:20,130
That vertically down, it's pipelines, right,

804
00:51:20,130 --> 00:51:22,170
there's just two types of parallelism,

805
00:51:22,170 --> 00:51:23,490
there stage parallelism,

806
00:51:23,550 --> 00:51:26,070
and there is sort of parallelism between different partitions,

807
00:51:26,340 --> 00:51:29,400
and we could think about this thing, you know this whole thing,

808
00:51:29,640 --> 00:51:32,850
like running many many times on different partitions.

809
00:51:45,300 --> 00:51:46,620
So in this case,

810
00:51:46,620 --> 00:51:50,310
the collect at the very end will be the only place where we have a wide.

811
00:51:50,520 --> 00:51:52,380
Yeah, exactly exactly,

812
00:51:52,500 --> 00:51:54,180
the collect is the only one that's gonna,

813
00:51:55,080 --> 00:51:57,450
you know whatever here we have more partitions, correct,

814
00:51:58,420 --> 00:51:59,920
I making a mess of this picture,

815
00:51:59,920 --> 00:52:03,010
but that is gonna have to get from everyone.

816
00:52:09,120 --> 00:52:09,630
Okay?

817
00:52:10,080 --> 00:52:12,600
I hope that everyone sees this is actually pretty cool, right,

818
00:52:13,300 --> 00:52:16,030
you know just by expressing these computations,

819
00:52:16,030 --> 00:52:18,940
and sort of lineage graph or data flow computation,

820
00:52:19,120 --> 00:52:21,850
the scheduler has a bit of [room] for organizations,

821
00:52:21,880 --> 00:52:26,200
like this, this [] hash partitions,

822
00:52:26,500 --> 00:52:28,660
the you know we get a lot of parallelism,

823
00:52:28,690 --> 00:52:30,640
we can also reuse,

824
00:52:30,640 --> 00:52:33,610
you know we can keep the results of one RDD in memory,

825
00:52:33,610 --> 00:52:35,530
so that we can reuse it for the next iteration,

826
00:52:35,920 --> 00:52:37,480
and you can sort of see,

827
00:52:37,480 --> 00:52:39,010
that you know these techniques combined,

828
00:52:39,010 --> 00:52:42,040
you know going to give you significant performance optimization,

829
00:52:44,640 --> 00:52:49,800
and allows you to express more powerful or more interesting computations.

830
00:52:52,420 --> 00:52:54,790
So maybe with that all, like summarize this lecture.

831
00:53:02,100 --> 00:53:05,820
So a couple things,

832
00:53:06,760 --> 00:53:13,230
you know so RDDs are made by functional transformations,

833
00:53:19,110 --> 00:53:22,650
they're group together in sort of lineage graph,

834
00:53:23,510 --> 00:53:25,580
you can think about as a data flow graph,

835
00:53:26,140 --> 00:53:30,910
and this gives the, this allows reuse,

836
00:53:32,890 --> 00:53:36,250
it also allows some clever organizations by the scheduler.

837
00:53:41,600 --> 00:53:44,840
And basically it was also more,

838
00:53:44,840 --> 00:53:55,130
it's more expressiveness than you know mapreduce by itself,

839
00:53:58,290 --> 00:54:02,010
and which results basically in good performance,

840
00:54:02,010 --> 00:54:03,990
because a lot of the data stays in memory.

841
00:54:14,790 --> 00:54:17,250
So if you, if you're excited about this,

842
00:54:17,250 --> 00:54:18,030
you can try it out,

843
00:54:18,120 --> 00:54:22,380
you can download you know Spark playaround and write some programs,

844
00:54:22,380 --> 00:54:24,750
or you know go to databricks.com,

845
00:54:24,750 --> 00:54:25,890
and you know creating accounts,

846
00:54:25,890 --> 00:54:30,000
and then you can run Spark computations on their on their clusters.

847
00:54:31,030 --> 00:54:33,490
So if you excited about this and want to try it out,

848
00:54:33,490 --> 00:54:35,050
you know it's pretty easy to do so,

849
00:54:35,470 --> 00:54:37,870
unlike FaRM, you can just like not play with,

850
00:54:38,200 --> 00:54:41,170
but this actually you can actually go out and try out.

851
00:54:41,650 --> 00:54:44,530
Okay, with that, I want to stop for today,

852
00:54:44,530 --> 00:54:49,420
and and the people that want to hang around and ask more questions,

853
00:54:49,420 --> 00:54:50,890
please feel free to do so,

854
00:54:51,190 --> 00:54:53,830
the only thing I want to remind people of is that,

855
00:54:53,860 --> 00:54:59,050
you know the deadline of for 4b is a little bit away,

856
00:54:59,050 --> 00:55:02,140
but I just want to remind people the 4b is pretty tricky,

857
00:55:02,170 --> 00:55:04,240
requires a bit of design,

858
00:55:04,240 --> 00:55:06,040
so, don't don't start too late.

859
00:55:06,660 --> 00:55:08,940
And with that, I'll see you on Tuesday.

860
00:55:14,450 --> 00:55:17,150
And again, questions, I'm happy to answer them.

861
00:55:18,560 --> 00:55:19,160
Thank you.

862
00:55:21,010 --> 00:55:24,070
I had a question about the checkpoints,

863
00:55:24,220 --> 00:55:27,940
I think if they mention automatic checkpoints,

864
00:55:28,300 --> 00:55:32,230
using data about how long each computation took,

865
00:55:34,320 --> 00:55:36,750
I wasn't really sure what they mean by this,

866
00:55:36,750 --> 00:55:40,620
but what, what are they going to be optimizing for.

867
00:55:41,300 --> 00:55:43,280
I'm gonna build a whole checkpoints is, correct,

868
00:55:43,280 --> 00:55:44,840
there's an optimization between,

869
00:55:45,020 --> 00:55:47,600
you know taking checkpoint is expensive,

870
00:55:48,220 --> 00:55:50,680
and so you know takes time,

871
00:55:51,100 --> 00:55:54,670
but in reexecution, if there's a machine failure,

872
00:55:54,700 --> 00:55:55,780
also takes a lot of time,

873
00:55:56,520 --> 00:55:59,160
and so for example if you take never checkpoint,

874
00:55:59,160 --> 00:56:01,920
then you basically have to reexecute the computation from the beginning, right,

875
00:56:02,540 --> 00:56:04,520
but if you take periodically checkpoints,

876
00:56:04,520 --> 00:56:07,310
you know you don't have to repeat you know the computation,

877
00:56:07,310 --> 00:56:08,840
that you did before the checkpoint,

878
00:56:08,840 --> 00:56:10,550
but checking the checkpoint takes time,

879
00:56:11,640 --> 00:56:13,680
so if you take very frequent checkpoints,

880
00:56:13,680 --> 00:56:15,030
you don't have to recompute a lot,

881
00:56:15,030 --> 00:56:16,740
but you spend all your time taking checkpoints.

882
00:56:18,060 --> 00:56:22,300
And so there's sort of you know an optimization problem here,

883
00:56:22,600 --> 00:56:24,160
you know you want to take the checkpoints,

884
00:56:24,160 --> 00:56:29,710
and some regular interval, make willing to take to recompute.

885
00:56:32,060 --> 00:56:36,770
Okay, so maybe like compute checkpoints only for very large computations.

886
00:56:36,800 --> 00:56:38,840
Yeah, or like for example in the case of PageRank,

887
00:56:38,840 --> 00:56:41,570
you know maybe you know do it ever 10 iterations.

888
00:56:44,800 --> 00:56:45,580
Thank you.

889
00:56:46,030 --> 00:56:48,100
It depends, of course, if decides to checkpoint, correct,

890
00:56:48,100 --> 00:56:51,340
decides to checkpoint is small, you can checkpoint more frequently,

891
00:56:52,260 --> 00:56:54,090
but in the case of this PageRank,

892
00:56:54,090 --> 00:56:55,710
you know that checkpoint is going to be pretty big,

893
00:56:56,600 --> 00:57:01,250
there is going to be a line or record you know per web page.

894
00:57:03,310 --> 00:57:04,900
That makes sense, thank you.

895
00:57:05,110 --> 00:57:05,590
You're welcome.

896
00:57:07,300 --> 00:57:09,670
I have a question about the driver,

897
00:57:10,150 --> 00:57:11,470
does the application,

898
00:57:11,470 --> 00:57:15,550
is like does the driver, is the driver on the client side,

899
00:57:15,580 --> 00:57:15,910
or is.

900
00:57:15,910 --> 00:57:16,240
Yes.

901
00:57:16,720 --> 00:57:17,440
Okay.

902
00:57:17,530 --> 00:57:18,910
Yeah, the driver.

903
00:57:19,150 --> 00:57:19,840
If it crashes,

904
00:57:19,840 --> 00:57:21,760
we lose like the the whole graph

905
00:57:21,760 --> 00:57:22,420
and that's fine,

906
00:57:22,420 --> 00:57:24,400
because that's the application.

907
00:57:25,140 --> 00:57:26,670
Yeah, I don't actually know what happens,

908
00:57:26,670 --> 00:57:30,030
because the scheduler has a [],

909
00:57:31,140 --> 00:57:34,830
and the scheduler fault tolerance,

910
00:57:35,160 --> 00:57:37,080
so I don't know exactly you know what happens,

911
00:57:37,080 --> 00:57:39,360
maybe you could reconnect, I I don't know.

912
00:57:43,940 --> 00:57:47,780
I had a question about the why dependency optimization,

913
00:57:47,780 --> 00:57:50,570
you mentioned that they do the hash partitioning,

914
00:57:50,570 --> 00:57:51,680
how does that work?

915
00:57:52,490 --> 00:57:53,840
Okay, I say a little bit more,

916
00:57:53,840 --> 00:57:54,530
so this is not,

917
00:57:54,530 --> 00:57:56,300
hash is not the topic today,

918
00:57:56,300 --> 00:57:57,620
then there is actually something,

919
00:57:57,620 --> 00:58:03,380
that's is a standard database partitioning scheme,

920
00:58:04,120 --> 00:58:05,320
and it is cool,

921
00:58:05,320 --> 00:58:06,910
because if you need computer join,

922
00:58:07,440 --> 00:58:09,900
you don't have to do a lot of communication,

923
00:58:09,900 --> 00:58:12,030
so let me just start a new slide,

924
00:58:12,030 --> 00:58:14,850
because it's a little hard to read.

925
00:58:14,850 --> 00:58:15,960
So hash partition,

926
00:58:20,210 --> 00:58:21,590
so if you have two datasets,

927
00:58:22,460 --> 00:58:24,980
here's dataset one, here's dataset two,

928
00:58:25,220 --> 00:58:28,280
they have keys, right, key 1, key 2,

929
00:58:29,620 --> 00:58:31,540
so they have the same set of keys,

930
00:58:32,250 --> 00:58:35,100
then what you do by hash partitioning,

931
00:58:35,190 --> 00:58:37,470
you partition the dataset number of partitions,

932
00:58:37,500 --> 00:58:39,240
so boom boom boom,

933
00:58:39,930 --> 00:58:42,570
and you hash the key,

934
00:58:44,830 --> 00:58:47,080
so you hash K1, you hash K2

935
00:58:47,080 --> 00:58:49,270
and that actually determines the partition ends up,

936
00:58:50,000 --> 00:58:53,960
and so all the keys that actually have the same hash

937
00:58:53,960 --> 00:58:55,070
went up in the same place,

938
00:58:55,070 --> 00:58:59,600
so like this machine 1, machine 2, machine 3,

939
00:58:59,810 --> 00:59:03,560
so you take whatever you hash K1, that goes in here,

940
00:59:03,920 --> 00:59:06,080
you know you hash whatever K2,

941
00:59:06,080 --> 00:59:08,450
may be somewhere else in the file, who knows where it is,

942
00:59:08,660 --> 00:59:09,890
and you hash to that partition,

943
00:59:10,130 --> 00:59:13,780
you do the same thing here, for the other datasets,

944
00:59:13,780 --> 00:59:15,010
so here's dataset 1,

945
00:59:17,620 --> 00:59:18,730
here's dataset 2,

946
00:59:21,060 --> 00:59:22,260
like links and ranks

947
00:59:22,590 --> 00:59:24,780
and what will happen is that,

948
00:59:24,930 --> 00:59:26,970
all the records in this dataset,

949
00:59:26,970 --> 00:59:31,740
that have the same keys as records in the other dataset,

950
00:59:31,860 --> 00:59:35,340
those keys or those records will end up in the same machine, right,

951
00:59:35,340 --> 00:59:37,990
so you're gonna partition this guy

952
00:59:37,990 --> 00:59:44,550
and basically K1 will end up here too, on the same machine, correct

953
00:59:44,550 --> 00:59:46,380
and same for the other keys,

954
00:59:46,380 --> 00:59:48,450
because you basically use the same hash function,

955
00:59:48,450 --> 00:59:49,770
and you have the same set of keys,

956
00:59:50,040 --> 00:59:52,530
and so this allows you to take a dataset you know partition

957
00:59:52,530 --> 00:59:53,730
that both in the same way

958
00:59:53,760 --> 00:59:55,440
using this hashing trick.

959
00:59:56,100 --> 00:59:56,940
And this is cool,

960
00:59:56,940 --> 01:00:00,600
because now you can do a join over these two datasets,

961
01:00:00,600 --> 01:00:02,790
like, if you need to join these two datasets,

962
01:00:02,940 --> 01:00:04,800
that basically you can just join the partitions,

963
01:00:06,420 --> 01:00:08,100
and you don't have to communicate,

964
01:00:08,800 --> 01:00:12,670
you know each of these machines doesn't have to communicate with any other machine,

965
01:00:12,700 --> 01:00:17,320
because it knows it has all the keys you know the other dataset has,

966
01:00:17,320 --> 01:00:18,580
and they're all on the same machine.

967
01:00:20,260 --> 01:00:24,370
Gotcha, so basically it's just trying sort, not sort,

968
01:00:24,370 --> 01:00:27,430
but like bucket the different objects in the same machine,

969
01:00:27,430 --> 01:00:29,170
so that it doesn't communicate.

970
01:00:32,070 --> 01:00:33,510
Okay great, thank you so much.

971
01:00:35,010 --> 01:00:37,890
So this means the hash function has to make sure that,

972
01:00:38,310 --> 01:00:39,930
there are no links,

973
01:00:40,230 --> 01:00:44,010
that would have to be like using the computation on another machine?

974
01:00:44,910 --> 01:00:47,490
Yeah, well, since they use the same hash function,

975
01:00:47,640 --> 01:00:48,720
and they have the same keys,

976
01:00:48,720 --> 01:00:49,500
you know that will happen.

977
01:00:54,100 --> 01:00:54,910
Yeah.

978
01:00:56,340 --> 01:00:58,050
I had a question,

979
01:00:58,050 --> 01:01:00,900
I actually wanted to come back to the question I asked before.

980
01:01:01,020 --> 01:01:05,550
Yeah, good.

981
01:01:06,000 --> 01:01:07,500
So let me open up.

982
01:01:07,530 --> 01:01:09,570
Yeah, let me also open the paper, I guess.

983
01:01:09,780 --> 01:01:12,570
Any other people that have questions,

984
01:01:12,600 --> 01:01:14,520
because maybe this will take a little bit of time.

985
01:01:15,530 --> 01:01:16,790
Yeah, actually, I had a quick question

986
01:01:16,790 --> 01:01:20,300
on the fault tolerance, fault tolerance of FaRM,

987
01:01:20,480 --> 01:01:24,740
so so just to clarify what happens,

988
01:01:24,740 --> 01:01:29,420
so if a failure occurs before the decision point,

989
01:01:29,840 --> 01:01:32,660
then the entire thing is aborted,

990
01:01:32,690 --> 01:01:34,580
but it occurs after the decision point,

991
01:01:34,580 --> 01:01:37,820
then after the failed computers come back up,

992
01:01:37,820 --> 01:01:40,400
they have to reask the coordinator,

993
01:01:40,400 --> 01:01:41,570
for whether or not they should commit.

994
01:01:42,340 --> 01:01:44,530
They don't really reask, right,

995
01:01:44,530 --> 01:01:46,810
like what happens is there,

996
01:01:46,810 --> 01:01:49,000
after failure, there's a recovery process runs,

997
01:01:49,500 --> 01:01:53,790
and the recovery process looks basically all the logs are []

998
01:01:53,790 --> 01:01:57,360
and then the recovery process looks at the state of the system,

999
01:01:57,690 --> 01:01:59,400
and based on the state of system,

1000
01:01:59,400 --> 01:02:00,990
and decides what to do with transaction,

1001
01:02:01,260 --> 01:02:02,760
either abort it or commit it,

1002
01:02:03,600 --> 01:02:07,590
and the key aspect here in this protocol is,

1003
01:02:07,590 --> 01:02:09,660
to ensure that at the point,

1004
01:02:09,720 --> 01:02:15,020
when the transaction coordinator actually have reported to the application,

1005
01:02:15,020 --> 01:02:17,510
that the transaction succeeded committed,

1006
01:02:17,810 --> 01:02:18,950
it has to be the case,

1007
01:02:18,950 --> 01:02:22,550
that there are enough pieces of evidence left around in the system,

1008
01:02:22,550 --> 01:02:24,050
so that during the recovery process,

1009
01:02:24,050 --> 01:02:25,640
that transaction is definitely committed,

1010
01:02:26,780 --> 01:02:30,000
and, that's, that's sort of the plan.

1011
01:02:30,700 --> 01:02:32,680
And the reason that there's enough evidence is,

1012
01:02:32,680 --> 01:02:35,170
because there's these log records lying around,

1013
01:02:35,560 --> 01:02:38,560
there's commit backup records right lying around,

1014
01:02:38,680 --> 01:02:40,690
and there's this one commit record.

1015
01:02:43,140 --> 01:02:43,590
I see,

1016
01:02:43,590 --> 01:02:44,880
so if something,

1017
01:02:44,880 --> 01:02:50,790
for example, if a failure occurs on a primary, before [] commit primary.

1018
01:02:51,000 --> 01:02:51,540
Yep.

1019
01:02:52,070 --> 01:02:53,150
What happens there?

1020
01:02:53,300 --> 01:02:54,200
So that's interesting,

1021
01:02:54,200 --> 01:02:56,090
so there's enough backup record, correct,

1022
01:02:56,620 --> 01:02:58,090
to basically decide you know that,

1023
01:02:58,090 --> 01:03:00,520
every backup that every shard actually has committed,

1024
01:03:01,360 --> 01:03:04,030
and, and so that's enough information

1025
01:03:04,030 --> 01:03:05,200
for the recovery process to say,

1026
01:03:05,200 --> 01:03:07,390
I am going to run for that transaction,

1027
01:03:07,630 --> 01:03:08,800
because it could have committed.

1028
01:03:10,630 --> 01:03:12,910
Got it, so it doesn't need the primary in that case,

1029
01:03:12,910 --> 01:03:15,580
can use the backups, because the backups have.

1030
01:03:17,140 --> 01:03:17,860
Yeah, exactly.

1031
01:03:20,030 --> 01:03:20,990
Got it, thank you,

1032
01:03:21,350 --> 01:03:23,840
and what happens if the backup fails?

1033
01:03:24,620 --> 01:03:26,660
Well, then one of the backup fails,

1034
01:03:26,660 --> 01:03:29,510
and presumably that means that the commit record is still there,

1035
01:03:30,100 --> 01:03:32,290
and then again there's enough information to decide,

1036
01:03:32,290 --> 01:03:33,910
that actual transaction needs to commit,

1037
01:03:34,400 --> 01:03:37,640
and there's enough backups around to actually know what the new values,

1038
01:03:37,640 --> 01:03:40,880
or there's also the log entries which actually contains,

1039
01:03:40,880 --> 01:03:42,290
so if a primary is up,

1040
01:03:42,620 --> 01:03:43,880
it will have a log entry,

1041
01:03:43,940 --> 01:03:45,290
there will be a commit entry

1042
01:03:45,410 --> 01:03:48,230
plus there are you know backups actually finish the transaction.

1043
01:03:50,650 --> 01:03:51,220
Thank you.

1044
01:03:52,380 --> 01:03:53,970
Oh, sorry to follow up on that,

1045
01:03:53,970 --> 01:03:57,630
if you said that if the primary failed,

1046
01:03:57,630 --> 01:04:01,110
then you could use the backups to complete the action,

1047
01:04:01,110 --> 01:04:02,160
if there's enough of them,

1048
01:04:02,550 --> 01:04:05,400
so we need to elect a new primary?

1049
01:04:06,540 --> 01:04:09,870
I I think this is all happened during the,

1050
01:04:09,930 --> 01:04:12,960
basically you can think of the recovery process as a primary,

1051
01:04:13,350 --> 01:04:15,090
and that just finishes everything off.

1052
01:04:16,520 --> 01:04:19,490
Oh, so whoever does the recovery is the primary.

1053
01:04:19,900 --> 01:04:21,550
Yeah.

1054
01:04:22,280 --> 01:04:24,470
Okay, make sense, thank you.

1055
01:04:24,560 --> 01:04:27,230
I don't think explicitly they you know promote primary,

1056
01:04:27,230 --> 01:04:28,640
just like go ahead and do it.

1057
01:04:30,060 --> 01:04:32,130
And what is like enough backups?

1058
01:04:32,490 --> 01:04:36,310
Well, we have f+1, right,

1059
01:04:36,430 --> 01:04:40,120
and so it means that, you know so as one is left,

1060
01:04:40,920 --> 01:04:42,330
you know we were good,

1061
01:04:42,360 --> 01:04:44,670
so we can have more than f+1 failures,

1062
01:04:45,060 --> 01:04:46,380
we can only have f failures,

1063
01:04:46,380 --> 01:04:48,570
in this particular drawing, f is 1,

1064
01:04:51,100 --> 01:04:53,980
so there has to be per shard you know one machine left.

1065
01:04:56,090 --> 01:04:58,100
Okay, that makes sense, thank you.

1066
01:04:58,640 --> 01:04:59,240
You're welcome.

1067
01:05:02,440 --> 01:05:04,240
Okay, Felipe, it just me and you.

1068
01:05:06,750 --> 01:05:10,200
Well, unless anyone else ask questions, I think.

1069
01:05:14,710 --> 01:05:16,420
Yeah, I and you.

1070
01:05:16,420 --> 01:05:20,500
So it's page, page they explain on page 6.

1071
01:05:20,920 --> 01:05:22,420
Yeah, yeah let me.

1072
01:05:23,590 --> 01:05:25,450
That below table 3.

1073
01:05:25,630 --> 01:05:26,200
Yep.

1074
01:05:27,660 --> 01:05:31,770
Some the paragraph starts, the most interesting question, right,

1075
01:05:31,770 --> 01:05:34,140
it goes on to define.

1076
01:05:34,890 --> 01:05:35,940
Yeah, we found it both sufficient and usual

1077
01:05:35,940 --> 01:05:37,650
to classify dependencies in two types,

1078
01:05:37,650 --> 01:05:40,200
narrow, where parent partition of the parent RDD

1079
01:05:40,200 --> 01:05:43,170
is used by most one partition of the child RDD, right,

1080
01:05:43,170 --> 01:05:43,860
so this is.

1081
01:05:44,070 --> 01:05:44,580
Right.

1082
01:05:44,820 --> 01:05:46,680
So let me, so let's draw this, correct,

1083
01:05:46,680 --> 01:05:51,840
so here we have a parent partition.

1084
01:05:52,980 --> 01:05:53,400
Right.

1085
01:05:54,960 --> 01:05:56,580
And let's say there's a map,

1086
01:05:57,820 --> 01:05:59,620
and here we have the child partition.

1087
01:06:04,640 --> 01:06:05,090
Okay?

1088
01:06:05,840 --> 01:06:06,410
Right.

1089
01:06:07,550 --> 01:06:09,890
Good, so that's the narrow, this is the narrow case.

1090
01:06:12,600 --> 01:06:17,310
Right, but I think,

1091
01:06:17,340 --> 01:06:21,380
like the the example of I was thinking of is

1092
01:06:21,380 --> 01:06:31,420
you know each parent is used by at most one partition of the child,

1093
01:06:31,950 --> 01:06:35,220
but the child, right, that doesn't say anything about,

1094
01:06:35,220 --> 01:06:37,530
like it says, right,

1095
01:06:37,530 --> 01:06:41,610
like it doesn't necessarily mean it's a one-to-one relationship, right.

1096
01:06:41,640 --> 01:06:44,220
Well, because more or less have to, correct,

1097
01:06:44,220 --> 01:06:47,550
let's say, let's say we have a wide one, correct,

1098
01:06:47,550 --> 01:06:53,250
so then here we have parent partition 1,

1099
01:06:56,350 --> 01:06:59,140
here we have, maybe it has n of them, correct,

1100
01:06:59,140 --> 01:07:01,240
here's 2, here's n,

1101
01:07:01,870 --> 01:07:07,710
in the wide one, the child is.

1102
01:07:09,920 --> 01:07:13,370
Right, what what what I what I was saying is,

1103
01:07:13,520 --> 01:07:17,030
I I think you know based on the definition given in the paper,

1104
01:07:17,720 --> 01:07:20,480
this could be a narrow partition,

1105
01:07:24,010 --> 01:07:26,800
and in fact, I mean like if you look at,

1106
01:07:28,580 --> 01:07:32,690
like a join with input co-partition, like you have.

1107
01:07:32,930 --> 01:07:35,930
Yeah, the hash partition, that actually the wide one turns into a narrow one.

1108
01:07:36,530 --> 01:07:42,230
Right, but you still have like a partition, a child partition,

1109
01:07:42,230 --> 01:07:49,360
getting like like being computed from several parent partitions, right.

1110
01:07:49,360 --> 01:07:52,570
Explicitly mention that, right,

1111
01:07:52,630 --> 01:07:54,160
I think that's the only example.

1112
01:07:54,910 --> 01:07:57,130
Yeah, but like.

1113
01:07:57,430 --> 01:07:59,080
I think there's a typo in that sentence, right.

1114
01:08:02,250 --> 01:08:04,080
I I mean, I'm not sure it's,

1115
01:08:04,080 --> 01:08:06,150
yeah, I I'm not sure if it's,

1116
01:08:06,450 --> 01:08:08,610
what they meant to like [].

1117
01:08:09,240 --> 01:08:12,330
Alright, well, we know we conclude,

1118
01:08:12,330 --> 01:08:14,470
this is the two cases, like I got.

1119
01:08:14,470 --> 01:08:14,950
Yeah.

1120
01:08:14,980 --> 01:08:16,720
I guess and there are no other cases,

1121
01:08:17,350 --> 01:08:21,310
and so then we can go for every operation, correct,

1122
01:08:21,310 --> 01:08:23,380
and then we can see when it's narrow or wide one.

1123
01:08:23,910 --> 01:08:24,690
Right, right.

1124
01:08:25,200 --> 01:08:30,270
So once the job of the programmer that defines these operations

1125
01:08:30,270 --> 01:08:33,300
to actually indicate whether it's a wide partition or narrow,

1126
01:08:33,600 --> 01:08:35,610
wide dependency or narrow dependency,

1127
01:08:36,360 --> 01:08:39,300
that's what, for figure table 3 about.

1128
01:08:39,750 --> 01:08:43,230
Yeah, like what I'm saying is like,

1129
01:08:43,230 --> 01:08:46,200
usually like like the way I saw it through paper,

1130
01:08:46,230 --> 01:08:49,590
like like, your example on the wide would be,

1131
01:08:49,590 --> 01:08:52,500
could would be a narrow dependency,

1132
01:08:52,500 --> 01:08:55,200
unless, right, like you have several child,

1133
01:08:55,410 --> 01:08:57,390
and the parent partitions are like.

1134
01:08:57,660 --> 01:08:59,430
Okay, so in general,

1135
01:08:59,430 --> 01:09:00,780
okay, it is the case of course,

1136
01:09:00,930 --> 01:09:02,520
if there's another child partition here,

1137
01:09:02,550 --> 01:09:04,230
okay so maybe that's why we're trying to get,

1138
01:09:04,650 --> 01:09:06,000
so let's separate,

1139
01:09:06,060 --> 01:09:07,860
the real picture actually draws this,

1140
01:09:08,330 --> 01:09:09,920
so it has another child partition,

1141
01:09:10,070 --> 01:09:14,750
and basically operations, the transformations are.

1142
01:09:17,210 --> 01:09:21,560
Exactly, yeah and that's narrow for sure, like.

1143
01:09:21,920 --> 01:09:23,960
This, on the right side, this is wide.

1144
01:09:23,960 --> 01:09:25,280
Sorry, that's wide, yeah.

1145
01:09:25,280 --> 01:09:29,480
That's what I meant, that for sure is wide.

1146
01:09:29,480 --> 01:09:31,040
Another one I drew is also wide, I believe,

1147
01:09:31,850 --> 01:09:33,380
like if you do join,

1148
01:09:33,380 --> 01:09:37,580
if you do an action like collect at the very end,

1149
01:09:37,730 --> 01:09:38,570
you know it's wide dependency.

1150
01:09:38,570 --> 01:09:40,120
Yeah, okay.

1151
01:09:40,120 --> 01:09:42,970
It doesn't say, it doesn't say, it has to come from different RDDs,

1152
01:09:42,970 --> 01:09:45,280
it just says like it has to come from different partitions.

1153
01:09:45,580 --> 01:09:46,600
And so this is narrow,

1154
01:09:47,230 --> 01:09:49,900
so I think narrow is only the case where is one-to-one.

1155
01:09:50,610 --> 01:09:51,420
Okay.

1156
01:09:51,450 --> 01:09:54,210
Narrow is like, narrow means no communication.

1157
01:09:55,870 --> 01:10:03,680
Right, okay, yep.

1158
01:10:07,660 --> 01:10:11,290
Yeah, I think I think the case where I was confused was,

1159
01:10:11,290 --> 01:10:15,280
like like many-to-one,

1160
01:10:15,310 --> 01:10:19,750
like I think based on the definition of the, like of the of the paper,

1161
01:10:19,750 --> 01:10:23,980
like the many-to-one relation is still, still narrow.

1162
01:10:24,340 --> 01:10:26,920
No, I think they mean it to be.

1163
01:10:26,920 --> 01:10:30,760
Yeah yeah, but like strictly, like if you [],

1164
01:10:30,760 --> 01:10:34,660
like yeah, I I think maybe like an implementation you'll see like,

1165
01:10:35,020 --> 01:10:37,330
yeah, what are you saying, right, like it's wide,

1166
01:10:37,710 --> 01:10:38,280
I was just like,

1167
01:10:38,280 --> 01:10:39,300
I think if you read like.

1168
01:10:39,300 --> 01:10:41,040
Yeah, you could be confusing.

1169
01:10:41,040 --> 01:10:41,970
Paper actually,

1170
01:10:41,970 --> 01:10:45,330
yeah, you can get confused like many-to-one relationship,

1171
01:10:45,420 --> 01:10:48,300
but the one-to-many is clearly wide, yeah,

1172
01:10:49,000 --> 01:10:50,890
but, yeah okay, sounds good.

1173
01:10:51,430 --> 01:10:51,970
Okay.

1174
01:10:52,000 --> 01:10:57,570
Yeah, I think the paper easier to understand in general.

1175
01:10:57,810 --> 01:10:58,470
Okay.

1176
01:10:59,390 --> 01:11:00,260
Good okay.

1177
01:11:00,500 --> 01:11:03,590
Is that easier than the paper for FaRM?

1178
01:11:03,590 --> 01:11:07,490
Oh, yeah, sure.

1179
01:11:07,550 --> 01:11:11,840
Yeah, I think, so I think those were probably the most two heavy-duty paper,

1180
01:11:11,840 --> 01:11:13,490
so that we've seen in this term.

1181
01:11:14,940 --> 01:11:15,780
Okay, nice.

1182
01:11:15,780 --> 01:11:17,400
The FaRM, the FaRM and Spanner,

1183
01:11:17,430 --> 01:11:21,900
I think the remaining was a little bit more you know more,

1184
01:11:21,900 --> 01:11:23,640
I'm going to say straightforward,

1185
01:11:24,060 --> 01:11:26,880
perhaps fewer moving pieces.

1186
01:11:27,710 --> 01:11:28,310
Nice.

1187
01:11:29,980 --> 01:11:32,650
Okay, awesome, thanks professor.

1188
01:11:32,770 --> 01:11:34,570
Can I ask one last question,

1189
01:11:34,600 --> 01:11:36,970
I just realized that I have,

1190
01:11:37,450 --> 01:11:41,020
it was about the conversation, you can [] it if,

1191
01:11:41,490 --> 01:11:42,690
if it's a different partitions,

1192
01:11:42,690 --> 01:11:46,520
but if it's also, you said if it's in.

1193
01:11:46,520 --> 01:11:48,680
Yeah, the stage, stages, right,

1194
01:11:48,680 --> 01:11:52,040
you know there's sort of, what is it like streaming parallelism,

1195
01:11:52,040 --> 01:11:53,840
if you will or pipeline parallelism.

1196
01:11:54,530 --> 01:11:56,780
Let me see if I can find a picture,

1197
01:11:58,110 --> 01:11:58,950
there's one of them.

1198
01:12:04,840 --> 01:12:10,510
I gotta find lineage graph, no, no,

1199
01:12:11,260 --> 01:12:12,730
right here, what is lineage graph,

1200
01:12:12,760 --> 01:12:15,910
great, maybe here's one picture that we can modify.

1201
01:12:16,800 --> 01:12:17,640
Do you see?

1202
01:12:18,900 --> 01:12:19,590
Yes.

1203
01:12:19,770 --> 01:12:24,210
Okay, so basically this is the lineage graph here to collect,

1204
01:12:24,510 --> 01:12:27,900
and so this is like one stage, right,

1205
01:12:30,820 --> 01:12:35,440
the scheduler runs one of the stages on each worker or each partition, right,

1206
01:12:35,890 --> 01:12:43,670
so each worker runs a stage on the partition,

1207
01:12:48,770 --> 01:12:50,570
so basically all these partitions,

1208
01:12:50,570 --> 01:12:54,140
all these stages running parallel on different workers,

1209
01:12:55,200 --> 01:12:58,290
then within a stage, there's also parallelism,

1210
01:12:59,100 --> 01:13:03,480
because you know every filter is pipelined,

1211
01:13:04,010 --> 01:13:05,930
with that you know they mean,

1212
01:13:05,930 --> 01:13:08,660
like you read maybe like the first n records,

1213
01:13:09,450 --> 01:13:11,430
and then you apply the filter operation,

1214
01:13:12,780 --> 01:13:19,320
and then you know that produces you know whatever n records,

1215
01:13:20,030 --> 01:13:24,860
and you know then the next you know filter you know process those n records,

1216
01:13:24,890 --> 01:13:26,960
the while processing those n records,

1217
01:13:27,200 --> 01:13:29,060
the first filter reads the next n,

1218
01:13:30,560 --> 01:13:33,560
and you know produces them and then pass them on,

1219
01:13:33,560 --> 01:13:35,960
and then you know making results in some number records,

1220
01:13:35,960 --> 01:13:37,820
again, and it goes on and on,

1221
01:13:38,270 --> 01:13:40,670
so basically all these, all these,

1222
01:13:40,670 --> 01:13:45,380
oops, like these all these transformations are pipelined,

1223
01:13:48,200 --> 01:13:50,030
and so they're almost running concurrently,

1224
01:13:50,030 --> 01:13:51,920
they're running not truly concurrently,

1225
01:13:51,920 --> 01:13:53,240
they're running in a pipeline fashion.

1226
01:13:54,440 --> 01:13:58,220
Oh, I just said this is a batch thing, they were talking about.

1227
01:13:58,250 --> 01:14:01,130
Yeah, so things are past and batches,

1228
01:14:01,130 --> 01:14:04,280
and basically every stage the pipeline processes batch.

1229
01:14:05,500 --> 01:14:07,450
Okay, okay, yeah, makes for clear,

1230
01:14:07,450 --> 01:14:08,500
yeah, thank you so much,

1231
01:14:08,500 --> 01:14:11,320
that was it was an interesting lecture, thank you.

1232
01:14:11,350 --> 01:14:13,940
Okay, you're welcome, glad you enjoy it,

1233
01:14:14,650 --> 01:14:15,460
it's a cool system.

1234
01:14:23,780 --> 01:14:25,430
Sorry, sorry, can you hear me now?

1235
01:14:25,670 --> 01:14:30,710
Yeah yeah, I was wondering what did you.

1236
01:14:30,890 --> 01:14:32,540
No problem, I'm sorry about that,

1237
01:14:32,540 --> 01:14:34,070
I, yeah, sorry, I was listening,

1238
01:14:34,070 --> 01:14:37,940
I I I realized we'd be going late,

1239
01:14:37,940 --> 01:14:39,740
I'm going to try to make this question very quick,

1240
01:14:39,830 --> 01:14:42,290
I've gotten to use Spark before.

1241
01:14:43,310 --> 01:14:46,580
Thank you, thank you so much.

1242
01:14:47,360 --> 01:14:50,930
So yeah, yeah I really, really appreciate this lecture,

1243
01:14:50,930 --> 01:14:54,080
Spark is actually something that I'm gonna use in my future job,

1244
01:14:54,080 --> 01:14:55,730
so I appreciate you teaching this me,

1245
01:14:55,790 --> 01:14:57,470
just one.

1246
01:14:57,470 --> 01:15:01,550
Probably, probably I'm not sure this will really help your writing Spark programs.

1247
01:15:01,700 --> 01:15:06,830
No, I mean I did it as an intern, really not knowing what I was doing,

1248
01:15:06,830 --> 01:15:10,670
but like this has helped me give give more context with it.

1249
01:15:12,100 --> 01:15:15,550
So I guess the the quick question with a Spark programs that,

1250
01:15:15,550 --> 01:15:19,390
the way that I've understood Spark jobs is,

1251
01:15:19,420 --> 01:15:27,160
how Spark constructs a directed, a directed acyclic graph of all the tasks, and.

1252
01:15:27,760 --> 01:15:31,810
Yep, and this is what you talked about with the wide partitions and narrow partitions.

1253
01:15:31,810 --> 01:15:35,800
I think they're called dependencies, not partitions.

1254
01:15:35,800 --> 01:15:41,190
Oh, yep, okay, okay, okay, sorry,

1255
01:15:41,190 --> 01:15:44,460
that with RDDs and I guess like,

1256
01:15:45,010 --> 01:15:47,650
okay, this is different terminology between,

1257
01:15:48,660 --> 01:15:53,580
the dependencies are like these tasks in the directed acyclic graph of all the tasks,

1258
01:15:53,580 --> 01:15:54,840
but the RDDs,

1259
01:15:55,400 --> 01:16:00,650
that's like, each task in this graph is now represented by this RDD.

1260
01:16:01,370 --> 01:16:02,570
Let me actually go back,

1261
01:16:02,990 --> 01:16:04,520
maybe these pictures are right.

1262
01:16:04,550 --> 01:16:07,820
And oh gosh I appreciate you staying here,

1263
01:16:07,820 --> 01:16:13,160
please let me know if you have to go to.

1264
01:16:13,370 --> 01:16:14,600
No no no no no, that's right, I have more time.

1265
01:16:14,750 --> 01:16:18,620
So, okay, so this is sort of a RDD, correct,

1266
01:16:19,010 --> 01:16:21,350
let me draw another color,

1267
01:16:21,350 --> 01:16:22,730
so we can [],

1268
01:16:22,910 --> 01:16:27,390
this is an RDD, RDD has a bunch of partitions,

1269
01:16:28,420 --> 01:16:30,220
and here's another RDD.

1270
01:16:33,230 --> 01:16:34,430
Okay, yep.

1271
01:16:34,820 --> 01:16:37,820
And the arrows are basically,

1272
01:16:37,820 --> 01:16:39,860
as sort of like here the same story,

1273
01:16:39,950 --> 01:16:42,170
let we actually finish this picture too,

1274
01:16:42,170 --> 01:16:44,360
on the side here more partitions,

1275
01:16:45,050 --> 01:16:51,230
here's the RDDs, boom RDD, boom RDD,

1276
01:16:51,230 --> 01:16:54,080
those transformations basically between RDDs, right,

1277
01:16:54,860 --> 01:16:56,060
so the arrows,

1278
01:16:56,880 --> 01:16:58,950
let me pick another color,

1279
01:16:58,980 --> 01:17:02,370
you know these arrows, those are transformations.

1280
01:17:04,010 --> 01:17:04,850
Gotcha,

1281
01:17:04,850 --> 01:17:09,980
and this is, this is like part of the directed acyclic graph of the Spark job,

1282
01:17:10,070 --> 01:17:13,730
every single transformation leads to creating another RDD,

1283
01:17:14,090 --> 01:17:16,160
that makes sense, that makes sense.

1284
01:17:16,220 --> 01:17:17,540
Okay and then the only thing is,

1285
01:17:17,540 --> 01:17:21,520
like some of these arrows are wide and some of them are narrow,

1286
01:17:22,040 --> 01:17:24,950
and the graph, from the graph, you can't really tell,

1287
01:17:25,370 --> 01:17:28,760
whether which ones are narrow or which ones are,

1288
01:17:28,760 --> 01:17:33,530
which transformations are narrow transformations or wide transformations.

1289
01:17:33,740 --> 01:17:37,970
You're talking about the graph that's the Spark program actually shows you in there?

1290
01:17:38,520 --> 01:17:40,500
Yeah, this lineage graph, you can't.

1291
01:17:40,500 --> 01:17:41,910
Gotcha, okay, okay.

1292
01:17:42,150 --> 01:17:42,900
Yeah, that's,

1293
01:17:42,900 --> 01:17:46,860
here, so here look at this lineage graph,

1294
01:17:46,860 --> 01:17:49,530
like this transformation, that transformation, that transformation,

1295
01:17:49,530 --> 01:17:51,090
all that transformation narrow, would like narrow,

1296
01:17:51,090 --> 01:17:52,230
because it's a single arrow,

1297
01:17:52,440 --> 01:17:53,790
but it's not really true, right,

1298
01:17:53,790 --> 01:17:56,640
like the last one, for example, must be a wide one,

1299
01:17:58,450 --> 01:18:00,040
because it will collect information from all.

1300
01:18:00,040 --> 01:18:00,490
Gotcha.

1301
01:18:03,110 --> 01:18:05,720
Okay, then I I guess I was wondering,

1302
01:18:05,720 --> 01:18:11,030
like do you have recommendations on resources for things,

1303
01:18:11,030 --> 01:18:14,450
that can show me how Spark figures out,

1304
01:18:14,450 --> 01:18:19,220
how to construct the directed acyclic graph to do all these tasks.

1305
01:18:19,220 --> 01:18:23,960
Yeah, look at scheduler, that's all.

1306
01:18:23,960 --> 01:18:26,930
Right and yeah I read in the paper,

1307
01:18:26,930 --> 01:18:30,950
like I I you know I was trying to comprehend the paper as best as I could,

1308
01:18:30,950 --> 01:18:33,290
but it's you know it's difficult, but.

1309
01:18:33,680 --> 01:18:34,880
It will always be difficult to read,

1310
01:18:35,180 --> 01:18:40,610
so the scheduler I think I would go back first to thesis [] thesis,

1311
01:18:40,610 --> 01:18:42,410
I'm sure it has a [] on the scheduler.

1312
01:18:43,440 --> 01:18:44,220
Gotcha gotcha,

1313
01:18:44,280 --> 01:18:46,050
oh okay and that'll show me

1314
01:18:46,050 --> 01:18:48,240
just how Spark figures out how to make this graph.

