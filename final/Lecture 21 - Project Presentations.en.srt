1
00:00:09,730 --> 00:00:13,000
So feel free to get started, whenever you want to be ready.

2
00:00:19,430 --> 00:00:21,470
Alright, should we start then.

3
00:00:22,560 --> 00:00:23,130
Please.

4
00:00:24,420 --> 00:00:27,240
Alright, hey everyone, I'm Felipe,

5
00:00:27,240 --> 00:00:28,770
I'm working with Caralina,

6
00:00:28,770 --> 00:00:30,360
and today we're going to be presenting

7
00:00:30,690 --> 00:00:33,750
on our project on distributed private electronic voting.

8
00:00:34,400 --> 00:00:39,260
So motivating this project was you know actually sort of sort of easy,

9
00:00:39,290 --> 00:00:41,990
given the current events and elections,

10
00:00:41,990 --> 00:00:44,090
that had to happen with [covert] restrictions,

11
00:00:44,330 --> 00:00:46,100
so we ask the question,

12
00:00:46,100 --> 00:00:49,190
how would [] voting work?

13
00:00:49,760 --> 00:00:55,480
And, we're we're focusing specifically on maintaining voter privacy,

14
00:00:55,970 --> 00:00:58,460
or you know keeping votes private.

15
00:00:59,010 --> 00:01:03,150
And so here's a sketch of how a voting system might work,

16
00:01:03,520 --> 00:01:06,100
you have a bunch of voters,

17
00:01:06,160 --> 00:01:07,840
in this case, you know five,

18
00:01:08,110 --> 00:01:10,600
and then a vote counter,

19
00:01:10,600 --> 00:01:13,450
the voters send their votes to the vote counter,

20
00:01:14,060 --> 00:01:16,520
and the vote counter is,

21
00:01:16,520 --> 00:01:20,840
you know maybe they send them encrypted or some sort of security,

22
00:01:20,870 --> 00:01:23,060
the vote counter decrypts them,

23
00:01:23,060 --> 00:01:25,370
make sure that each voter votes at most once

24
00:01:25,670 --> 00:01:26,870
and computes a winner.

25
00:01:27,590 --> 00:01:32,540
And so you know key thing to notice here is that,

26
00:01:32,600 --> 00:01:38,360
in order for the vote counter to determine that every voter vote [] at most once,

27
00:01:38,360 --> 00:01:42,170
each vote has to be in some way linked to the voter,

28
00:01:42,500 --> 00:01:43,940
which which is dangerous.

29
00:01:43,970 --> 00:01:45,740
So we're going to explain our threat model here,

30
00:01:46,040 --> 00:01:50,570
which is we're giving the attacker two sort of capacity

31
00:01:50,570 --> 00:01:52,400
or two two sort of powers,

32
00:01:52,490 --> 00:01:55,940
the first is to create fail stop failures,

33
00:01:56,000 --> 00:01:57,800
but not byzantine failures,

34
00:01:57,800 --> 00:02:00,530
so the attacker can crash server,

35
00:02:00,620 --> 00:02:04,580
but it cannot sort of make it misbehave.

36
00:02:04,930 --> 00:02:08,380
And you know whether there is a reasonable assumption is a question for another day,

37
00:02:08,830 --> 00:02:11,890
but we think there are other protocols that deal with this issues,

38
00:02:11,980 --> 00:02:14,590
we're not dealing with Byzantine failures.

39
00:02:15,090 --> 00:02:18,240
The the second power we give them is

40
00:02:18,240 --> 00:02:21,720
to passively spy on a on a vote counter,

41
00:02:21,840 --> 00:02:23,280
and so this is problematic,

42
00:02:23,280 --> 00:02:26,970
because as we said votes are linked with their voters,

43
00:02:26,970 --> 00:02:28,530
and so a passive attacker,

44
00:02:28,530 --> 00:02:31,530
that's spying on the server,

45
00:02:31,560 --> 00:02:33,600
can sort of deanonymized those votes.

46
00:02:33,840 --> 00:02:36,900
And so here's where we come in,

47
00:02:36,900 --> 00:02:39,330
and present our distributed voting design,

48
00:02:39,510 --> 00:02:42,360
we're going to deal with the first problem,

49
00:02:42,450 --> 00:02:46,500
which is you know the adversary crashing the servers,

50
00:02:46,500 --> 00:02:50,370
and so we are going to have several vote counters,

51
00:02:50,460 --> 00:02:52,380
and the idea is that,

52
00:02:52,410 --> 00:02:56,700
each voter will send other votes to all the vote counters,

53
00:02:57,030 --> 00:03:02,520
and the vote counters will use the same protocols as before

54
00:03:02,550 --> 00:03:03,630
and compute winners,

55
00:03:04,170 --> 00:03:06,330
[] winners, sorry,

56
00:03:06,480 --> 00:03:08,760
and it's this is good,

57
00:03:08,760 --> 00:03:11,280
because add a second crash,

58
00:03:11,310 --> 00:03:14,310
sort of like n-1 vote counters,

59
00:03:14,310 --> 00:03:16,890
as long as one of them is up and running,

60
00:03:17,070 --> 00:03:18,810
we will be able to compute a winner.

61
00:03:19,320 --> 00:03:24,660
However it's very you know arguably more unsafe

62
00:03:24,900 --> 00:03:26,340
for the second type of attack,

63
00:03:26,340 --> 00:03:27,990
which was passively [fine],

64
00:03:28,020 --> 00:03:32,460
because as long as you know as as a adversary,

65
00:03:32,460 --> 00:03:36,000
the attacker can compromise, even one server,

66
00:03:36,090 --> 00:03:38,490
you know it can deanonymized votes,

67
00:03:38,490 --> 00:03:40,710
so that's where we're introducing Shamir Secret Sharing,

68
00:03:41,370 --> 00:03:44,310
for Shamir Secret Sharing, we have a voter,

69
00:03:44,460 --> 00:03:48,450
that's gonna, you know choose a vote either 0 or 1,

70
00:03:48,820 --> 00:03:49,690
and we're gonna,

71
00:03:49,690 --> 00:03:52,750
you know I'm not going to explain how Shamir actually works,

72
00:03:52,750 --> 00:03:54,610
it's a cryptographic protocol,

73
00:03:54,640 --> 00:03:58,180
but we're gonna just you know show what it allows us to do,

74
00:03:58,420 --> 00:04:01,060
you pass vote through Shamir,

75
00:04:01,090 --> 00:04:02,980
where you give it two parameters, n and k,

76
00:04:03,070 --> 00:04:06,730
and it's going to create n part,

77
00:04:06,880 --> 00:04:10,420
which allow you to re-compute that vote,

78
00:04:10,450 --> 00:04:12,250
the n parts could completely random,

79
00:04:12,280 --> 00:04:14,710
and in fact, what is sort of powerful,

80
00:04:14,710 --> 00:04:18,550
but Shamir is even k-1 shares

81
00:04:18,550 --> 00:04:21,550
give you no information about the original vote,

82
00:04:21,580 --> 00:04:24,340
but given k or more shares,

83
00:04:24,370 --> 00:04:27,070
you can use Shamir to re-compute that vote.

84
00:04:29,780 --> 00:04:31,700
So we're going to show Shamir Voting Scheme.

85
00:04:32,570 --> 00:04:36,290
So, yeah, now, for the Shamir Voting Voting Scheme,

86
00:04:36,290 --> 00:04:39,410
so first all the voters are going to choose their vote,

87
00:04:39,410 --> 00:04:41,690
and share parts of the vote counters,

88
00:04:41,690 --> 00:04:44,330
and the complete vote on the different parts,

89
00:04:44,720 --> 00:04:47,990
and so the both counters are going to receive the parts,

90
00:04:47,990 --> 00:04:50,180
and when they have the parts of all the voters,

91
00:04:50,610 --> 00:04:52,410
they're going to SUM these parts,

92
00:04:52,650 --> 00:04:54,690
and share the SUM with the vote counters,

93
00:04:54,780 --> 00:04:56,820
so here it's important to note that,

94
00:04:56,820 --> 00:04:59,190
this SUM looks like completely at random,

95
00:04:59,280 --> 00:05:01,440
and so by sharing it with the both counters,

96
00:05:01,560 --> 00:05:03,150
the other both counters,

97
00:05:03,150 --> 00:05:07,440
they cannot learn anything about the parts that a received,

98
00:05:07,680 --> 00:05:12,450
so, this ensures that the privacy of the voters is maintained,

99
00:05:13,970 --> 00:05:16,010
so they exchange their votes,

100
00:05:16,100 --> 00:05:19,100
and when a vote counter has received case sums

101
00:05:19,370 --> 00:05:22,700
from other voters are including their part,

102
00:05:22,730 --> 00:05:24,740
they can finally compute the winner,

103
00:05:24,980 --> 00:05:28,340
so using Shamir Secret Sharing again, black box,

104
00:05:28,460 --> 00:05:31,520
it will like re-compute the sum,

105
00:05:31,760 --> 00:05:34,850
and if we get something that is greater than half of the number of voters,

106
00:05:34,850 --> 00:05:35,960
then the winner is 1,

107
00:05:35,960 --> 00:05:38,120
and if it is less than, the winner would be 0,

108
00:05:38,420 --> 00:05:40,910
and yes so we finally have a winner.

109
00:05:41,680 --> 00:05:44,860
Now, some of the assumptions of our scheme are,

110
00:05:44,860 --> 00:05:47,890
first that the voters and vote counters are well behaved,

111
00:05:47,890 --> 00:05:49,030
and follow the protocol

112
00:05:49,180 --> 00:05:53,920
or their parts some to what they say sounds like good intentions,

113
00:05:53,950 --> 00:05:59,470
and we only handle fail stop failures with this scheme.

114
00:05:59,920 --> 00:06:02,410
So yeah now to handle some of the scenarios,

115
00:06:02,410 --> 00:06:07,620
first if we have an unreliable network scenario,

116
00:06:07,620 --> 00:06:12,240
all of the RPCs that we send in our servers are going to be sent periodically

117
00:06:12,240 --> 00:06:15,450
until we receive an acknowledgement that it has been received,

118
00:06:17,130 --> 00:06:19,620
then to handle voter failures,

119
00:06:19,710 --> 00:06:22,980
so we we need to persist all of the voters,

120
00:06:23,010 --> 00:06:26,040
like persist the parts they computed and their vote,

121
00:06:26,130 --> 00:06:29,280
because if we re-compute parts,

122
00:06:29,280 --> 00:06:31,380
and then the vote counters had different parts,

123
00:06:31,380 --> 00:06:33,180
then the correctness of the scheme will be gone,

124
00:06:33,180 --> 00:06:34,800
and they would not be able to correct the sum,

125
00:06:34,950 --> 00:06:39,720
so it's important to always share like parts from the same computation,

126
00:06:39,720 --> 00:06:41,660
and not like change parts,

127
00:06:42,500 --> 00:06:45,560
and finally to handle the vote counter failures,

128
00:06:45,560 --> 00:06:49,580
here we rely on Shamir Secret Sharing scheme,

129
00:06:49,670 --> 00:06:51,140
and so as we mentioned before,

130
00:06:51,140 --> 00:06:54,260
we only need k servers to compute the winner,

131
00:06:54,530 --> 00:06:59,540
so, the system is [] to n-k crashes of vote counters.

132
00:07:00,480 --> 00:07:01,950
And now it is demo time,

133
00:07:01,980 --> 00:07:05,580
so I'll stop sharing screen and share another screen.

134
00:07:08,520 --> 00:07:11,190
And so here is our demo,

135
00:07:11,310 --> 00:07:13,920
basically we have 5 voters here,

136
00:07:13,920 --> 00:07:16,470
so we have 3 vote counters, 5 voters,

137
00:07:16,470 --> 00:07:18,120
and k is equal to 2,

138
00:07:18,710 --> 00:07:22,100
this means that network is unreliable,

139
00:07:22,280 --> 00:07:23,480
and now if we run it,

140
00:07:26,000 --> 00:07:27,860
here, so we got 3,

141
00:07:27,860 --> 00:07:30,470
and we get the winner is 1,

142
00:07:30,860 --> 00:07:32,900
we can also crash one of the servers,

143
00:07:33,530 --> 00:07:35,060
since k is equal to 2,

144
00:07:35,060 --> 00:07:39,050
we can still compute a winner by having only two servers up,

145
00:07:39,230 --> 00:07:40,640
and so now if we run it,

146
00:07:40,970 --> 00:07:44,090
we get the winner of the election is say 1 again,

147
00:07:44,210 --> 00:07:46,370
and this is the end of our presentation,

148
00:07:46,370 --> 00:07:47,810
thank you very much for listening

149
00:07:47,810 --> 00:07:49,130
and we'll take some questions.

150
00:08:03,940 --> 00:08:05,320
Feel free to ask questions.

151
00:08:12,130 --> 00:08:15,160
Curious, how extensively you tested this system,

152
00:08:15,430 --> 00:08:19,660
like do you have a try in much different other configurations,

153
00:08:19,720 --> 00:08:21,430
do you have performance numbers?

154
00:08:22,680 --> 00:08:29,190
Sure, so, we we did have like different sizes of,

155
00:08:29,220 --> 00:08:30,660
we create a whole test suite,

156
00:08:30,690 --> 00:08:36,120
and make sure to test like voter failures, server failures have different sizes,

157
00:08:36,360 --> 00:08:39,990
and we don't have performance numbers,

158
00:08:40,110 --> 00:08:41,160
we didn't test that,

159
00:08:41,190 --> 00:08:43,620
but in terms of like failures,

160
00:08:43,620 --> 00:08:47,220
and and different like numbers of voters, vote counters,

161
00:08:47,220 --> 00:08:49,410
yeah, we have a full test suite,

162
00:08:49,410 --> 00:08:52,440
and that we should have put the link and get for github,

163
00:08:52,440 --> 00:08:55,810
but we can you know send it to you, if you, you want,

164
00:08:55,810 --> 00:09:00,690
so you can check like look over implementation, under testing.

165
00:09:09,490 --> 00:09:13,070
There, more questions?

166
00:09:13,070 --> 00:09:16,220
Can you say something maybe a little bit like what ideas from 6.824

167
00:09:16,220 --> 00:09:18,290
were you able to apply in the in this system,

168
00:09:20,390 --> 00:09:22,100
other than using the testing framework.

169
00:09:24,170 --> 00:09:26,810
Sure, Caro, do you want to take this one or should I take?

170
00:09:27,340 --> 00:09:30,730
Sure, so here,

171
00:09:30,730 --> 00:09:33,640
I guess, yeah this game actually comes from

172
00:09:33,640 --> 00:09:35,830
a [pizza] from the cryptography class,

173
00:09:35,860 --> 00:09:39,310
and so it was fun to look at it from a different perspective,

174
00:09:39,310 --> 00:09:43,150
so now we're like focusing that much on the security,

175
00:09:43,150 --> 00:09:46,720
I need more like what happens if the individual like servers,

176
00:09:46,720 --> 00:09:47,620
if they fail,

177
00:09:47,650 --> 00:09:53,950
so both handling voter failures, handling vote counter failures,

178
00:09:54,040 --> 00:09:55,690
problems with the network,

179
00:09:55,720 --> 00:09:59,020
I guess we didn't address specific partitions,

180
00:09:59,020 --> 00:10:04,120
because it would work similar to vote counter crushed,

181
00:10:04,150 --> 00:10:08,740
but, yeah, it was, it was fun to look at this problem from a different perspective.

182
00:10:11,300 --> 00:10:13,460
Yeah, thanks.

183
00:10:14,300 --> 00:10:15,470
Unless you have something to say,

184
00:10:15,500 --> 00:10:16,610
we're out of time a bit, so,

185
00:10:17,940 --> 00:10:21,000
yeah, cool awesome, nice job, thanks for sharing.

186
00:10:21,360 --> 00:10:24,960
Okay, we have a presentation on private analytics.

187
00:10:27,830 --> 00:10:30,380
If Kevin is ready to share, awesome,

188
00:10:30,900 --> 00:10:31,950
take it away, I'll be ready.

189
00:10:34,150 --> 00:10:36,940
Okay, you guys can hear me and you guys can see my pointer?

190
00:10:37,420 --> 00:10:37,930
Yes.

191
00:10:38,440 --> 00:10:39,940
Okay, cool,

192
00:10:40,390 --> 00:10:41,470
everyone, I'm Kevin,

193
00:10:41,470 --> 00:10:45,010
and today I'm going to be presenting this very creatively named system called Sys,

194
00:10:45,310 --> 00:10:49,750
and this is a system for collecting aggregate statistics in a privacy preserving way,

195
00:10:50,440 --> 00:10:53,320
and so the last presentation was good leading to my project,

196
00:10:53,350 --> 00:10:55,480
so for the most part in 6.824,

197
00:10:55,780 --> 00:10:56,380
we've talked about,

198
00:10:56,380 --> 00:11:00,880
how we can build reliable systems in the presence of byzantine or [] fails,

199
00:11:01,030 --> 00:11:02,080
so servers can crash,

200
00:11:02,080 --> 00:11:03,820
and clients are generally well behaved,

201
00:11:04,030 --> 00:11:05,110
but from this talk,

202
00:11:05,110 --> 00:11:06,610
I hope you learn some new concepts,

203
00:11:06,610 --> 00:11:11,200
on how we can build systems with strong guarantees in the presence of byzantine failures

204
00:11:11,440 --> 00:11:13,690
on behalf of both clients and servers within the system.

205
00:11:14,450 --> 00:11:16,460
And the main tools that we're going to use

206
00:11:16,460 --> 00:11:18,770
to achieve these guarantees are cryptographic primitives,

207
00:11:18,770 --> 00:11:21,770
such as multiparty computation and zero-knowledge proofs

208
00:11:21,860 --> 00:11:24,470
and also distributed computing primitives, such as broadcast.

209
00:11:27,040 --> 00:11:28,210
Okay, so for simplicity,

210
00:11:28,240 --> 00:11:30,670
let's say we just want to build a system that computes sums,

211
00:11:30,820 --> 00:11:32,350
so we're going to have an aggregation server,

212
00:11:32,350 --> 00:11:34,210
that stores key value store,

213
00:11:34,450 --> 00:11:37,720
so the keys are going to be indices for some statistics,

214
00:11:37,780 --> 00:11:39,910
and the values are going to be tuples of sums,

215
00:11:40,850 --> 00:11:42,350
and we're gonna have a bunch of clients,

216
00:11:42,440 --> 00:11:44,330
so each client is gonna have some identity,

217
00:11:44,330 --> 00:11:45,980
say its client IP addresses,

218
00:11:46,380 --> 00:11:48,930
it's going to have the index of the statistic they wanna bump,

219
00:11:49,230 --> 00:11:51,570
and it's also going to have its private inputs,

220
00:11:52,040 --> 00:11:53,480
so the most straightforward thing to do is,

221
00:11:53,480 --> 00:11:57,230
we can have all the clients send their inputs to the server as is,

222
00:11:57,230 --> 00:11:58,430
and they can computes the sums,

223
00:11:59,120 --> 00:11:59,990
but obviously this is bad,

224
00:11:59,990 --> 00:12:02,180
because now this leads everything, right,

225
00:12:02,330 --> 00:12:04,010
the server will learn the client's identity,

226
00:12:04,130 --> 00:12:05,570
it'll learn the index being bumped,

227
00:12:05,750 --> 00:12:07,580
and they'll learn their private input.

228
00:12:09,130 --> 00:12:10,330
So we can do a little bit better,

229
00:12:10,420 --> 00:12:12,220
we can compute these sums privately,

230
00:12:12,340 --> 00:12:14,800
if we deploy to non colluding servers,

231
00:12:15,070 --> 00:12:16,810
and then we're going to have each client

232
00:12:16,840 --> 00:12:19,180
secret share their input to each of these servers,

233
00:12:19,620 --> 00:12:21,570
so as we said in the previous presentation,

234
00:12:21,570 --> 00:12:26,730
so each server, each signature alone leaks no information about the client's private input,

235
00:12:27,000 --> 00:12:29,760
but still the servers can add up these shares

236
00:12:29,760 --> 00:12:33,000
and compute a local version of the key value store,

237
00:12:33,660 --> 00:12:36,720
and then later when the servers want to recover the actual sums,

238
00:12:36,810 --> 00:12:38,760
they can combine their local key value stores

239
00:12:38,760 --> 00:12:40,860
to reconstruct the global key value store,

240
00:12:42,120 --> 00:12:43,140
and this is a little bit better,

241
00:12:43,140 --> 00:12:45,660
at least if one of these servers is honest,

242
00:12:45,810 --> 00:12:48,030
then the servers are still gonna learn the client's identity,

243
00:12:48,060 --> 00:12:50,310
they're still going to learn the client's index,

244
00:12:50,370 --> 00:12:53,040
but now instead of learning each individual client's input,

245
00:12:53,130 --> 00:12:56,220
they're going to learn the sums of all clients and inputs,

246
00:12:56,880 --> 00:12:57,780
okay, so that's better,

247
00:12:57,780 --> 00:12:59,220
but still a problem,

248
00:12:59,220 --> 00:13:03,690
namely that this identity index relation can still leak a lot of information.

249
00:13:04,510 --> 00:13:05,620
So how can we fix this,

250
00:13:05,680 --> 00:13:07,120
we can make things anonymous,

251
00:13:07,330 --> 00:13:10,300
so we're still going to adapt setup from the [],

252
00:13:10,510 --> 00:13:13,990
but now we're going to give each server a public key for encryption,

253
00:13:14,840 --> 00:13:17,840
and that each client is going to encrypt each of their shares,

254
00:13:18,380 --> 00:13:21,380
and instead of having the clients send their shares directly to servers,

255
00:13:21,410 --> 00:13:24,050
now we're going to have a layer of forwarding proxies in between,

256
00:13:24,980 --> 00:13:25,880
and so what's going to happen is,

257
00:13:25,880 --> 00:13:31,520
the clients going to send their encrypted shares via broadcast to those proxies,

258
00:13:31,700 --> 00:13:35,570
and the proxies will route each share to their respective servers,

259
00:13:36,160 --> 00:13:39,490
and then the aggregation can go as as explained,

260
00:13:40,720 --> 00:13:42,370
and so what are the privacy guarantees here,

261
00:13:42,670 --> 00:13:44,860
if at least one of these properties is honest,

262
00:13:44,950 --> 00:13:47,380
then the proxy will still learn the client's identity,

263
00:13:47,650 --> 00:13:49,210
and it'll learn some timing information,

264
00:13:49,210 --> 00:13:51,430
based on when the client set the share,

265
00:13:52,100 --> 00:13:52,940
but nothing else,

266
00:13:52,940 --> 00:13:55,400
because the shares are encrypted to the server,

267
00:13:55,400 --> 00:13:56,930
so they learn nothing from that,

268
00:13:58,100 --> 00:14:00,980
and then if at least one of the servers is honest,

269
00:14:01,010 --> 00:14:03,230
then the servers will also learn some timing information

270
00:14:03,260 --> 00:14:04,700
based when the proxy forward it,

271
00:14:05,180 --> 00:14:07,400
it will certainly learn the index of the statistic

272
00:14:07,490 --> 00:14:08,510
and learn the sum,

273
00:14:09,260 --> 00:14:10,610
but most importantly,

274
00:14:11,060 --> 00:14:15,470
as long as not both proxy and server are compromised at the same time,

275
00:14:15,590 --> 00:14:18,830
then this design unlinks the identity from the index being bumped,

276
00:14:18,830 --> 00:14:21,000
which is exactly what we wanted, right,

277
00:14:21,000 --> 00:14:21,840
so this is great,

278
00:14:21,960 --> 00:14:24,210
but this also leads to another problem,

279
00:14:24,300 --> 00:14:27,540
namely that now clients can hide behind the privacy,

280
00:14:27,540 --> 00:14:31,230
and anonymity guarantees of the system to send bad inputs, right.

281
00:14:31,230 --> 00:14:34,590
So let's say the system expected client inputs to be zeros and ones,

282
00:14:34,710 --> 00:14:36,300
while behind this [] of privacy,

283
00:14:36,330 --> 00:14:39,600
now the client can send the signature like a billion through the system,

284
00:14:39,750 --> 00:14:42,450
and now it can undetectably skew this sum,

285
00:14:42,840 --> 00:14:44,190
so clearly this is bad.

286
00:14:45,280 --> 00:14:45,970
Let's fix this,

287
00:14:45,970 --> 00:14:47,680
we want to make the system more robust,

288
00:14:47,830 --> 00:14:49,510
so what we're going to do is,

289
00:14:50,320 --> 00:14:51,640
we're going to have each client generate,

290
00:14:51,640 --> 00:14:54,610
what's called a zero-knowledge proof over their shares,

291
00:14:54,610 --> 00:14:55,930
and send these to the servers,

292
00:14:56,680 --> 00:14:59,500
and the many servers collect these zero-knowledge proofs,

293
00:14:59,590 --> 00:15:03,250
they can interactively check that the client's inputs,

294
00:15:03,250 --> 00:15:07,930
actually the client shares actually reconstruct to some well formed input,

295
00:15:08,640 --> 00:15:10,920
and because this proof is in zero-knowledge,

296
00:15:11,010 --> 00:15:13,920
it leaves nothing about input,

297
00:15:13,950 --> 00:15:15,420
other than that it's well formed,

298
00:15:16,160 --> 00:15:18,080
and so again our privacy properties stay the same,

299
00:15:18,080 --> 00:15:20,120
the proxy still learns that client's identity,

300
00:15:20,210 --> 00:15:21,950
is still learns some timing information,

301
00:15:22,340 --> 00:15:23,600
the server learns timing information,

302
00:15:23,600 --> 00:15:25,130
it learns the index and learns the sums,

303
00:15:25,510 --> 00:15:28,330
but now we've protected the system against malicious clients,

304
00:15:28,480 --> 00:15:30,670
because it'll only accept well formed inputs,

305
00:15:31,610 --> 00:15:32,570
and so this is great,

306
00:15:32,720 --> 00:15:34,460
still there's another problem,

307
00:15:34,460 --> 00:15:36,050
which is that servers can crash,

308
00:15:36,260 --> 00:15:37,190
and we can lose data,

309
00:15:37,220 --> 00:15:39,290
so we obviously need both servers to be online,

310
00:15:39,290 --> 00:15:42,200
in order to reconstruct global key value store.

311
00:15:42,840 --> 00:15:44,550
And so if we want to make the system more reliable,

312
00:15:44,550 --> 00:15:47,700
we can do what we know best which is to replicate the servers,

313
00:15:47,850 --> 00:15:49,890
and so we can use Raft style replication,

314
00:15:49,890 --> 00:15:52,500
or we can also use primary backup style replication.

315
00:15:53,340 --> 00:15:54,240
Now the question is,

316
00:15:54,240 --> 00:15:57,090
okay, with all this replication, all this cryptographic machinery

317
00:15:57,120 --> 00:15:58,110
and on this message routing,

318
00:15:58,110 --> 00:16:00,090
can we still achieve good throughput,

319
00:16:01,180 --> 00:16:03,970
and it turns out that we can actually parallelize the server step here,

320
00:16:04,000 --> 00:16:05,740
that does proof checking [],

321
00:16:05,770 --> 00:16:07,900
which is likely to be the bottleneck of the system,

322
00:16:08,260 --> 00:16:09,490
and what's going to happen is that,

323
00:16:09,490 --> 00:16:11,380
proxies are going to hash partition,

324
00:16:11,410 --> 00:16:13,030
their inputs to each of these servers,

325
00:16:13,300 --> 00:16:15,580
and then each of these servers in sort of reduce step,

326
00:16:15,610 --> 00:16:17,740
will combine their intermediate key value stores

327
00:16:17,800 --> 00:16:20,950
to reconstruct the global key value store containing all the sums.

328
00:16:21,910 --> 00:16:22,420
Okay?

329
00:16:23,200 --> 00:16:24,400
And so now the final question is,

330
00:16:24,400 --> 00:16:26,230
did I implement all this before the due date,

331
00:16:27,040 --> 00:16:27,730
unfortunately, no,

332
00:16:27,730 --> 00:16:29,950
but I did make it much of the way there,

333
00:16:29,950 --> 00:16:31,750
so I'm gonna show a quick demo

334
00:16:31,870 --> 00:16:34,090
of the non replicated, non parallel version of it.

335
00:16:34,700 --> 00:16:37,580
So, I'm going to quickly switch to my other laptop,

336
00:16:42,520 --> 00:16:44,080
I gotta stop sharing this one first.

337
00:16:46,630 --> 00:16:53,040
Yes, cool,

338
00:16:53,070 --> 00:16:54,840
okay, so great,

339
00:16:55,430 --> 00:16:57,800
so, on these right two terminals are going to be with servers,

340
00:16:57,800 --> 00:16:58,700
so I'm going to run them,

341
00:17:00,640 --> 00:17:02,080
it is implemented in Rust,

342
00:17:02,200 --> 00:17:05,020
now I'm going to hook up these two proxies in the middle,

343
00:17:07,420 --> 00:17:08,410
and then on the left terminal,

344
00:17:08,410 --> 00:17:10,720
I'm just gonna simulate a thousand honest clients.

345
00:17:11,820 --> 00:17:14,970
And what's happening is all the clients are generating their input sharing

346
00:17:14,970 --> 00:17:16,380
and generating zero-knowledge proofs

347
00:17:16,440 --> 00:17:17,910
and sending them with proxies,

348
00:17:18,320 --> 00:17:20,780
and the proxies are simply forwarding them to the servers,

349
00:17:21,280 --> 00:17:22,930
and then here finally on the server side,

350
00:17:22,930 --> 00:17:24,520
they're going to be checking all the proofs,

351
00:17:24,610 --> 00:17:26,830
and if the inputs are in fact well formed,

352
00:17:26,830 --> 00:17:29,170
it's going to add it to its local key value store,

353
00:17:29,560 --> 00:17:30,490
and then at some time later,

354
00:17:30,490 --> 00:17:33,040
when the servers want to reconstruct the final statistics,

355
00:17:33,040 --> 00:17:35,620
they can just combine their key value store to recover the sums.

356
00:17:36,760 --> 00:17:38,740
And that's it for my presentation,

357
00:17:38,740 --> 00:17:40,090
I'm happy to take any questions.

358
00:17:45,830 --> 00:17:47,300
Do you have any questions from the audience?

359
00:17:54,380 --> 00:17:55,490
I guess I have a question,

360
00:17:55,550 --> 00:17:58,130
so like with what you implemented so far,

361
00:17:58,220 --> 00:18:03,020
what sort of, like which at what point do you accept,

362
00:18:03,230 --> 00:18:05,510
what point do tolerate failures and like stuff,

363
00:18:05,510 --> 00:18:10,730
like that I can talk about the reliability of your current implementation.

364
00:18:11,640 --> 00:18:15,690
Yeah, so the reliability of the current implementation is not great,

365
00:18:16,050 --> 00:18:18,270
mostly because the servers aren't replicated,

366
00:18:18,720 --> 00:18:19,770
so for the proxies,

367
00:18:19,770 --> 00:18:23,100
because, the clients broadcast the proxies,

368
00:18:23,100 --> 00:18:25,830
all you require is that one of the proxies is up,

369
00:18:26,540 --> 00:18:27,560
so if we have two proxies,

370
00:18:27,650 --> 00:18:29,990
we can tolerate one failure of the proxies,

371
00:18:29,990 --> 00:18:33,110
and we'll still get the messages to the servers,

372
00:18:33,380 --> 00:18:35,270
but if any of the servers goes down,

373
00:18:35,300 --> 00:18:38,030
then you're just not going to be reconstruct on the data for that.

374
00:18:41,670 --> 00:18:43,410
Is it only for sums

375
00:18:43,410 --> 00:18:48,630
or did you implemented for any general function that operates on the all those inputs?

376
00:18:49,620 --> 00:18:52,770
Yeah, for now, I've only implemented for sums,

377
00:18:52,770 --> 00:18:55,320
but basically with this additive secret sharing scheme,

378
00:18:55,350 --> 00:18:58,260
you can compute any linear function you want,

379
00:18:59,000 --> 00:19:00,770
and maybe more complex ones are possible,

380
00:19:00,770 --> 00:19:02,960
but I'm still haven't explored those yet,

381
00:19:04,100 --> 00:19:06,020
it turns out at least in practice,

382
00:19:06,020 --> 00:19:08,540
sums probably get like 90% of the way there.

383
00:19:10,350 --> 00:19:13,830
So what do you performance numbers looked like?

384
00:19:14,790 --> 00:19:15,990
Performance numbers, yeah,

385
00:19:16,080 --> 00:19:18,330
so the main things we want to measure are,

386
00:19:18,420 --> 00:19:21,750
for the client side clients on computation and client bandwidth,

387
00:19:22,080 --> 00:19:23,310
I have some numbers,

388
00:19:23,310 --> 00:19:24,600
at least for client computation,

389
00:19:24,690 --> 00:19:28,650
I'm generating these shares and these proofs takes less than a few milliseconds,

390
00:19:28,650 --> 00:19:30,270
so it's very lightweight,

391
00:19:30,450 --> 00:19:32,340
the bandwidth is just a few kilobytes,

392
00:19:32,840 --> 00:19:36,020
and then for the throughput on the server side,

393
00:19:36,140 --> 00:19:37,400
actually ran on EC2,

394
00:19:37,400 --> 00:19:40,880
but I only allocated four cores to each server,

395
00:19:41,090 --> 00:19:42,350
because I only had 64 cores,

396
00:19:42,350 --> 00:19:43,820
and I wanted most of them to be on the client,

397
00:19:43,820 --> 00:19:45,530
so I could remove that bottleneck,

398
00:19:45,680 --> 00:19:46,790
and so at four cores,

399
00:19:46,790 --> 00:19:49,930
I think, what is that,

400
00:19:49,930 --> 00:19:52,420
probably like a thousand queries per second,

401
00:19:52,780 --> 00:19:58,300
and then estimating I guess if you parallelize by twenty servers for each logical machine,

402
00:19:58,360 --> 00:20:02,680
they can probably achieve close to 22000 queries per second,

403
00:20:03,110 --> 00:20:05,390
but this is all run on the same data center,

404
00:20:05,390 --> 00:20:07,520
so it doesn't factor into latency,

405
00:20:07,550 --> 00:20:10,100
so the actual numbers will probably be a little bit lower than that.

406
00:20:12,040 --> 00:20:14,050
I had a question about your implementation,

407
00:20:14,170 --> 00:20:15,310
how do you do,

408
00:20:16,010 --> 00:20:21,080
so how do you actually implement your knowledge proofs code not theoretical.

409
00:20:21,770 --> 00:20:23,780
Yeah, that's a great question,

410
00:20:24,170 --> 00:20:28,130
I can send you the paper, that I implemented it out,

411
00:20:28,950 --> 00:20:31,350
but it's it's not too complicated,

412
00:20:31,350 --> 00:20:33,870
basically it's just a bunch of [finite] field operations,

413
00:20:33,870 --> 00:20:35,400
and so if you find your [file] library,

414
00:20:35,400 --> 00:20:37,560
just follow the paper and follow the steps

415
00:20:37,920 --> 00:20:40,510
and it's so much straightforward from that point,

416
00:20:40,660 --> 00:20:43,300
as long as you can decrypt the paper.

417
00:20:44,020 --> 00:20:47,560
Okay, and is that and is it possible to like test that,

418
00:20:48,100 --> 00:20:51,620
like, like, how would you know that it's working or not working.

419
00:20:52,070 --> 00:20:52,850
Yeah, so I guess,

420
00:20:52,880 --> 00:20:55,760
yeah, I only showed the simulation with a honest clients,

421
00:20:55,760 --> 00:20:59,990
but you can also generate a simulation with clients that submit bad proofs,

422
00:20:59,990 --> 00:21:02,300
and then you can see them being rejected.

423
00:21:02,960 --> 00:21:04,520
Okay, yeah, that makes sense, thank you.

424
00:21:04,550 --> 00:21:04,940
Yeah.

425
00:21:10,280 --> 00:21:12,530
Great, thanks, pretty [],

426
00:21:12,920 --> 00:21:15,170
is the next group ready to go?

427
00:21:22,300 --> 00:21:27,280
Hello, I'm Shannen and Nik and Johan here,

428
00:21:27,280 --> 00:21:29,680
so Johan take it away.

429
00:21:32,850 --> 00:21:33,750
Thank you, Shannen,

430
00:21:33,900 --> 00:21:35,520
so we'll talking about BukaDocs,

431
00:21:35,550 --> 00:21:38,010
BukaDocs is a distributive collaborative editor,

432
00:21:38,340 --> 00:21:39,540
it's similar to Google Docs,

433
00:21:39,540 --> 00:21:40,560
just a little bit better,

434
00:21:41,130 --> 00:21:44,400
so to move onto the next slide,

435
00:21:45,090 --> 00:21:46,410
can you look next for them?

436
00:21:47,140 --> 00:21:49,390
Okay, so as you've seen in this class,

437
00:21:49,390 --> 00:21:52,720
achieving consistency is very hard to do in the distributed system,

438
00:21:53,140 --> 00:21:56,050
there are many ways that consistency could go wrong,

439
00:21:56,440 --> 00:21:58,360
so a very simple example is,

440
00:21:58,360 --> 00:22:02,800
if the order that you received RPCs is different in each peer,

441
00:22:02,950 --> 00:22:04,990
you might end up with an inconsistent state,

442
00:22:06,100 --> 00:22:10,600
and there's a data structure called CRDTs,

443
00:22:10,600 --> 00:22:14,860
which we use in our system to mitigate this issue,

444
00:22:14,890 --> 00:22:18,850
CRDTs achieve eventually consistency,

445
00:22:19,300 --> 00:22:22,810
by making every single operation that you make on the document,

446
00:22:22,930 --> 00:22:27,160
globally unique, not just unique to every single peer, but globally unique,

447
00:22:27,250 --> 00:22:29,560
so if I press the letter a on my editor,

448
00:22:29,860 --> 00:22:33,160
it's different from Shannen or Nik pressing the letter a on their editor,

449
00:22:34,280 --> 00:22:38,550
so, for example here, even if the bottom here,

450
00:22:38,550 --> 00:22:42,300
for example here adds a new turtle to a document,

451
00:22:43,260 --> 00:22:48,030
even if they receive remove request from the other two peers,

452
00:22:48,090 --> 00:22:50,460
they will never remove the golden turtle,

453
00:22:50,790 --> 00:22:54,330
because the operation is itself different from remove,

454
00:22:54,510 --> 00:22:58,740
so remove green turtle is different from remove gold turtle.

455
00:22:59,980 --> 00:23:02,860
And this, this is how we achieve eventual consistency,

456
00:23:03,220 --> 00:23:04,150
so from here,

457
00:23:04,150 --> 00:23:07,510
I believe Nik is going to talk a little bit more about what is here,

458
00:23:07,510 --> 00:23:08,890
how we implement third case.

459
00:23:09,600 --> 00:23:13,440
Yeah, so for BukaDocs,

460
00:23:13,440 --> 00:23:15,960
we chose to use a CRDT called LSEQ,

461
00:23:16,020 --> 00:23:20,250
which represents a sequence of elements with variable length keys,

462
00:23:20,430 --> 00:23:21,780
so the goal is,

463
00:23:21,900 --> 00:23:24,720
let's say we want a sequence that represents the alphabet,

464
00:23:24,720 --> 00:23:26,850
and so far we have the letters a and c,

465
00:23:27,460 --> 00:23:31,480
so one editor might choose to try adding the letter b between them,

466
00:23:31,720 --> 00:23:35,470
and another editor may choose to try adding the letter d after c,

467
00:23:35,890 --> 00:23:38,770
and the goal is that with eventual consistency,

468
00:23:38,770 --> 00:23:42,010
will eventually reach the state a b c d,

469
00:23:42,040 --> 00:23:43,930
so the way that LSEQ achieves this is,

470
00:23:43,930 --> 00:23:45,880
by using a START and END token,

471
00:23:46,350 --> 00:23:50,670
and then it gives every character in the document an individual token,

472
00:23:50,670 --> 00:23:52,260
that is between the START and the END,

473
00:23:52,560 --> 00:23:55,020
so we can insert h between start and end,

474
00:23:55,350 --> 00:23:57,540
if we want the letter i after h,

475
00:23:57,630 --> 00:23:59,160
then we can insert that at 7,

476
00:23:59,160 --> 00:24:00,600
which is between 4 and 8,

477
00:24:01,060 --> 00:24:05,440
now, if we want to insert an exclamation point between i and the end of the document,

478
00:24:05,680 --> 00:24:08,230
we can insert it with the key 7 2,

479
00:24:08,410 --> 00:24:10,660
so we increase key [] on 2

480
00:24:10,690 --> 00:24:14,440
to create key between two other keys that are adjacent,

481
00:24:14,890 --> 00:24:18,790
so in this way, we can always create a key between any two other keys,

482
00:24:19,000 --> 00:24:20,800
so we can always insert,

483
00:24:21,540 --> 00:24:24,780
now LSEQ forms well,

484
00:24:24,780 --> 00:24:28,860
in that it reaches eventual consistency with very little effort for coordination,

485
00:24:29,280 --> 00:24:31,560
and it has some optimizations,

486
00:24:31,560 --> 00:24:35,010
that cause the length of the keys to grow relatively slowly.

487
00:24:36,000 --> 00:24:39,030
However, some cons are that,

488
00:24:39,060 --> 00:24:41,370
in order to support deletion of these elements,

489
00:24:41,370 --> 00:24:45,120
it relies on causal delivery and exactly-once delivery,

490
00:24:45,120 --> 00:24:46,710
and we didn't really want to implement this,

491
00:24:46,710 --> 00:24:49,050
since it was based on a number of other works,

492
00:24:49,420 --> 00:24:51,340
so we use a slightly simpler approach,

493
00:24:51,580 --> 00:24:53,140
which was a deletion set,

494
00:24:53,660 --> 00:24:56,300
so this is a grow only set, where we add in elements,

495
00:24:56,510 --> 00:24:59,090
so for example to delete letters h and i,

496
00:24:59,090 --> 00:25:01,850
we would add in 4 7 into this deletion set,

497
00:25:02,580 --> 00:25:07,440
then this whole state becomes equivalent to just having to START and END tokens,

498
00:25:07,440 --> 00:25:09,810
and the exclamation point at key 7,2.

499
00:25:14,820 --> 00:25:16,740
So we built the Buka Docs service,

500
00:25:16,770 --> 00:25:19,290
similar to how we implemented kv Raft,

501
00:25:19,290 --> 00:25:21,600
we have multiple servers, multiple clients,

502
00:25:21,600 --> 00:25:25,080
and multiple clients, they only talk to one server at a time,

503
00:25:25,080 --> 00:25:28,200
and they continuously try the operations

504
00:25:28,200 --> 00:25:31,020
until they get a successful reply from the server,

505
00:25:34,320 --> 00:25:39,450
clients and servers both maintain an AVL tree of characters in the document,

506
00:25:39,450 --> 00:25:43,260
as well as our own deletion set of removed keys,

507
00:25:43,260 --> 00:25:46,710
we chose to store characters in AVL tree,

508
00:25:46,710 --> 00:25:48,870
for I guess performance reasons,

509
00:25:50,340 --> 00:25:52,290
the chain of events goes something like this,

510
00:25:52,290 --> 00:25:55,530
so the client will send an insertion and deletion to the server,

511
00:25:55,860 --> 00:26:00,240
a server will update its own AVL tree and deletion set and persist that,

512
00:26:00,630 --> 00:26:03,300
and for those updates to all the other servers and clients,

513
00:26:03,300 --> 00:26:06,630
and then the server will respond success to the client.

514
00:26:07,870 --> 00:26:10,660
So we're going to demo it,

515
00:26:11,200 --> 00:26:15,640
we build a very simple UI for it here,

516
00:26:15,940 --> 00:26:17,350
so let me,

517
00:26:18,350 --> 00:26:23,900
Johan and Nik are also accessing this from different clients right now,

518
00:26:24,230 --> 00:26:27,170
so you can see them typing,

519
00:26:27,170 --> 00:26:28,580
you can type something else,

520
00:26:29,330 --> 00:26:30,740
I'm typing here,

521
00:26:34,210 --> 00:26:37,750
I think Johan is typing hi,

522
00:26:38,710 --> 00:26:40,810
Nik is typing something here,

523
00:26:53,360 --> 00:26:57,310
and we can edit each others text as well.

524
00:27:02,930 --> 00:27:08,010
Then, yeah, that is Buka Docs,

525
00:27:12,770 --> 00:27:16,700
we're happy to answer any questions, I guess.

526
00:27:21,550 --> 00:27:22,600
I have a question here,

527
00:27:23,050 --> 00:27:25,150
I really like the turtle theme,

528
00:27:25,150 --> 00:27:26,380
that you guys came up with this,

529
00:27:26,380 --> 00:27:29,680
and I was just wondering where the, where the turtle theme came from?

530
00:27:30,930 --> 00:27:37,170
Oh, yeah, so the turtle is Nik Johan and I talk the class over IP

531
00:27:37,170 --> 00:27:40,170
and the turtle is our mascot for the class,

532
00:27:40,170 --> 00:27:42,510
and we are inspired to build Buka Docs,

533
00:27:42,510 --> 00:27:45,870
because we use Google Docs is a question document

534
00:27:45,870 --> 00:27:47,100
for the students ask questions,

535
00:27:47,100 --> 00:27:51,270
it couldn't handle a load of over 75 users typing at the same time,

536
00:27:51,570 --> 00:27:53,610
and that's how Buka Docs was born.

537
00:27:54,560 --> 00:27:55,670
Thank you for ask.

538
00:27:56,360 --> 00:27:56,840
That's cool.

539
00:27:58,060 --> 00:27:59,560
I had a quick question,

540
00:27:59,740 --> 00:28:02,590
and so first of all, I got called this presentation,

541
00:28:02,590 --> 00:28:04,180
sorry, if you mentioned this, you may have,

542
00:28:04,210 --> 00:28:09,280
but so first of all, first question just to make sure I understood correctly,

543
00:28:09,280 --> 00:28:11,800
is the are these data structures which are very interesting,

544
00:28:11,800 --> 00:28:13,390
they stored on the client or the server,

545
00:28:14,090 --> 00:28:14,930
if they're stored,

546
00:28:14,930 --> 00:28:16,850
on I imagine right now the server is not replicated,

547
00:28:16,850 --> 00:28:18,260
but that's something you could easily do,

548
00:28:18,290 --> 00:28:21,050
but if they, if they are stored on the client,

549
00:28:21,500 --> 00:28:23,180
what happens if one of the clients fails?

550
00:28:24,500 --> 00:28:28,820
So the data structures are actually stored on all the clients and all the servers,

551
00:28:29,660 --> 00:28:33,020
and so right now, we're assuming the clients are fully trustworthy,

552
00:28:33,230 --> 00:28:35,690
and so if client fails,

553
00:28:35,690 --> 00:28:38,330
any edits they haven't sent to a server,

554
00:28:38,360 --> 00:28:40,340
will just be on their end,

555
00:28:40,340 --> 00:28:41,690
until they come back online,

556
00:28:41,690 --> 00:28:43,580
in which case, they can send it,

557
00:28:43,580 --> 00:28:45,890
and it will achieve eventual consistency.

558
00:28:47,180 --> 00:28:48,380
Interesting, thank you.

559
00:28:49,850 --> 00:28:50,930
Oh, I had a question,

560
00:28:50,990 --> 00:28:53,840
why LSEQ over other CRDTs?

561
00:28:58,460 --> 00:29:01,520
Yeah, we chose LSEQ mainly just because,

562
00:29:01,520 --> 00:29:03,260
it was one of the first ones we found

563
00:29:03,260 --> 00:29:04,400
and we wanted to get started

564
00:29:04,580 --> 00:29:08,900
and because after implementing the logic of the variable length keys,

565
00:29:08,900 --> 00:29:10,790
there was really not much more, we have to do,

566
00:29:10,790 --> 00:29:11,900
in order to make sure,

567
00:29:11,900 --> 00:29:13,880
that the clients and servers stayed consistent.

568
00:29:15,610 --> 00:29:16,930
Oh, okay, thank you.

569
00:29:16,930 --> 00:29:19,060
So beyond that it was just like message passing

570
00:29:19,060 --> 00:29:20,860
and making sure everyone gets all the messages.

571
00:29:22,100 --> 00:29:23,300
Okay, thanks.

572
00:29:24,780 --> 00:29:27,120
So, how many servers are you running within this example,

573
00:29:27,120 --> 00:29:28,590
was it just one or was it multiple?

574
00:29:30,080 --> 00:29:32,480
There were three servers and three clients,

575
00:29:32,510 --> 00:29:35,960
each one of the clients was connected to known to its own server.

576
00:29:37,070 --> 00:29:43,760
So you are using the servers for scaling or for fault tolerance, or both?

577
00:29:44,900 --> 00:29:46,190
A little bit for both,

578
00:29:46,190 --> 00:29:48,530
when we ran the performance metrics,

579
00:29:49,040 --> 00:29:53,510
by distributing the load of the client requests across many servers,

580
00:29:53,510 --> 00:29:56,120
you are able to handle a little bit more bandwidth request,

581
00:29:56,740 --> 00:29:58,780
however, this is a trade-off between,

582
00:29:58,810 --> 00:30:01,390
because eventual consistency will take a little bit more time,

583
00:30:01,390 --> 00:30:05,780
because the servers will have to send RPCs to every single other servers,

584
00:30:06,680 --> 00:30:09,890
so at our paper, we have a whole diagram everything, but.

585
00:30:12,770 --> 00:30:14,900
Yeah, how well does it scale,

586
00:30:14,930 --> 00:30:16,640
as you add more servers.

587
00:30:17,220 --> 00:30:20,460
Yeah, so have the metrics on my other screen,

588
00:30:20,640 --> 00:30:24,630
so it's able to handle,

589
00:30:25,920 --> 00:30:32,100
if we gave it to more or less 3000 requests across five clients and five servers,

590
00:30:32,550 --> 00:30:34,980
it achieve eventual consistency within three seconds,

591
00:30:34,980 --> 00:30:38,820
but this was mainly because it was also on a computer,

592
00:30:38,820 --> 00:30:41,010
that's on local host with RPCs were really fast.

593
00:30:42,460 --> 00:30:45,730
However, that also means that all the computers happening on one machine,

594
00:30:45,730 --> 00:30:49,300
so this can go either way when we put it on real hardware.

595
00:30:54,080 --> 00:30:56,570
Yeah, another thing is that,

596
00:30:57,170 --> 00:31:00,290
if you make too many requests at once,

597
00:31:00,680 --> 00:31:04,460
eventually consistency kind of hard to achieve,

598
00:31:04,520 --> 00:31:10,730
so for example we tried to do 19000 edits at once,

599
00:31:10,760 --> 00:31:13,070
across a hundred clients and five servers,

600
00:31:13,400 --> 00:31:16,670
and it took around 32 seconds to reach eventual consistency,

601
00:31:17,480 --> 00:31:21,260
however, if we only did 3000 edits across 100 clients and 5 servers,

602
00:31:21,260 --> 00:31:22,670
it only took around three seconds.

603
00:31:25,740 --> 00:31:28,860
When you say you do whatever the 19000 or whatever that was,

604
00:31:28,860 --> 00:31:30,660
is that like sending all those at once,

605
00:31:30,660 --> 00:31:32,520
or is it spread out over time.

606
00:31:33,210 --> 00:31:34,290
Every single one is sent,

607
00:31:34,290 --> 00:31:37,160
every single edit sent in parallel, and at least at test.

608
00:31:38,990 --> 00:31:39,440
Interesting.

609
00:31:42,580 --> 00:31:43,750
I had a quick question,

610
00:31:43,750 --> 00:31:45,460
so one of the motivators,

611
00:31:45,460 --> 00:31:48,070
that collaborative text editors like Google Docs

612
00:31:48,580 --> 00:31:52,840
can't support high scale updates concurrent updates,

613
00:31:52,960 --> 00:31:56,590
so why don't you think they use an approach like what you guys are proposing?

614
00:31:59,650 --> 00:32:03,610
I think Google Docs uses a similar approach called operational transform,

615
00:32:03,640 --> 00:32:07,150
which I'm not sure, if it can be parallelized as well,

616
00:32:07,600 --> 00:32:08,740
and they probably don't want to,

617
00:32:08,740 --> 00:32:10,630
because they don't want to give that much server power,

618
00:32:12,320 --> 00:32:15,260
it's probably just like for cost effectiveness,

619
00:32:15,290 --> 00:32:15,890
I guess.

620
00:32:18,950 --> 00:32:21,470
Yeah, I'm just, how [] say,

621
00:32:21,980 --> 00:32:25,100
I don't think they use case a hundred people editing one document,

622
00:32:25,100 --> 00:32:26,360
comes up too often,

623
00:32:26,960 --> 00:32:30,170
at least in the Google, Google Docs usage,

624
00:32:31,290 --> 00:32:33,930
so it's probably not something you want to spend extra engineering time on.

625
00:32:39,200 --> 00:32:41,420
Great, thank you, cool demo.

626
00:32:41,990 --> 00:32:44,150
So up next, we have the eggscrambler,

627
00:32:44,150 --> 00:32:44,780
if you all ready.

628
00:32:51,910 --> 00:32:54,580
Cool, hello everyone, my name is Arvid

629
00:32:54,580 --> 00:32:56,860
and together with Arman and Wendy,

630
00:32:56,860 --> 00:32:58,360
we have been working on eggscrambler,

631
00:32:58,360 --> 00:33:02,350
which is anonymous broadcasting using commutative encryption.

632
00:33:02,350 --> 00:33:05,020
So let's start with a motivating problem,

633
00:33:05,470 --> 00:33:07,840
we all know we all love MIT Confessions,

634
00:33:07,840 --> 00:33:11,560
so let's say that one person in this class wants to submit a confession,

635
00:33:11,560 --> 00:33:13,840
telling people how much they love this class,

636
00:33:13,930 --> 00:33:14,890
and so what do they do,

637
00:33:14,890 --> 00:33:17,890
well, they submit this confession to a Google Forms,

638
00:33:18,010 --> 00:33:21,860
and then that goes to the MIT Confessions admin,

639
00:33:21,860 --> 00:33:24,380
and then that hopefully goes to the Facebook page.

640
00:33:24,560 --> 00:33:26,360
But now, let's consider a scenario,

641
00:33:26,360 --> 00:33:29,420
where the confessions admin is actually an MIT professor,

642
00:33:29,570 --> 00:33:32,300
particularly for another classic course,

643
00:33:32,300 --> 00:33:35,270
so obviously they would not want this confession to be posted,

644
00:33:35,670 --> 00:33:38,370
so they would censor it, because they can,

645
00:33:38,670 --> 00:33:39,900
but it's like even worse,

646
00:33:39,900 --> 00:33:42,810
because they could, they could, there like an MIT professor,

647
00:33:42,810 --> 00:33:45,900
so they could go to Google look at their network traffic,

648
00:33:45,900 --> 00:33:47,790
they could look at the MIT WiFi network traffic,

649
00:33:47,880 --> 00:33:50,220
so they could figure out who actually sent this,

650
00:33:50,610 --> 00:33:54,330
and then obviously they would like to give that person an F- in their class

651
00:33:54,600 --> 00:33:55,950
and that's terrible.

652
00:33:55,950 --> 00:33:56,970
So what do we want to do,

653
00:33:56,970 --> 00:34:00,240
well, we want anonymous broadcasting

654
00:34:00,240 --> 00:34:02,610
and that's like a common theme today, it seems like,

655
00:34:02,640 --> 00:34:04,680
and so what is anonymous broadcasting,

656
00:34:04,680 --> 00:34:10,590
well, we define it as a protocol to broadcast a message to everyone, all the users,

657
00:34:10,620 --> 00:34:12,510
without revealing who the message comes from,

658
00:34:12,790 --> 00:34:14,590
so with our further do,

659
00:34:14,590 --> 00:34:18,250
I will let Wendy talk about how we do that.

660
00:34:20,540 --> 00:34:23,720
So yeah, the two main features of our design are that,

661
00:34:23,720 --> 00:34:26,600
it is decentralized and uses cryptography,

662
00:34:26,630 --> 00:34:29,960
in particular, commutative encryption is necessary,

663
00:34:29,960 --> 00:34:32,000
because otherwise the description order

664
00:34:32,000 --> 00:34:34,850
will reveal the original identities of message senders.

665
00:34:36,880 --> 00:34:40,150
Yeah, so here's the protocol be implemented,

666
00:34:40,210 --> 00:34:42,430
ensure participants join the network,

667
00:34:42,580 --> 00:34:44,560
and encrypt and submit their message,

668
00:34:44,560 --> 00:34:48,880
which one the encrypted by all participants exactly once in the encrypt phase,

669
00:34:49,060 --> 00:34:51,100
the scramble phase provides anonymity,

670
00:34:51,100 --> 00:34:52,870
because as highlighted,

671
00:34:53,110 --> 00:34:56,940
because every participant encrypts the messages,

672
00:34:56,940 --> 00:34:58,620
and randomize is there order,

673
00:34:59,070 --> 00:35:02,310
finally decryption results in the desired list of messages

674
00:35:02,310 --> 00:35:04,950
from all participants with no connection to identity.

675
00:35:08,340 --> 00:35:11,100
So yeah, if all participants are honest,

676
00:35:11,100 --> 00:35:14,070
this protocol successfully results in anonymity,

677
00:35:14,940 --> 00:35:18,570
while it is possible for a malicious user to determine the sender of a message,

678
00:35:18,570 --> 00:35:21,690
we believe that such actions are detectable [],

679
00:35:21,810 --> 00:35:23,430
if people are interested,

680
00:35:23,430 --> 00:35:25,320
there exist other solutions to this problems,

681
00:35:25,320 --> 00:35:27,270
such as mix-nets and DC-nets,

682
00:35:27,390 --> 00:35:31,050
next Arman will describe the structure of the implementation.

683
00:35:34,800 --> 00:35:38,040
So, first of all, look at this assuming that,

684
00:35:38,840 --> 00:35:41,000
we're looking at the high-level interface for,

685
00:35:41,030 --> 00:35:43,610
someone trying to submit a message and interaction with our servers,

686
00:35:43,640 --> 00:35:46,340
so let's say you're the person trying to submit a confession,

687
00:35:46,370 --> 00:35:47,750
you're this application client,

688
00:35:48,170 --> 00:35:51,680
so the first thing you're going to do is send a Start message,

689
00:35:51,710 --> 00:35:55,070
so that indicates that you want to start a new broadcast round,

690
00:35:55,820 --> 00:35:58,610
and the Service is going to ask all the people,

691
00:35:58,610 --> 00:36:01,160
who are participating in the round, for a message,

692
00:36:02,050 --> 00:36:03,730
and then it's going to run this protocol,

693
00:36:04,090 --> 00:36:07,450
and then it's going to send the results to everyone who participated,

694
00:36:07,480 --> 00:36:10,270
or if it's something like confessions going to publish it,

695
00:36:11,140 --> 00:36:13,570
and at this point, it would be anonymized,

696
00:36:14,440 --> 00:36:16,120
but this is a very simple view,

697
00:36:16,120 --> 00:36:17,170
in reality,

698
00:36:17,470 --> 00:36:21,970
we're actually building this on top of our Raft implementation from the labs.

699
00:36:22,390 --> 00:36:24,610
So the reason we use lab Raft is two [],

700
00:36:24,700 --> 00:36:28,210
so one, we implement the protocol as a state machine,

701
00:36:28,980 --> 00:36:34,260
and so Raft is among many instances that are running the protocol,

702
00:36:34,350 --> 00:36:36,540
and so Raft is used to replicate this state,

703
00:36:37,020 --> 00:36:38,940
and then two, when we're doing the scramble phase,

704
00:36:38,940 --> 00:36:42,630
we need to make sure that each participant is scrambling in a specific order,

705
00:36:42,630 --> 00:36:45,660
so we actually use Raft as a coordination mechanism

706
00:36:45,660 --> 00:36:46,830
to do this test-and-set,

707
00:36:47,220 --> 00:36:49,350
where they all proceed in a single order,

708
00:36:49,980 --> 00:36:52,440
so similar to the kv store,

709
00:36:52,440 --> 00:36:54,030
we have the state machine server,

710
00:36:54,030 --> 00:36:56,940
that's reading the Raft updates and updating the state machine

711
00:36:57,450 --> 00:36:59,490
with whatever they agreed [] state is,

712
00:36:59,970 --> 00:37:02,310
and we also add in the State Machine Client,

713
00:37:02,310 --> 00:37:04,410
which is reading how the state machine is changing,

714
00:37:04,770 --> 00:37:07,410
and then this client is what's actually taking the actions

715
00:37:07,410 --> 00:37:10,290
to progress the protocol in some way,

716
00:37:11,640 --> 00:37:14,430
and then again just as in kv store,

717
00:37:14,980 --> 00:37:17,680
in order to progress the protocol,

718
00:37:17,680 --> 00:37:21,100
this client is going to contact the leader server,

719
00:37:21,370 --> 00:37:24,280
with a request to the state machine updates,

720
00:37:24,900 --> 00:37:28,410
and then we also implement the Raft Configuration Change RPCs,

721
00:37:28,870 --> 00:37:32,710
so servers can add or remove themselves to the configuration,

722
00:37:32,770 --> 00:37:33,700
and this means that,

723
00:37:34,090 --> 00:37:37,180
new people can join subsequent broadcast rounds,

724
00:37:37,570 --> 00:37:38,920
and then also if there's a failure,

725
00:37:38,920 --> 00:37:40,870
we can cut someone out of the configuration,

726
00:37:40,900 --> 00:37:43,450
so that the next broadcast round can proceed,

727
00:37:43,780 --> 00:37:48,200
because our protocol requires that everyone is actually active

728
00:37:48,200 --> 00:37:49,580
in order for the round to complete.

729
00:37:51,380 --> 00:37:52,940
And then finally we have the application,

730
00:37:52,940 --> 00:37:56,840
where the client is like something like confessions running on your phone,

731
00:37:57,200 --> 00:37:59,750
will send these broadcast messages to the clients,

732
00:38:00,080 --> 00:38:02,960
and then it will receive the update,

733
00:38:02,960 --> 00:38:04,850
once the protocol has completed.

734
00:38:05,950 --> 00:38:09,190
So we're going to show a quick demo recorded,

735
00:38:09,220 --> 00:38:11,950
just let me know if you can't hear this.

736
00:38:16,730 --> 00:38:17,810
So now onto the demo,

737
00:38:17,810 --> 00:38:20,750
we start working at Wendy's computer right now,

738
00:38:21,080 --> 00:38:23,870
and she's going to start the application,

739
00:38:23,870 --> 00:38:26,990
so she's basically gonna start an instance on her computer and try

740
00:38:27,080 --> 00:38:32,860
and she's currently sending the link for her configuration into the Zoom chat,

741
00:38:32,890 --> 00:38:33,850
as we can see here,

742
00:38:35,210 --> 00:38:37,670
so now we're gonna go to Arvid's computer,

743
00:38:37,670 --> 00:38:38,870
he's gonna take that link

744
00:38:38,870 --> 00:38:41,720
and join the same configuration as Wendy,

745
00:38:43,110 --> 00:38:44,130
so he does that,

746
00:38:44,520 --> 00:38:47,310
and now he's going to start the round,

747
00:38:48,540 --> 00:38:50,700
well, can send their messages,

748
00:38:53,220 --> 00:38:54,990
and we have the round succeeded.

749
00:38:55,470 --> 00:38:58,740
So now are we going to show that we can seamlessly add a new client,

750
00:38:58,740 --> 00:39:00,780
so this is both someone is going to send a message,

751
00:39:00,810 --> 00:39:02,970
and an entirely new Raft instance,

752
00:39:03,390 --> 00:39:04,170
so we see here,

753
00:39:04,170 --> 00:39:08,510
this new instance quickly catches up on the left there,

754
00:39:08,660 --> 00:39:12,290
and now all three of them are going to send a message for this next round,

755
00:39:15,980 --> 00:39:18,230
and we get all three messages,

756
00:39:18,260 --> 00:39:21,500
so we demonstrated that we can broadcast over the network

757
00:39:21,950 --> 00:39:25,400
and also add in clients at will.

758
00:39:30,260 --> 00:39:33,670
Yes, that's a demo of eggscrambler,

759
00:39:33,760 --> 00:39:36,010
does anyone have any questions?

760
00:39:39,610 --> 00:39:41,140
This is, this is a really fast one,

761
00:39:41,170 --> 00:39:44,620
this first of all, this is a really cool system,

762
00:39:44,920 --> 00:39:52,780
well, so the application is separate from the client Raft state machine, correct?

763
00:39:53,660 --> 00:39:54,140
Yes.

764
00:39:54,260 --> 00:39:56,150
And it's like two different devices,

765
00:39:56,740 --> 00:39:58,600
so I guess my question sort of is,

766
00:39:58,750 --> 00:40:01,570
like in the overall picture of how you would deploy this thing,

767
00:40:01,570 --> 00:40:02,410
why is that,

768
00:40:02,710 --> 00:40:04,600
I mean, I imagine you could run

769
00:40:04,870 --> 00:40:07,480
a [], like a Raft state machine with a bunch of phones,

770
00:40:07,940 --> 00:40:10,670
it might be poor availability,

771
00:40:10,670 --> 00:40:13,220
but you've implemented the configuration changes,

772
00:40:13,220 --> 00:40:14,420
so in principle could that,

773
00:40:15,290 --> 00:40:17,390
really, I'm just trying to figure out what the role of the client is,

774
00:40:17,390 --> 00:40:17,840
in this whole thing,

775
00:40:17,840 --> 00:40:19,580
the server is a cluster of servers that makes sense,

776
00:40:19,580 --> 00:40:21,320
the applications that make sense, but.

777
00:40:22,270 --> 00:40:23,650
So basically the client,

778
00:40:23,680 --> 00:40:28,660
it's basically just like a nice way to like write the implementation,

779
00:40:28,930 --> 00:40:30,460
so the server and the client,

780
00:40:30,460 --> 00:40:32,110
you can basically think of that as a server,

781
00:40:32,260 --> 00:40:35,080
is just like in our implementation,

782
00:40:35,080 --> 00:40:38,500
it made sense to create this abstraction, yeah.

783
00:40:38,940 --> 00:40:40,110
Okay, thanks.

784
00:40:43,760 --> 00:40:45,500
In order for the round to progress,

785
00:40:45,500 --> 00:40:48,650
all peers must receive the message from all the other peers,

786
00:40:48,650 --> 00:40:50,090
where n-1 peers,

787
00:40:50,300 --> 00:40:52,940
and so if you have some very weird partition,

788
00:40:52,940 --> 00:40:56,660
where like some subset unable to talk to the rest of the subset,

789
00:40:56,720 --> 00:40:58,970
how would that be resolved here?

790
00:41:01,640 --> 00:41:04,850
Yeah, so if you, if you like partition into

791
00:41:04,850 --> 00:41:09,440
like smaller than, like if everyone is minorities,

792
00:41:09,470 --> 00:41:11,900
then then it would just be stuck forever,

793
00:41:12,080 --> 00:41:14,090
until you find a majority,

794
00:41:14,090 --> 00:41:15,650
and then once you have a majority,

795
00:41:15,650 --> 00:41:18,770
you can like they, they automatically submit,

796
00:41:18,770 --> 00:41:21,410
like remove server RPCs,

797
00:41:21,500 --> 00:41:24,110
whenever they don't get a response from someone,

798
00:41:24,110 --> 00:41:26,120
so then the cluster like shrinks

799
00:41:26,150 --> 00:41:29,870
until like the non responding nodes disappeared.

800
00:41:34,130 --> 00:41:36,950
I noticed in the demo example you gave,

801
00:41:37,010 --> 00:41:40,430
you had one person who wrote two of the messages,

802
00:41:40,490 --> 00:41:44,810
and so, someone else who wrote the third message,

803
00:41:44,840 --> 00:41:45,830
and that meant that you could,

804
00:41:45,890 --> 00:41:47,210
that the person who wrote two messages,

805
00:41:47,210 --> 00:41:50,300
could always tell what that third person had shared,

806
00:41:50,360 --> 00:41:53,000
doesn't that sort of defeat the purpose of your protocol?

807
00:41:55,040 --> 00:41:58,640
Yeah, yeah, so the reason why it was two messages,

808
00:41:58,640 --> 00:42:00,470
because we're running,

809
00:42:01,280 --> 00:42:04,190
we're running both of those instances on the same computer,

810
00:42:04,750 --> 00:42:07,930
that is, that is an attack though where,

811
00:42:08,050 --> 00:42:09,130
you know you have it,

812
00:42:09,160 --> 00:42:14,320
you start around where one person is controlled by everyone,

813
00:42:14,350 --> 00:42:16,240
well, like five of the servers,

814
00:42:16,240 --> 00:42:18,460
and five of them submitting messages are controlled by everyone,

815
00:42:18,460 --> 00:42:20,260
and you know who the last person is,

816
00:42:20,950 --> 00:42:25,970
but, I think so we currently consider

817
00:42:25,970 --> 00:42:29,180
that like a byzantine or non byzantine failures,

818
00:42:29,180 --> 00:42:30,920
so the clients are acting honestly,

819
00:42:30,920 --> 00:42:34,850
and they're not trying to gain an unfair amount of advantage in the system,

820
00:42:35,120 --> 00:42:38,720
and the attacker or someone who's listening to all the network traffic.

821
00:42:39,950 --> 00:42:43,910
Okay, so you're not worried about people involved in the system undermining it,

822
00:42:44,030 --> 00:42:45,680
you're only worried about people

823
00:42:45,680 --> 00:42:49,070
who are like peeking in on the system and driving it.

824
00:42:50,610 --> 00:42:51,210
Yeah.

825
00:42:51,210 --> 00:42:51,990
Or like kind of,

826
00:42:51,990 --> 00:42:55,140
so I think we're like decently confident that,

827
00:42:55,140 --> 00:42:58,620
like a if if they're byzantine participants,

828
00:42:58,620 --> 00:43:02,340
then, the protocol will fail, and it will fail,

829
00:43:02,430 --> 00:43:04,860
like explicitly, like and we,

830
00:43:04,860 --> 00:43:07,320
but it's like there are some different absolutely

831
00:43:07,320 --> 00:43:09,660
not exactly certain that that is the case,

832
00:43:09,870 --> 00:43:10,770
but we think so,

833
00:43:10,800 --> 00:43:12,090
but it's like it's.

834
00:43:12,090 --> 00:43:14,550
So for the attack that got discussed then,

835
00:43:14,580 --> 00:43:17,190
where someone takes control of the cluster,

836
00:43:17,220 --> 00:43:20,160
and but it takes over all the

837
00:43:20,160 --> 00:43:24,300
and essentially is all of the clients, except for one of them,

838
00:43:24,390 --> 00:43:26,040
and if you're a leader, you can control,

839
00:43:26,040 --> 00:43:28,830
presumably control who gets added or removed from the groups,

840
00:43:28,830 --> 00:43:30,360
you could make that happen,

841
00:43:31,660 --> 00:43:34,870
would that be detected by your current system?

842
00:43:35,490 --> 00:43:38,370
No, no, not not when it's like n-1,

843
00:43:38,460 --> 00:43:40,560
so we need at least two,

844
00:43:40,590 --> 00:43:43,260
if we have two honest people,

845
00:43:43,500 --> 00:43:44,940
then that would be enough,

846
00:43:44,970 --> 00:43:46,290
because they would be like,

847
00:43:46,350 --> 00:43:48,570
if they were disconnected from each other,

848
00:43:48,570 --> 00:43:49,830
they could know that.

849
00:43:50,300 --> 00:43:51,080
Yeah.

850
00:43:53,810 --> 00:43:54,740
Alright, cool, thank you.

851
00:43:57,960 --> 00:43:59,850
Overtime, thanks very cool,

852
00:43:59,880 --> 00:44:02,280
maybe a MIT concession will reach out.

853
00:44:02,940 --> 00:44:08,290
Let's hear from the fault tolerance project,

854
00:44:09,550 --> 00:44:11,350
maybe that's a bad way of summarizing.

855
00:44:12,370 --> 00:44:12,880
No worries.

856
00:44:13,840 --> 00:44:15,310
Yeah, everyone, I'm Ariel,

857
00:44:15,340 --> 00:44:18,220
I've been working on this project with [] for a bit now,

858
00:44:18,220 --> 00:44:20,140
so let's see I can share my screen.

859
00:44:22,050 --> 00:44:25,590
And so, yeah as you can see here,

860
00:44:25,590 --> 00:44:29,670
project involves giving fault tolerance for free at the 9P interface,

861
00:44:29,670 --> 00:44:33,990
and also break down what this means exactly and extra slides.

862
00:44:33,990 --> 00:44:36,270
We start by looking at serverless computation,

863
00:44:36,270 --> 00:44:40,020
so if you've heard of some things like AWS Lambda

864
00:44:40,110 --> 00:44:44,730
or GCP also Google Cloud Functions,

865
00:44:44,880 --> 00:44:47,700
essentially what this does is,

866
00:44:47,700 --> 00:44:51,720
instead of having to set a bunch of VMs and scale them

867
00:44:51,720 --> 00:44:53,460
with requests to your customer load everything,

868
00:44:53,460 --> 00:44:56,100
you can just write a small stateless function,

869
00:44:56,430 --> 00:44:59,640
develop your application through a bunch of small stateless functions,

870
00:44:59,640 --> 00:45:01,680
which are triggered over HTTP requests,

871
00:45:01,920 --> 00:45:07,140
and then AWS or Google cloud or who are your service providers

872
00:45:07,140 --> 00:45:10,410
will handle the scaling and all the infrastructure management for you,

873
00:45:10,880 --> 00:45:11,750
and this is nice,

874
00:45:11,750 --> 00:45:15,680
because in theory operators can balance resources more effectively,

875
00:45:15,680 --> 00:45:17,960
they can get high utilization,

876
00:45:18,380 --> 00:45:20,960
and in theory it's easier for developers as well,

877
00:45:20,960 --> 00:45:21,860
all you have to do is,

878
00:45:22,490 --> 00:45:25,880
is you don't have to worry about all this kind of infrastructure management,

879
00:45:25,880 --> 00:45:30,050
and sort of having enough headroom and your number of VMs and everything

880
00:45:30,050 --> 00:45:32,930
to handle spikes in customer load and things like this.

881
00:45:34,440 --> 00:45:38,100
However, the reality is not as nice as the theory,

882
00:45:38,280 --> 00:45:42,000
which is, in the end,

883
00:45:42,350 --> 00:45:44,150
in current implementations,

884
00:45:44,210 --> 00:45:48,650
actually building these complex applications is quite difficult,

885
00:45:49,450 --> 00:45:53,050
and decomposing them into these stateless functions quite difficult,

886
00:45:53,050 --> 00:45:55,090
in part because these Lambdas,

887
00:45:55,090 --> 00:45:59,500
these stateless functions lack rich communication primitives,

888
00:46:00,220 --> 00:46:03,880
which would be important support to build complex applications,

889
00:46:03,910 --> 00:46:07,120
so they can't directly communicate or through to each other,

890
00:46:07,720 --> 00:46:10,750
therefore systems that try to utilize serverless functions,

891
00:46:10,780 --> 00:46:15,520
have had to sort of build their own custom communication solutions,

892
00:46:15,550 --> 00:46:18,310
like maybe setting up the ends on the side,

893
00:46:18,310 --> 00:46:19,780
that they round it talk through,

894
00:46:19,780 --> 00:46:22,840
or talking through a storage systems, like S3,

895
00:46:22,840 --> 00:46:24,010
and this makes them inefficient

896
00:46:24,010 --> 00:46:27,700
and difficult for developers to actually put together in a sensible way,

897
00:46:29,290 --> 00:46:33,580
all this complexity causes data centers to actually manage resources quite poorly,

898
00:46:33,610 --> 00:46:39,160
and and the problem is that,

899
00:46:39,430 --> 00:46:42,640
we still have all these data center resources

900
00:46:42,640 --> 00:46:45,340
segmented into a bunch of local name spaces,

901
00:46:45,340 --> 00:46:46,420
which are [],

902
00:46:46,420 --> 00:46:49,480
and and you can't share resources across them.

903
00:46:50,790 --> 00:46:53,190
So what we were looking at was

904
00:46:53,190 --> 00:46:59,160
[] some work from past decades called Plan9,

905
00:46:59,250 --> 00:47:04,260
and Plan9 are essentially provides plan for unifying data center resources,

906
00:47:04,320 --> 00:47:06,390
and providing single system image,

907
00:47:06,920 --> 00:47:09,140
what this means essentially when you write your application,

908
00:47:09,170 --> 00:47:11,780
you don't have to worry about what runs on what machines,

909
00:47:11,780 --> 00:47:14,900
about provisioning VMs or anything like that,

910
00:47:15,170 --> 00:47:20,600
or making sure that certain parts of the application on certain VMs,

911
00:47:20,600 --> 00:47:25,680
it all looks like one giants namespace to your application,

912
00:47:25,800 --> 00:47:29,910
so your application knows nothing about what hardware it's actually running on,

913
00:47:29,910 --> 00:47:33,210
or where other services are in the data center,

914
00:47:33,450 --> 00:47:35,820
the ways achieved is

915
00:47:35,850 --> 00:47:40,380
applications services communicate through a global hierarchical namespace,

916
00:47:40,970 --> 00:47:46,820
services and resources expose uniform file system like interface,

917
00:47:47,090 --> 00:47:52,190
you don't have to worry about implementing and calling custom RPCs.

918
00:47:52,190 --> 00:47:55,590
And so for example and this, in this tree,

919
00:47:55,590 --> 00:47:56,670
you can see here,

920
00:47:56,700 --> 00:48:01,560
we have the top level, root level name space,

921
00:48:01,560 --> 00:48:04,620
and then under that we have three services registered,

922
00:48:04,620 --> 00:48:06,180
we have an S3 service,

923
00:48:06,180 --> 00:48:11,010
which you can use to connect your Lambdas to S3,

924
00:48:11,010 --> 00:48:15,440
you can access keys, S3 keys and values through that,

925
00:48:15,440 --> 00:48:18,800
we have an in-memory file system and sched

926
00:48:19,010 --> 00:48:20,980
and so quickly.

927
00:48:21,790 --> 00:48:28,290
Yeah, so this is what the what like a typical 9P namespace would look like.

928
00:48:29,550 --> 00:48:32,970
We have done some work to implement

929
00:48:32,970 --> 00:48:37,410
example of of this 9P architecture,

930
00:48:37,590 --> 00:48:40,620
we have a, like a zookeeper, like configuration service,

931
00:48:40,620 --> 00:48:42,510
which hosts the top level name space,

932
00:48:42,630 --> 00:48:47,520
we have a scheduler and a bunch of these np servers, 9P servers,

933
00:48:47,880 --> 00:48:48,900
what these do is,

934
00:48:48,900 --> 00:48:51,990
they expose resources and functionality, such as the,

935
00:48:52,050 --> 00:48:53,280
if we go back here,

936
00:48:53,370 --> 00:48:55,140
this S3 part of namespace,

937
00:48:55,140 --> 00:49:00,120
you can write a service which connected S3 buckets,

938
00:49:00,120 --> 00:49:02,010
and and expose it to 9P

939
00:49:02,010 --> 00:49:03,780
and this would be a 9P server.

940
00:49:05,250 --> 00:49:08,890
And the RPCs or the operations

941
00:49:08,890 --> 00:49:12,670
that used to interact with these different services

942
00:49:12,670 --> 00:49:15,310
are all the 9P operations,

943
00:49:15,310 --> 00:49:19,060
so it's a very small, well defined set of operations,

944
00:49:19,060 --> 00:49:21,160
that you can see a few of them here,

945
00:49:21,160 --> 00:49:24,490
such as Read and Write Remove Stat,

946
00:49:24,490 --> 00:49:27,820
small number of canonical file system operations.

947
00:49:29,470 --> 00:49:31,510
I so quickly demo what this looks like,

948
00:49:31,630 --> 00:49:34,930
so for example here,

949
00:49:34,930 --> 00:49:36,850
I can start up correct infrastructure.

950
00:49:40,690 --> 00:49:42,130
So I can look into,

951
00:49:42,160 --> 00:49:45,010
you can map this namespace right into Linux,

952
00:49:45,670 --> 00:49:49,060
so I can see some services,

953
00:49:49,060 --> 00:49:53,140
and different namespaces that we've mounted here,

954
00:49:53,140 --> 00:49:55,150
for example if I look at S3,

955
00:49:58,610 --> 00:50:03,410
this is actually connecting to S3 bucket that we have,

956
00:50:03,980 --> 00:50:09,440
do you actually see the keys in the S3 bucket,

957
00:50:09,440 --> 00:50:11,210
just as files in your file system,

958
00:50:11,510 --> 00:50:14,600
and this is S3 bucket over here,

959
00:50:14,960 --> 00:50:18,410
so if I want to create, write a new key, for example,

960
00:50:18,590 --> 00:50:24,620
all I need to do is say cat, echo to,

961
00:50:30,090 --> 00:50:35,250
so instead of having to use AWS custom library,

962
00:50:35,250 --> 00:50:37,680
in order to so write S3,

963
00:50:37,680 --> 00:50:40,350
I can just write a file system,

964
00:50:40,350 --> 00:50:43,050
do a file system operation to create a file and write to it

965
00:50:43,350 --> 00:50:48,140
and then S3, this actually should show up as a new key now,

966
00:50:49,600 --> 00:50:52,030
so we have test456, which we just wrote.

967
00:50:56,630 --> 00:50:59,810
So, what does fault tolerance part coming here,

968
00:50:59,870 --> 00:51:01,490
well, as I mentioned before,

969
00:51:01,490 --> 00:51:05,510
all these 9P services have a very well defined uniform interface,

970
00:51:05,540 --> 00:51:07,370
there are no custom RPCs,

971
00:51:07,370 --> 00:51:10,400
and this gives us a very unique opportunity,

972
00:51:10,400 --> 00:51:16,460
which is we can [slice] this 9P interface to replicate unmodified services,

973
00:51:16,490 --> 00:51:23,540
so if if I am able to replicate all these operations to different server, instances of servers,

974
00:51:23,540 --> 00:51:29,210
then servers gets replicated free without having to modify it at all,

975
00:51:29,270 --> 00:51:30,080
so this is what we did,

976
00:51:30,080 --> 00:51:31,040
when I worked on,

977
00:51:31,040 --> 00:51:39,460
I implemented chain replication based fault tolerance scheme,

978
00:51:39,880 --> 00:51:44,920
I use named is like a configuration, configuration service,

979
00:51:44,920 --> 00:51:47,110
you can think of this as like a Zookeeper,

980
00:51:47,140 --> 00:51:51,430
and I replicated 2 different services without any modification,

981
00:51:51,430 --> 00:51:52,690
and in-memory file system

982
00:51:52,690 --> 00:51:59,410
and a service which exposes durable storage from a local machines,

983
00:51:59,530 --> 00:52:02,560
and so I'll quickly show them that.

984
00:52:03,460 --> 00:52:05,350
So let me stop this,

985
00:52:05,380 --> 00:52:05,890
let's see.

986
00:52:08,180 --> 00:52:12,980
So, for example I can,

987
00:52:20,000 --> 00:52:21,770
so I can start up a bunch of replicates,

988
00:52:32,840 --> 00:52:36,710
so if I look into the 9P namespace module in Linux,

989
00:52:36,980 --> 00:52:38,780
you see this memfs-replica,

990
00:52:39,200 --> 00:52:41,600
here's where these replicas register themselves,

991
00:52:44,590 --> 00:52:45,520
so we can see that,

992
00:52:45,520 --> 00:52:47,590
we have five of these replicas,

993
00:52:47,620 --> 00:52:49,600
I can write to one of them.

994
00:52:53,640 --> 00:52:54,210
Oops.

995
00:52:58,110 --> 00:53:00,240
Okay, so I can write to one of the replicas,

996
00:53:00,450 --> 00:53:02,190
and then I can read from another one group,

997
00:53:02,190 --> 00:53:06,880
because we should gets the same result out,

998
00:53:06,880 --> 00:53:11,520
wrote small string to file in the first replica,

999
00:53:11,550 --> 00:53:12,990
you can read from the last replica,

1000
00:53:13,020 --> 00:53:14,220
should get the same thing back,

1001
00:53:14,610 --> 00:53:19,170
can even crash replica, because,

1002
00:53:22,590 --> 00:53:26,070
so, let's kill one of these guys,

1003
00:53:32,960 --> 00:53:34,250
we can see this,

1004
00:53:37,650 --> 00:53:40,440
see there are only four replicas left now in the namespace,

1005
00:53:40,770 --> 00:53:45,570
then write a different string to this file,

1006
00:53:48,910 --> 00:53:51,460
and, I should,

1007
00:53:52,300 --> 00:53:57,490
yeah, so, so now, despite a failure,

1008
00:53:57,520 --> 00:53:59,320
the service automatically reconfigured,

1009
00:53:59,800 --> 00:54:05,620
and the in-memory file systems services based of,

1010
00:54:05,620 --> 00:54:06,880
did not have to be modified at all,

1011
00:54:06,880 --> 00:54:09,430
you can just plug in any new service you want to,

1012
00:54:09,430 --> 00:54:11,290
that implements this 9P interface,

1013
00:54:11,290 --> 00:54:13,120
and it should work out of the box.

1014
00:54:15,100 --> 00:54:17,890
So, I think that's,

1015
00:54:18,460 --> 00:54:22,150
I think that sort of concludes my presentation,

1016
00:54:22,150 --> 00:54:23,950
and I'd be happy to take any more questions,

1017
00:54:24,430 --> 00:54:25,210
you guys might have.

1018
00:54:33,330 --> 00:54:35,730
I regret not wearing my Plan9 shirt this time,

1019
00:54:37,530 --> 00:54:38,190
I should have,

1020
00:54:38,250 --> 00:54:39,270
I'm curious though,

1021
00:54:39,450 --> 00:54:43,050
I'm working on something that was sort of similar earlier this semester,

1022
00:54:43,050 --> 00:54:45,180
except there was no replication,

1023
00:54:45,180 --> 00:54:48,360
it was just 9P services under Linux box,

1024
00:54:48,420 --> 00:54:50,820
why, this is sort of an offhand question,

1025
00:54:50,820 --> 00:54:51,900
why chain replication,

1026
00:54:52,580 --> 00:54:55,070
I imagine there are other replication schemes,

1027
00:54:55,490 --> 00:54:58,100
that are available for this sort of thing,

1028
00:54:58,100 --> 00:54:59,630
and in theory, because this is 9P,

1029
00:54:59,630 --> 00:55:02,330
you could probably bind all these replicas over the same namespace,

1030
00:55:02,330 --> 00:55:03,440
they look like, it's a machine,

1031
00:55:03,440 --> 00:55:05,120
but why chain replication?

1032
00:55:06,480 --> 00:55:08,040
Yeah, yeah, good question,

1033
00:55:08,040 --> 00:55:10,950
you could in theory do any any replication scheme you'd like,

1034
00:55:10,980 --> 00:55:12,120
under the hood,

1035
00:55:12,210 --> 00:55:14,310
I did chain replication,

1036
00:55:14,820 --> 00:55:18,560
because it seemed like a simple starting point, I guess,

1037
00:55:18,560 --> 00:55:21,380
I could also throw this on top of a Raft implementation,

1038
00:55:21,380 --> 00:55:23,120
like the entry [plus] or anything like that,

1039
00:55:24,530 --> 00:55:26,120
it's something fundamental about the choice.

1040
00:55:31,840 --> 00:55:36,550
Does your system support adding additional replicas after it started?

1041
00:55:36,880 --> 00:55:38,140
Yeah, great question,

1042
00:55:38,170 --> 00:55:43,300
so you already know that's a work in progress,

1043
00:55:44,060 --> 00:55:50,030
but, yeah, currently we don't support adding additional replicas out of box.

1044
00:55:56,500 --> 00:55:57,640
This is an open question,

1045
00:55:57,640 --> 00:55:59,530
it's sort of like you may not know this,

1046
00:55:59,530 --> 00:56:01,090
this wasn't the express intent of the purpose,

1047
00:56:01,090 --> 00:56:04,060
but one of the things that's really cool about Plan9 and 9P is that,

1048
00:56:04,270 --> 00:56:06,640
you treat network connections like their files as well,

1049
00:56:06,850 --> 00:56:08,980
so you're like you have like memfs,

1050
00:56:08,980 --> 00:56:11,680
you have, from what I saw, a scheduler queue was implemented,

1051
00:56:11,680 --> 00:56:12,550
just set of files,

1052
00:56:14,330 --> 00:56:16,280
do you and this is an open question,

1053
00:56:16,280 --> 00:56:19,640
because the question of whether network stuff over file interfaces,

1054
00:56:19,640 --> 00:56:21,890
scalable is difficult to answer,

1055
00:56:21,920 --> 00:56:25,130
but do you also implement this in sort of that way

1056
00:56:25,130 --> 00:56:27,770
or do you use more traditional [] interfaces to,

1057
00:56:27,920 --> 00:56:29,450
when the clients use this thing,

1058
00:56:29,450 --> 00:56:32,300
they presume they use traditional sockets and things to communicate.

1059
00:56:33,400 --> 00:56:35,110
Yeah, yeah, good question,

1060
00:56:35,110 --> 00:56:36,790
so yeah, everything,

1061
00:56:36,820 --> 00:56:42,910
so all the clients and services communicate over TCP at the moment,

1062
00:56:44,250 --> 00:56:46,440
and yeah it is a good question

1063
00:56:46,440 --> 00:56:48,930
to whether the 9P namespaces actually performance enough,

1064
00:56:48,930 --> 00:56:50,100
for what we're gonna do with it,

1065
00:56:50,190 --> 00:56:52,200
as far as we can see now,

1066
00:56:52,230 --> 00:56:53,970
there's not,

1067
00:56:55,050 --> 00:56:57,660
so we've done some performance benchmarking,

1068
00:56:57,660 --> 00:57:04,020
to see how well like we've written a scheduler over 9P,

1069
00:57:04,640 --> 00:57:08,150
and we've done some performance benchmarking to see how it performs,

1070
00:57:08,150 --> 00:57:10,280
and it seems to not add a ton of overhead at the moment,

1071
00:57:10,280 --> 00:57:15,250
but, alright, we can imagine those trade-offs changing for different types of services,

1072
00:57:15,430 --> 00:57:18,850
and as the scheduler becomes more or less oversubscribed.

1073
00:57:20,750 --> 00:57:21,950
I guess, I guess to clarify,

1074
00:57:21,950 --> 00:57:24,410
so sorry I'm taking up taking up a lot of time,

1075
00:57:24,410 --> 00:57:27,110
but just briefly the whole 9P,

1076
00:57:27,110 --> 00:57:29,120
if you if you were able to,

1077
00:57:29,120 --> 00:57:30,350
and this is again very open,

1078
00:57:30,350 --> 00:57:32,510
but if you were able to treat network connections as files,

1079
00:57:32,510 --> 00:57:34,220
you have replicas, right,

1080
00:57:34,250 --> 00:57:36,690
you could have something for each card,

1081
00:57:36,690 --> 00:57:38,850
then shard network traffic over those network cards,

1082
00:57:38,850 --> 00:57:40,710
which I guess this is serverless anyway,

1083
00:57:40,710 --> 00:57:42,720
so it probably doesn't make that much of a difference,

1084
00:57:42,840 --> 00:57:43,500
yeah, forget it,

1085
00:57:45,900 --> 00:57:48,480
because you already have the handles for you.

1086
00:57:50,440 --> 00:57:51,340
Thank you.

1087
00:57:52,150 --> 00:57:52,870
Thank you.

1088
00:57:54,130 --> 00:57:57,940
Alright, so let's hear about some verification stuff, if you're ready.

1089
00:58:02,730 --> 00:58:03,870
Can you guys hear me, okay?

1090
00:58:04,490 --> 00:58:04,940
Yes.

1091
00:58:05,590 --> 00:58:06,820
Alright, excellent,

1092
00:58:07,090 --> 00:58:10,090
so indeed, I'm gonna talk to you about my project,

1093
00:58:10,090 --> 00:58:13,540
which is focused on modular verification for distributed systems.

1094
00:58:13,870 --> 00:58:17,650
So let's start by answering the obvious question,

1095
00:58:17,650 --> 00:58:20,440
which is why bother with you know this stuff,

1096
00:58:20,560 --> 00:58:24,070
and I think anyone that's worked in the lab for 6.824

1097
00:58:24,070 --> 00:58:25,420
must have discovered at some point,

1098
00:58:25,420 --> 00:58:28,660
that getting this [] right is hard,

1099
00:58:28,660 --> 00:58:32,380
there's a lot of non determinism caused by concurrency and network failure,

1100
00:58:32,470 --> 00:58:35,770
and that makes it very difficult to exhaust the test

1101
00:58:35,770 --> 00:58:37,630
to make sure there's no corner-case bugs,

1102
00:58:38,680 --> 00:58:44,140
verification is an alternative to, alternative approach testing to try and get correctness,

1103
00:58:44,230 --> 00:58:47,200
and in principle, it can entirely rule out bugs,

1104
00:58:47,440 --> 00:58:50,710
with verification, you've basically mathematically modeled your system,

1105
00:58:50,890 --> 00:58:53,290
and improve some theorem about that model,

1106
00:58:53,910 --> 00:58:57,600
and you know one of the downsides of verification is that,

1107
00:58:57,600 --> 00:58:59,100
it's quite a lot of work,

1108
00:58:59,280 --> 00:59:01,170
during these formal proofs is,

1109
00:59:01,170 --> 00:59:02,310
by no means easy,

1110
00:59:02,610 --> 00:59:04,860
and even if it was easy,

1111
00:59:04,860 --> 00:59:07,320
verification still wouldn't be a perfect silver bullet,

1112
00:59:07,590 --> 00:59:11,400
for one verification, you have to make sure that you get your specification right,

1113
00:59:11,640 --> 00:59:14,700
if the mathematical theorem you're proving by your system

1114
00:59:14,760 --> 00:59:17,610
doesn't actually say what you really wanted to say,

1115
00:59:17,640 --> 00:59:19,290
then what you've proved is useless,

1116
00:59:19,380 --> 00:59:21,030
and relatedly,

1117
00:59:21,030 --> 00:59:24,600
you have to make sure that the model that you have to [] them is also complete,

1118
00:59:24,930 --> 00:59:29,460
if you fail to model some execution that can happen in reality,

1119
00:59:29,460 --> 00:59:30,690
but you don't consider,

1120
00:59:30,690 --> 00:59:33,870
then your theorem wouldn't apply to the real world.

1121
00:59:35,190 --> 00:59:39,210
And some of you that are familiar with some distributed verification work,

1122
00:59:39,210 --> 00:59:42,150
might say oh, don't we already know how to do this,

1123
00:59:42,150 --> 00:59:44,100
indeed distributed systems have always been hard,

1124
00:59:44,100 --> 00:59:47,430
and people have recently worked on a project

1125
00:59:47,430 --> 00:59:50,670
to try to verify actual implementations of distributed systems,

1126
00:59:51,000 --> 00:59:53,670
so some of these projects include IronFleet and Verdi,

1127
00:59:54,210 --> 00:59:58,380
however, these projects didn't focus much on modularity,

1128
00:59:58,710 --> 01:00:02,520
or trying to prove reusable specifications for components of systems,

1129
01:00:02,580 --> 01:00:04,680
trying to build more complicated systems out of them,

1130
01:00:05,400 --> 01:00:09,180
and I'd argue that that's the way that distributed systems are actually built,

1131
01:00:09,300 --> 01:00:11,850
the way you build a distributed system is by

1132
01:00:11,850 --> 01:00:14,070
oftentimes using building blocks,

1133
01:00:14,190 --> 01:00:17,850
like key values services and lock services or or Zookeeper,

1134
01:00:17,880 --> 01:00:19,230
and putting them together

1135
01:00:19,230 --> 01:00:22,080
with some you know added code and novel functionality

1136
01:00:22,170 --> 01:00:24,570
to build your more interesting and more useful system,

1137
01:00:25,840 --> 01:00:28,180
and are sort of thesis if you will,

1138
01:00:28,300 --> 01:00:33,160
is that verification can and should exploit this compositionality,

1139
01:00:33,190 --> 01:00:35,110
as one sort of targeted goal,

1140
01:00:35,110 --> 01:00:38,350
we aim to prove specifications for client systems,

1141
01:00:38,650 --> 01:00:41,470
prior work like IronFleet Verdi simply reason about,

1142
01:00:41,470 --> 01:00:44,530
what the behavior of the actual server side [] looks like,

1143
01:00:44,680 --> 01:00:46,420
and don't explicitly model

1144
01:00:46,420 --> 01:00:49,210
or prove anything about what the client programs actually do,

1145
01:00:49,270 --> 01:00:51,220
and oftentimes there's a bit of logic on the client,

1146
01:00:51,400 --> 01:00:53,380
that's crucial for getting correctness,

1147
01:00:53,920 --> 01:00:55,780
and the approach we use is

1148
01:00:55,780 --> 01:00:58,660
to use advances in concurrent separation logic,

1149
01:00:58,960 --> 01:01:02,140
which is a compositional means of reasoning about concurrent programs,

1150
01:01:02,140 --> 01:01:05,920
that's lately become popular for, popular

1151
01:01:05,920 --> 01:01:08,740
and demonstrate to be successful at reason about real code.

1152
01:01:09,800 --> 01:01:12,230
So the first example that we worked on,

1153
01:01:12,230 --> 01:01:15,080
was verifying a sharded key-value system,

1154
01:01:15,080 --> 01:01:18,770
the keys in this are statically split up into shards,

1155
01:01:19,040 --> 01:01:22,490
and shards themselves can be moved between the shard servers,

1156
01:01:22,700 --> 01:01:25,670
so it's very similar to lab 4 of of 6.824,

1157
01:01:25,730 --> 01:01:27,650
except that it's not replicated,

1158
01:01:27,650 --> 01:01:29,360
so there's no there's no Raft in this,

1159
01:01:29,480 --> 01:01:31,130
and it's purely in memory.

1160
01:01:31,520 --> 01:01:36,140
Besides that, our system also has sharded servers and a coordinator server,

1161
01:01:36,140 --> 01:01:38,960
and the coordinator is the one that tells other shard servers

1162
01:01:38,960 --> 01:01:40,700
to move shard between themselves

1163
01:01:40,700 --> 01:01:43,490
as you know want to join or as you need to rebalance,

1164
01:01:44,360 --> 01:01:46,970
the top-level of library that we provide,

1165
01:01:46,970 --> 01:01:49,610
and that we want to prove specification for is

1166
01:01:49,610 --> 01:01:51,110
you know we call it a KVClerk,

1167
01:01:51,110 --> 01:01:54,080
which is a client object that one can use,

1168
01:01:54,080 --> 01:01:56,000
and we call these three functions on

1169
01:01:56,090 --> 01:01:57,980
to actually interact with the server,

1170
01:01:57,980 --> 01:02:00,830
so there's a Put, you you say what value to put in the key,

1171
01:02:00,860 --> 01:02:03,080
there's a Get, which will return the current value in the key,

1172
01:02:03,320 --> 01:02:04,610
and then there's a ConditionalPut,

1173
01:02:04,640 --> 01:02:09,140
which will only put the new value, if the old value is the expected one,

1174
01:02:10,020 --> 01:02:15,270
and we aim to basically implement, implement a linearizable key-value service,

1175
01:02:15,270 --> 01:02:18,300
and approve a specification that shows linearizable,

1176
01:02:18,750 --> 01:02:22,320
and the way you do this in the separation logic style

1177
01:02:22,320 --> 01:02:25,320
is basically by writing a specification that look a lot like this,

1178
01:02:25,680 --> 01:02:30,870
so this basically says that, if the object ck is a KVClerk,

1179
01:02:31,140 --> 01:02:35,070
then you have specifications for the Put and Get functions,

1180
01:02:35,310 --> 01:02:39,420
that say for example, if you start running the Put function,

1181
01:02:39,510 --> 01:02:43,860
with the precondition that key k has value w,

1182
01:02:43,980 --> 01:02:48,030
then by the end of it, you'll know that key k as value v,

1183
01:02:48,720 --> 01:02:51,720
similarly, if you do a Get,

1184
01:02:51,720 --> 01:02:54,540
and you know that key k has value v at the beginning,

1185
01:02:54,540 --> 01:02:56,250
then that's the thing that's going to be returned,

1186
01:02:56,460 --> 01:02:58,650
and you're still going to know that's the value of the key,

1187
01:02:59,160 --> 01:03:01,380
although the specifications look pretty simple,

1188
01:03:01,380 --> 01:03:04,200
and you might think, of course the key-value service does,

1189
01:03:04,350 --> 01:03:05,340
and that's sort of the point,

1190
01:03:05,340 --> 01:03:09,390
that these top-level client specifications are as simple as they can be,

1191
01:03:09,390 --> 01:03:12,360
and and basically hide all the details of the fact,

1192
01:03:12,360 --> 01:03:14,130
that there's multiple shard servers,

1193
01:03:14,130 --> 01:03:17,910
and that this clerk library might need to talk to servers multiple times,

1194
01:03:17,910 --> 01:03:19,680
it might need to refresh its information

1195
01:03:19,800 --> 01:03:22,500
about which server owns the you know which keys,

1196
01:03:22,650 --> 01:03:27,150
and we basically you know prove respect that allows you to forget all that,

1197
01:03:27,150 --> 01:03:29,940
and use a key-value service just by calling these puts and gets,

1198
01:03:30,060 --> 01:03:31,350
and having this idealized notion

1199
01:03:31,350 --> 01:03:33,990
of what the key-value mapping actually looks like.

1200
01:03:34,750 --> 01:03:36,430
So I won't talk too much more in detail

1201
01:03:36,430 --> 01:03:38,020
about what the actual proof looks like,

1202
01:03:38,230 --> 01:03:40,270
instead to focus to,

1203
01:03:40,390 --> 01:03:43,420
issue focus towards the next thing we were interested in doing,

1204
01:03:43,600 --> 01:03:46,990
which is actually doing something with a bit of fault tolerance to this.

1205
01:03:47,050 --> 01:03:50,080
So like I mentiond, the key-value itself is not replicated,

1206
01:03:50,080 --> 01:03:51,430
and isn't fault tolerant,

1207
01:03:51,580 --> 01:03:53,350
and so we started by trying to figure out

1208
01:03:53,350 --> 01:03:57,550
how to verify the simplest possible fault tolerant protocol,

1209
01:03:57,640 --> 01:04:00,550
and we basically started of Single-decree Paxos,

1210
01:04:00,730 --> 01:04:04,630
Single-decree Paxos is a classic protocol for getting consensus on a single value,

1211
01:04:05,060 --> 01:04:06,290
so whereas with a Raft,

1212
01:04:06,380 --> 01:04:08,870
you can get you replicate entire log

1213
01:04:08,870 --> 01:04:10,460
and you keep appending new entries to log,

1214
01:04:10,490 --> 01:04:13,820
Single-decree Paxos is the [] of Multi Paxos,

1215
01:04:13,820 --> 01:04:16,160
and basically functions of Write-Once Register,

1216
01:04:16,280 --> 01:04:19,220
if you want to set the value to something,

1217
01:04:19,250 --> 01:04:21,320
you can attempt to write to it,

1218
01:04:21,590 --> 01:04:22,760
and if someone else beat you,

1219
01:04:22,760 --> 01:04:24,830
then, that's too bad for you,

1220
01:04:24,830 --> 01:04:26,180
and now the value has already been decided,

1221
01:04:26,180 --> 01:04:27,350
and it's never going to change again.

1222
01:04:28,080 --> 01:04:32,700
So we implemented and partially verified a Single-decree Paxos implementation,

1223
01:04:32,790 --> 01:04:36,600
and basically prove a specification that shows that it's a Write-Once Register,

1224
01:04:37,560 --> 01:04:41,520
and the key idea in the specification and the proof is that,

1225
01:04:41,550 --> 01:04:44,610
when you commit a value in in Single-decree Paxos,

1226
01:04:44,850 --> 01:04:48,840
you get irrevocable knowledge of what that a committed value is,

1227
01:04:48,900 --> 01:04:50,790
and you basically know that from here on out,

1228
01:04:50,790 --> 01:04:53,430
if anybody else ever sees any committed value,

1229
01:04:53,550 --> 01:04:56,160
it's going to be the exact same thing that you see right now.

1230
01:04:57,190 --> 01:05:00,400
And thinking about this a little bit after we worked on the proof of it,

1231
01:05:00,910 --> 01:05:02,740
we sort of thinking that,

1232
01:05:02,800 --> 01:05:03,970
we sort of notice that,

1233
01:05:03,970 --> 01:05:07,240
there's a slight generalization you can do to Single-decree Paxos,

1234
01:05:07,240 --> 01:05:10,900
which a we all Monotone Paxos for lack of better name,

1235
01:05:11,320 --> 01:05:12,370
and the idea is,

1236
01:05:12,370 --> 01:05:16,210
rather than gaining knowledge about the exact value upon a commit,

1237
01:05:16,480 --> 01:05:19,450
you instead can we modify the protocol,

1238
01:05:19,630 --> 01:05:22,930
so that you only gain knowledge about a lower bound on the value,

1239
01:05:23,570 --> 01:05:27,410
so basically when you commit a value,

1240
01:05:27,470 --> 01:05:30,410
for example you commit the number 15 to this Write-Once Register,

1241
01:05:30,500 --> 01:05:32,960
rather than knowing that 15 is the only value

1242
01:05:32,960 --> 01:05:34,550
anybody else in the future will going to see,

1243
01:05:34,760 --> 01:05:37,130
you'll know that any value that people see in the future,

1244
01:05:37,130 --> 01:05:40,100
as committed is going to be at least 15,

1245
01:05:40,100 --> 01:05:42,470
and so of course doing this requires having some notion

1246
01:05:42,470 --> 01:05:45,200
of what larger than actually means for the value type.

1247
01:05:45,550 --> 01:05:48,790
And the key idea is

1248
01:05:48,940 --> 01:05:53,320
a replica can always find out what the latest committed value is,

1249
01:05:53,410 --> 01:05:54,670
and choose to increase it,

1250
01:05:54,850 --> 01:05:57,070
and other replicates can continually find out

1251
01:05:57,070 --> 01:06:00,970
larger and larger lower bounds on basically the value so far is,

1252
01:06:01,680 --> 01:06:05,700
and once we sort of came up with this idea of Monotone Paxos,

1253
01:06:05,760 --> 01:06:07,530
we realized immediately,

1254
01:06:07,530 --> 01:06:09,780
that we can do log replication with this,

1255
01:06:10,200 --> 01:06:13,020
so the set of values V,

1256
01:06:13,050 --> 01:06:15,600
we can choose to simply be all the logs,

1257
01:06:15,630 --> 01:06:18,540
that you might want to replicate your logs of operations,

1258
01:06:18,660 --> 01:06:21,060
and we can define one log to be bigger than another one,

1259
01:06:21,060 --> 01:06:25,110
if the smaller one is a prefix of, if l1 is a prefix of l2,

1260
01:06:26,820 --> 01:06:29,040
and this basically allows us to know,

1261
01:06:29,040 --> 01:06:31,770
this sort of yield a protocol which you'll have to trust,

1262
01:06:31,770 --> 01:06:33,000
that I'm showing you the code for,

1263
01:06:33,300 --> 01:06:38,070
in which you can gain information about what the prefix of the log is,

1264
01:06:38,340 --> 01:06:40,560
and over time you can add new things to log,

1265
01:06:40,560 --> 01:06:41,790
by making it larger and larger,

1266
01:06:41,790 --> 01:06:43,500
and that's pretty much exactly what we mean,

1267
01:06:43,500 --> 01:06:44,580
when we say log replication.

1268
01:06:45,380 --> 01:06:48,500
The problem with this exact protocol is that,

1269
01:06:48,500 --> 01:06:52,790
naively, we sort of implement the most naive version of this Monotone Paxos thing,

1270
01:06:53,000 --> 01:06:56,060
you would need to send around the full log on every single RPC,

1271
01:06:56,300 --> 01:06:57,320
in Single-decree Paxos,

1272
01:06:57,320 --> 01:07:00,320
you sent around the full value on all the RPCs,

1273
01:07:00,410 --> 01:07:04,490
and the you know trivial generalization of the model in Paxos

1274
01:07:04,490 --> 01:07:05,840
would have to be sent around the full log,

1275
01:07:06,080 --> 01:07:08,990
that's not really useful, the log to get larger and larger and larger,

1276
01:07:08,990 --> 01:07:11,930
and the begin of log is no longer relevant by the time,

1277
01:07:11,930 --> 01:07:13,070
everybody agreed to commit,

1278
01:07:13,070 --> 01:07:13,880
and all that,

1279
01:07:14,150 --> 01:07:17,810
so you could try to optimize this by only passing around a suffix of the log,

1280
01:07:18,170 --> 01:07:21,860
and indeed, there's a whole sequence of optimization,

1281
01:07:21,860 --> 01:07:25,100
you can make to this Monotone Paxos based log replication,

1282
01:07:25,460 --> 01:07:27,770
and as you start doing more and more of this,

1283
01:07:27,770 --> 01:07:30,470
you'll realize that this looks exactly like Raft.

1284
01:07:31,010 --> 01:07:36,350
And in fact, we aimed basically to use our idea of Monotone Paxos

1285
01:07:36,380 --> 01:07:38,930
not to implement a new replication protocol,

1286
01:07:38,990 --> 01:07:42,050
but rather to verify a Raft like system,

1287
01:07:42,320 --> 01:07:44,330
so we have this proof for Single-decree Paxos,

1288
01:07:44,450 --> 01:07:47,120
we have this clear generalization to this Monotone Paxos thing,

1289
01:07:47,690 --> 01:07:50,120
and our hope is that we can use the idea of Monotone Paxos,

1290
01:07:50,120 --> 01:07:52,910
to basically verify Raft directly,

1291
01:07:52,970 --> 01:07:57,800
as opposed to rely on the much more complicated correctness argued for Raft,

1292
01:07:58,010 --> 01:08:01,670
that has been described in sort of other state machine type styles.

1293
01:08:02,220 --> 01:08:04,740
So this is our future work,

1294
01:08:05,010 --> 01:08:09,960
and the key takeaway I I sort of want to leave you guys with is that,

1295
01:08:10,020 --> 01:08:13,470
reasoning both formal and informal about distributed systems,

1296
01:08:13,710 --> 01:08:15,840
should be as compositional as writing code,

1297
01:08:16,230 --> 01:08:18,030
the way you scale writing code is modular,

1298
01:08:18,030 --> 01:08:20,070
and that's the way reasoning should also scale.

1299
01:08:21,680 --> 01:08:24,530
And that's all for my presentation,

1300
01:08:24,530 --> 01:08:26,090
I'm happy to take questions.

1301
01:08:35,610 --> 01:08:36,810
If the answer is too long,

1302
01:08:36,810 --> 01:08:38,190
you can look me in the chat,

1303
01:08:38,190 --> 01:08:38,850
but I was curious,

1304
01:08:38,850 --> 01:08:39,840
I know there's a [],

1305
01:08:39,840 --> 01:08:40,710
do you have any resources

1306
01:08:40,710 --> 01:08:44,400
for somebody interested in getting into as a from a software perspective,

1307
01:08:44,400 --> 01:08:46,050
like any brief recommendations.

1308
01:08:46,780 --> 01:08:51,130
Are you, so yeah, I guess I'm not quite sure what the,

1309
01:08:51,780 --> 01:08:53,100
so, are you interested in like,

1310
01:08:53,100 --> 01:08:54,450
I I should message afterward,

1311
01:08:54,450 --> 01:08:55,500
but if you're interested,

1312
01:08:55,500 --> 01:08:59,550
I guess, in a sort most lightweight versions of verification,

1313
01:08:59,670 --> 01:09:01,350
I think Dafny is a great tool to learn,

1314
01:09:01,350 --> 01:09:03,720
because it's a pretty simple starting point,

1315
01:09:03,720 --> 01:09:06,540
you can write real code and get a feel for things,

1316
01:09:06,630 --> 01:09:09,360
I think a lot of a lot of verification is pretty academic,

1317
01:09:09,360 --> 01:09:13,500
and not super close to being really useful at verification like this,

1318
01:09:13,740 --> 01:09:16,440
so I'm not sure how useful it really would be for,

1319
01:09:16,470 --> 01:09:17,850
real software during just yet,

1320
01:09:18,060 --> 01:09:19,830
so the hope is that one day it will be.

1321
01:09:20,840 --> 01:09:21,230
Thanks.

1322
01:09:24,670 --> 01:09:29,260
Did you implement this version of Paxos you're talking about?

1323
01:09:29,770 --> 01:09:31,330
This Monotone Paxos those thing.

1324
01:09:32,500 --> 01:09:34,120
So, yeah I implemented,

1325
01:09:34,120 --> 01:09:36,610
rather than implementing a generic Monotone Paxos thing,

1326
01:09:36,610 --> 01:09:38,440
which wouldn't really make sense and go anyways,

1327
01:09:38,470 --> 01:09:42,400
I implemented directly the log replication over Monotone Paxos,

1328
01:09:42,430 --> 01:09:46,000
so in this Monotone, Monotone log application thing,

1329
01:09:46,000 --> 01:09:47,650
all the RPC send on the full log,

1330
01:09:47,800 --> 01:09:49,750
if you run it for a long time to get way too slow,

1331
01:09:49,750 --> 01:09:51,640
because the RPCs are sending too much stuff,

1332
01:09:51,640 --> 01:09:52,900
yeah, I did implement it,

1333
01:09:52,900 --> 01:09:56,080
and I think we're working on actually trying to reason about it.

1334
01:09:57,490 --> 01:10:02,890
Did you [] to figure out how performance it actually ends up being

1335
01:10:02,890 --> 01:10:04,960
or how it works in practice?

1336
01:10:06,040 --> 01:10:10,630
So I think the exact code that we have right now is not code you'd want to run,

1337
01:10:10,630 --> 01:10:12,070
and my,

1338
01:10:12,970 --> 01:10:15,370
in a sense, it ought to be as performance as Raft,

1339
01:10:15,370 --> 01:10:18,190
sort of is and we don't really have an optimized implementation,

1340
01:10:18,190 --> 01:10:20,650
so I haven't actually bothered getting performance numbers for it at all,

1341
01:10:22,200 --> 01:10:23,940
it's probably pretty slow, not really sure.

1342
01:10:29,930 --> 01:10:30,770
Great, thank you,

1343
01:10:30,950 --> 01:10:33,440
yeah, even if we verify my programs,

1344
01:10:33,440 --> 01:10:35,060
I'm sure on this log somewhere,

1345
01:10:35,630 --> 01:10:39,480
but let's hear from PP2 now.

1346
01:10:51,340 --> 01:10:52,120
Alright, can everyone see?

1347
01:10:54,150 --> 01:10:54,600
Great.

1348
01:10:54,990 --> 01:10:59,970
So, we are the [] protocol team, me, me Jay Timmy,

1349
01:10:59,970 --> 01:11:02,520
and we are we present a simple distributed file system,

1350
01:11:03,120 --> 01:11:05,520
and the reason we selected a distributed file system is

1351
01:11:05,520 --> 01:11:07,980
that users often times want to sort data privately

1352
01:11:07,980 --> 01:11:09,150
in a really accessible way,

1353
01:11:09,180 --> 01:11:11,130
with that the implications of using a cloud company,

1354
01:11:11,130 --> 01:11:12,600
where you don't own your own data.

1355
01:11:13,500 --> 01:11:15,120
So we wanted to create a solution,

1356
01:11:15,120 --> 01:11:16,500
where you self host your data

1357
01:11:16,530 --> 01:11:19,920
in a fault tolerant distributed manner on commodity hardware,

1358
01:11:20,690 --> 01:11:23,600
and our file system is really similar to Frangipani,

1359
01:11:23,600 --> 01:11:25,580
except that it uses Raft instead of Petal,

1360
01:11:25,610 --> 01:11:28,820
and the file system is on the servers instead of the clients.

1361
01:11:29,420 --> 01:11:30,980
In terms of file system parameters,

1362
01:11:30,980 --> 01:11:33,920
we also have a 4096 byte block size,

1363
01:11:33,920 --> 01:11:36,440
and a 2 megabyte maximum file size,

1364
01:11:36,950 --> 01:11:40,100
we theoretically have a 32 gigabyte maximum disk capacity,

1365
01:11:40,280 --> 01:11:42,080
however this is actually constrained by your RAM,

1366
01:11:42,080 --> 01:11:44,300
so if you only have 8 gigabyte of RAM,

1367
01:11:44,300 --> 01:11:45,050
you would have,

1368
01:11:45,440 --> 01:11:49,340
however much left over after your, after whatever your system takes up.

1369
01:11:50,140 --> 01:11:54,220
And we support as many servers and clients as we can within reason,

1370
01:11:54,250 --> 01:11:58,090
obviously the more servers and clients that you add to the locking contention,

1371
01:11:58,090 --> 01:12:00,040
there will be less performance

1372
01:12:00,040 --> 01:12:02,980
as you start to access the same file over and over again.

1373
01:12:03,690 --> 01:12:06,060
And in terms of performance,

1374
01:12:06,060 --> 01:12:09,660
well, we were very heavily focused on availability and crash recovery,

1375
01:12:09,660 --> 01:12:11,970
so we didn't, we didn't measure performance,

1376
01:12:11,970 --> 01:12:13,230
and we think it's probably pretty bad,

1377
01:12:13,230 --> 01:12:15,510
because our system is built on top of Raft,

1378
01:12:15,510 --> 01:12:18,990
which is not known to be the most performance of systems.

1379
01:12:19,890 --> 01:12:21,780
So, over to Jay.

1380
01:12:22,760 --> 01:12:25,700
So, again performance is not the biggest thing that we have,

1381
01:12:25,700 --> 01:12:28,610
but we do have very very strong consistency guarantees,

1382
01:12:28,670 --> 01:12:30,860
in particular, we enforce POSIX consistency,

1383
01:12:30,860 --> 01:12:32,720
which is a form of strong consistency,

1384
01:12:32,720 --> 01:12:34,610
we usually see on local file systems,

1385
01:12:34,820 --> 01:12:36,530
so we enforce the invariant,

1386
01:12:36,530 --> 01:12:39,170
that after a file right after you do a successful file write,

1387
01:12:39,530 --> 01:12:42,290
any read of your previously written bytes from anywhere,

1388
01:12:42,290 --> 01:12:44,540
will return the data specified by that previous write,

1389
01:12:44,720 --> 01:12:47,840
similarly any new writes over that data,

1390
01:12:47,870 --> 01:12:50,360
will result in visible overwrites of that data,

1391
01:12:50,610 --> 01:12:52,170
from the perspective of other readers.

1392
01:12:52,650 --> 01:12:54,360
So in order to achieve this,

1393
01:12:54,360 --> 01:12:55,500
we have a data-mode journal,

1394
01:12:55,500 --> 01:12:57,450
that is built into this sort of block layer,

1395
01:12:57,600 --> 01:13:00,660
that is again distributed with Raft and replicated

1396
01:13:00,840 --> 01:13:03,000
and which is effectively a write-ahead log,

1397
01:13:03,000 --> 01:13:06,630
that guarantees the atomicity of writes strong semantics in the presence of crashes,

1398
01:13:06,690 --> 01:13:08,160
the same as Raft pretty much,

1399
01:13:08,400 --> 01:13:11,340
and also the consistency model that we offer above.

1400
01:13:11,660 --> 01:13:13,880
So servers also in order to help with this,

1401
01:13:13,880 --> 01:13:15,080
we issue distributed locking,

1402
01:13:15,080 --> 01:13:17,030
so we can have this very primitive block cache,

1403
01:13:17,060 --> 01:13:18,800
as you would see in a local file system,

1404
01:13:19,100 --> 01:13:21,110
and also we have you know leases

1405
01:13:21,110 --> 01:13:23,690
to make sure that there's mutually exclusive access to all of these blocks.

1406
01:13:27,520 --> 01:13:30,190
Right, so to allow our clients to use our file system,

1407
01:13:30,190 --> 01:13:32,140
we created a POSIX like interface,

1408
01:13:32,140 --> 01:13:34,000
where users can interact with files,

1409
01:13:34,240 --> 01:13:37,300
we mainly have four functions, Open, Close, Read, Write,

1410
01:13:37,450 --> 01:13:40,240
Open and Close are pretty explanatory,

1411
01:13:40,240 --> 01:13:43,360
they just open and close file descriptors on our file system,

1412
01:13:44,020 --> 01:13:46,360
Read, it just takes a file descriptor

1413
01:13:46,360 --> 01:13:49,900
and read a fixed number of bytes at the current file position,

1414
01:13:50,260 --> 01:13:51,970
Write also takes a file descriptor

1415
01:13:51,970 --> 01:13:54,190
and Read flush is,

1416
01:13:54,340 --> 01:13:57,970
sorry, write takes a file descriptor

1417
01:13:57,970 --> 01:14:01,660
and just writes the data to the file,

1418
01:14:01,780 --> 01:14:05,380
but it's in a different way than a normal POSIX write,

1419
01:14:05,380 --> 01:14:09,940
because instead flushes the buffer copy of the file,

1420
01:14:09,940 --> 01:14:11,800
and then appends the new data on this,

1421
01:14:12,310 --> 01:14:14,260
so instead of a normal POSIX write,

1422
01:14:14,260 --> 01:14:17,750
where we just write to a file descriptor with a buffer,

1423
01:14:17,750 --> 01:14:20,840
and the number of bytes to write it does, what I just said that.

1424
01:14:21,020 --> 01:14:25,160
And we have a demo to represent clients interacting with the system.

1425
01:14:28,290 --> 01:14:30,450
So this is just a quick demonstration of our file system,

1426
01:14:30,450 --> 01:14:32,670
being run both serially and concurrently,

1427
01:14:32,670 --> 01:14:35,130
so what's going to happen is console one

1428
01:14:35,130 --> 01:14:36,900
or I should say left console

1429
01:14:36,930 --> 01:14:41,040
is going to open up a file called tt, just testing thing,

1430
01:14:42,670 --> 01:14:44,230
it's going to write something to the file,

1431
01:14:44,230 --> 01:14:46,810
and then the console on the right is going to read from it afterwards,

1432
01:14:47,020 --> 01:14:48,490
per the consistency model,

1433
01:14:48,520 --> 01:14:51,820
they should see the same thing as the left console wrote,

1434
01:14:51,820 --> 01:14:57,630
and indeed after a minute, we see that.

1435
01:14:57,840 --> 01:15:00,390
Okay, so the next thing that's going to happen is,

1436
01:15:00,570 --> 01:15:03,000
both console one and console two are going to try to

1437
01:15:03,000 --> 01:15:06,870
flush their local copies of this file at the same time,

1438
01:15:06,990 --> 01:15:08,970
this is not a traditional POSIX write,

1439
01:15:09,180 --> 01:15:11,730
they're both taking the copies of the file that they have,

1440
01:15:11,730 --> 01:15:13,710
and trying to put them onto disk at the same time,

1441
01:15:14,140 --> 01:15:16,810
so it's sort of like two writes from offset zero,

1442
01:15:17,140 --> 01:15:18,550
one of these is going to win,

1443
01:15:19,060 --> 01:15:21,550
and we can look at the log on the left console,

1444
01:15:21,550 --> 01:15:22,210
as it commits

1445
01:15:22,210 --> 01:15:24,820
and actually see, after a moment,

1446
01:15:25,520 --> 01:15:28,070
that both transactions run concurrently,

1447
01:15:28,160 --> 01:15:29,960
they both take up different parts of the log,

1448
01:15:30,050 --> 01:15:31,250
but at the end of the day,

1449
01:15:31,370 --> 01:15:34,130
the left console transaction is going to win,

1450
01:15:34,520 --> 01:15:37,370
so it was completely atomic, everything works.

1451
01:15:41,450 --> 01:15:41,960
So this.

1452
01:15:48,950 --> 01:15:49,970
I think you got muted.

1453
01:15:51,110 --> 01:15:52,160
Sorry, I muted,

1454
01:15:52,550 --> 01:15:53,810
due to time there are more,

1455
01:15:53,840 --> 01:15:56,810
there are some limitations and features we can add to our file system,

1456
01:15:57,020 --> 01:16:00,440
so firstly, we only have one root directory,

1457
01:16:00,440 --> 01:16:02,600
so adding more will definitely be a plus,

1458
01:16:02,990 --> 01:16:04,760
next we only,

1459
01:16:05,210 --> 01:16:08,240
we should persist blocks to disk rather than RAM,

1460
01:16:08,240 --> 01:16:09,770
because that's what we're currently doing,

1461
01:16:09,890 --> 01:16:13,190
but we should also consider that what we're doing,

1462
01:16:13,190 --> 01:16:14,120
has a lot of writes,

1463
01:16:14,120 --> 01:16:15,680
so it could be pretty bad,

1464
01:16:15,680 --> 01:16:16,940
if we just keep writing,

1465
01:16:17,310 --> 01:16:19,740
doing have a lot of writes for just one operation,

1466
01:16:19,950 --> 01:16:22,230
we also only have direct inode blocks

1467
01:16:22,230 --> 01:16:25,860
rather than having direct and indirect inodes,

1468
01:16:25,860 --> 01:16:27,510
so that would be a plus to add those,

1469
01:16:27,510 --> 01:16:28,410
and secondly,

1470
01:16:28,470 --> 01:16:32,850
there should be a better way for clients to interact with this file system,

1471
01:16:32,850 --> 01:16:34,080
so there could be a FUSE layer,

1472
01:16:34,080 --> 01:16:36,660
or they could just be better POSIX compliance in general.

1473
01:16:38,700 --> 01:16:40,050
Concludes our presentation.

1474
01:16:48,300 --> 01:16:50,760
Do you want to talk a little bit about how you tested this?

1475
01:16:52,290 --> 01:16:55,410
Sure, so we had a pretty, at your recommendation,

1476
01:16:55,410 --> 01:16:56,820
we had a pretty broad set of tests,

1477
01:16:56,820 --> 01:16:58,650
we had, so to begin each component,

1478
01:16:58,650 --> 01:17:00,870
so each you know the block layer right here,

1479
01:17:00,870 --> 01:17:02,220
individual Raft key values,

1480
01:17:02,220 --> 01:17:03,150
we had the journal,

1481
01:17:03,240 --> 01:17:04,380
and we had all these things,

1482
01:17:04,410 --> 01:17:06,720
we basically mocked every layer underneath,

1483
01:17:06,750 --> 01:17:09,420
each layer we were testing and tested,

1484
01:17:09,420 --> 01:17:10,890
did sort of unit testing,

1485
01:17:10,890 --> 01:17:12,870
you can't really call it unit testing, once you get high enough,

1486
01:17:13,160 --> 01:17:16,670
because you're so reliant on lower layers being correct,

1487
01:17:16,670 --> 01:17:18,620
but we did as best we could,

1488
01:17:18,620 --> 01:17:20,210
from there we did integration testing,

1489
01:17:20,210 --> 01:17:22,550
and you know wrote out a set of partitions,

1490
01:17:22,550 --> 01:17:24,860
one of those partitions is sort of what you just saw,

1491
01:17:25,010 --> 01:17:26,090
the most interesting of them,

1492
01:17:26,090 --> 01:17:26,930
there were five of them,

1493
01:17:26,930 --> 01:17:29,060
you can see them in our Git repository,

1494
01:17:29,360 --> 01:17:31,760
and then we also, to what degree, we could,

1495
01:17:31,760 --> 01:17:33,860
so this didn't necessarily entirely work,

1496
01:17:33,860 --> 01:17:35,960
just because of some of the time limitations that we had,

1497
01:17:35,960 --> 01:17:38,630
but we also attempted to do some stress testing,

1498
01:17:38,660 --> 01:17:40,220
obviously performance numbers aren't great,

1499
01:17:40,430 --> 01:17:42,020
because it's not supposed to be great,

1500
01:17:42,020 --> 01:17:43,820
but, the best we could,

1501
01:17:43,820 --> 01:17:46,490
so we're pretty sure this is at least correct verified here.

1502
01:18:02,790 --> 01:18:03,930
One interesting thing is that,

1503
01:18:03,930 --> 01:18:07,800
you're basically shooting for actually slightly stronger consistency property

1504
01:18:07,800 --> 01:18:09,090
that POSIX are required,

1505
01:18:09,510 --> 01:18:13,080
just two processes write into a single file,

1506
01:18:13,080 --> 01:18:14,310
there's actually not much,

1507
01:18:14,340 --> 01:18:17,070
actually the write we have to have to do.

1508
01:18:17,880 --> 01:18:21,150
Yes, that was kind of accidental,

1509
01:18:21,180 --> 01:18:22,320
but we were,

1510
01:18:22,500 --> 01:18:24,060
so it's kind of stronger.

1511
01:18:24,240 --> 01:18:24,810
Yeah.

1512
01:18:24,810 --> 01:18:26,550
It was accidental, but it happened,

1513
01:18:26,550 --> 01:18:28,680
so we were like cool, it worked.

1514
01:18:30,480 --> 01:18:33,210
The server, I mean I think part of what sort of happened was,

1515
01:18:33,210 --> 01:18:34,650
that we guarantee, we,

1516
01:18:34,650 --> 01:18:36,210
I positively guarantees I talked about,

1517
01:18:36,210 --> 01:18:38,730
but we also make the guarantee that the block writes are all atomic,

1518
01:18:38,760 --> 01:18:42,900
and I think that's why we get this sort of stronger consistency,

1519
01:18:42,900 --> 01:18:44,250
because of the whole journal thing,

1520
01:18:44,250 --> 01:18:46,050
yes, you could get your write overwritten,

1521
01:18:46,050 --> 01:18:47,070
if you do at the same time,

1522
01:18:47,070 --> 01:18:48,090
but it's always going to be clean.

1523
01:18:51,740 --> 01:18:53,150
If you have [aggressive] caching,

1524
01:18:53,150 --> 01:18:57,080
you might run in and don't write immediately to the log,

1525
01:18:57,080 --> 01:18:59,120
then you might get different behavior.

1526
01:19:00,370 --> 01:19:01,840
This is, why we don't aggressively cache,

1527
01:19:01,870 --> 01:19:04,240
we have a block cache of size one.

1528
01:19:10,890 --> 01:19:12,300
Awesome, thank you, very cool,

1529
01:19:12,660 --> 01:19:15,870
alright, our last group is presenting a game framework,

1530
01:19:16,430 --> 01:19:17,780
whenever you're ready, take it away.

1531
01:19:21,660 --> 01:19:23,280
Alright, so we're sure that,

1532
01:19:23,280 --> 01:19:26,190
many of you have played multiplayer games in quarantine,

1533
01:19:26,220 --> 01:19:26,850
when you're bored.

1534
01:19:27,660 --> 01:19:29,070
So let's imagine that,

1535
01:19:29,070 --> 01:19:31,740
you are a small indie game dev company,

1536
01:19:32,190 --> 01:19:34,140
and you're trying to develop a multiplayer game,

1537
01:19:34,170 --> 01:19:37,770
possibly that has either several different rooms,

1538
01:19:37,770 --> 01:19:38,910
something like chat penguin,

1539
01:19:38,910 --> 01:19:41,220
where you might interact with other players in the same room,

1540
01:19:41,370 --> 01:19:42,990
or perhaps it's like matchmaking case,

1541
01:19:42,990 --> 01:19:45,420
where you're in a lobby with several other players.

1542
01:19:45,930 --> 01:19:48,600
Well, so traditionally, how these work is that,

1543
01:19:48,690 --> 01:19:51,810
everything goes against process on one central server,

1544
01:19:52,050 --> 01:19:54,210
but that central server is a bottleneck,

1545
01:19:54,450 --> 01:19:58,590
if every single player has to connect to that server to handle the game logic,

1546
01:19:58,650 --> 01:20:02,550
that server began a bottleneck by the number of requests that come through,

1547
01:20:02,910 --> 01:20:04,560
and also if that server goes down,

1548
01:20:05,070 --> 01:20:05,970
there goes [].

1549
01:20:06,760 --> 01:20:10,060
So what we're proposing is to create a distributed game framework,

1550
01:20:10,060 --> 01:20:12,700
that instead, that's also fault tolerant,

1551
01:20:12,940 --> 01:20:16,210
so instead of having all the processing being on that central server,

1552
01:20:16,210 --> 01:20:20,410
we actually distribute the game logic processing to several different worker servers,

1553
01:20:20,980 --> 01:20:22,030
but on top of that,

1554
01:20:22,560 --> 01:20:23,880
to be fault tolerant,

1555
01:20:23,880 --> 01:20:26,340
so when one of these worker servers goes down,

1556
01:20:26,370 --> 01:20:29,010
we need to be able to handle that game logic,

1557
01:20:29,010 --> 01:20:32,550
and further move players to some other server workers,

1558
01:20:32,850 --> 01:20:34,470
so as part of that,

1559
01:20:34,470 --> 01:20:37,200
we need to actually balance latency with fault tolerance,

1560
01:20:37,200 --> 01:20:40,050
because if we make everything stay fault tolerant,

1561
01:20:40,170 --> 01:20:44,880
we might run into a, each move taking a long time to process.

1562
01:20:46,080 --> 01:20:47,820
So that's why we're introducing Pinguino,

1563
01:20:48,060 --> 01:20:49,860
which is our fault tolerant game framework,

1564
01:20:49,860 --> 01:20:53,880
that addresses all of the previous issues with distributed networks.

1565
01:20:59,160 --> 01:21:01,680
To dive a bit into the system of our framework,

1566
01:21:01,710 --> 01:21:03,690
let's imagine the game club penguin,

1567
01:21:04,050 --> 01:21:05,160
so in club penguin,

1568
01:21:05,190 --> 01:21:08,340
user that is assigned into one room or one region,

1569
01:21:08,580 --> 01:21:11,940
it only really cares about talking and interacting with other users,

1570
01:21:11,940 --> 01:21:14,610
and the objects in that one room,

1571
01:21:14,640 --> 01:21:16,170
and they don't really need to care about

1572
01:21:16,200 --> 01:21:17,940
anything else that's happening in another room,

1573
01:21:18,440 --> 01:21:20,210
so there's no reason to have

1574
01:21:20,510 --> 01:21:24,140
the request of every user be processed by one centralized servers,

1575
01:21:24,530 --> 01:21:25,910
so instead we decided that,

1576
01:21:25,910 --> 01:21:30,320
we will have all of these requests be process across multiple workers,

1577
01:21:30,470 --> 01:21:31,730
in order to do this,

1578
01:21:31,760 --> 01:21:34,550
we have workers that are assigned to different regions,

1579
01:21:34,550 --> 01:21:36,560
so if a player is in one region,

1580
01:21:36,680 --> 01:21:40,280
they might be talking with the worker that is assigned to that region,

1581
01:21:40,280 --> 01:21:41,510
so for example in here,

1582
01:21:41,630 --> 01:21:43,070
the penguin that is in worker,

1583
01:21:43,160 --> 01:21:45,320
that is assigned to the region for worker 2,

1584
01:21:45,320 --> 01:21:46,850
we'll talk with worker 2 only,

1585
01:21:46,940 --> 01:21:50,510
but then the one in the worker n will talk with worker n.

1586
01:21:51,210 --> 01:21:52,680
And additionally we mentioned,

1587
01:21:52,680 --> 01:21:56,940
we decided that this wouldn't the relation between worker and regions,

1588
01:21:56,940 --> 01:21:59,100
doesn't necessarily have to be one to one mapping,

1589
01:21:59,490 --> 01:22:02,760
for some rooms that might be less popular and have less traffic,

1590
01:22:02,790 --> 01:22:06,390
it's possible that a worker can handle multiple of those,

1591
01:22:06,510 --> 01:22:08,580
so there's that type of relation,

1592
01:22:08,820 --> 01:22:09,960
that we need to keep track of.

1593
01:22:10,540 --> 01:22:12,040
And so in order to keep track of this,

1594
01:22:12,040 --> 01:22:13,870
we do need one centralized server,

1595
01:22:13,870 --> 01:22:15,160
which is the coordinator,

1596
01:22:15,370 --> 01:22:18,250
so the coordinator will be keeping track of all these mappings,

1597
01:22:18,580 --> 01:22:22,510
and some of the mapping includes the region to worker relation

1598
01:22:22,540 --> 01:22:28,370
as well as region, the worker to their replica relation,

1599
01:22:28,370 --> 01:22:30,650
as well as the players itself,

1600
01:22:30,860 --> 01:22:33,350
so for the fault tolerant aspect,

1601
01:22:33,380 --> 01:22:36,620
we have, that the workers will have two replicate each,

1602
01:22:36,650 --> 01:22:39,440
and [] will talk a little bit more about

1603
01:22:39,470 --> 01:22:42,350
what kind of information is sent to the worker

1604
01:22:42,680 --> 01:22:45,050
from there to [] later.

1605
01:22:45,610 --> 01:22:47,470
And additionally, the coordinator,

1606
01:22:47,470 --> 01:22:49,330
because it is that one centralized server,

1607
01:22:49,330 --> 01:22:51,970
it is also a possible a failure point,

1608
01:22:51,970 --> 01:22:53,590
so we have a coordinator backup,

1609
01:22:53,650 --> 01:22:56,560
and here the coordinator's main role is,

1610
01:22:56,560 --> 01:22:59,980
just to keep track of all of these relations of the game states,

1611
01:23:00,100 --> 01:23:04,660
so information about the coordinator that changes for those relations

1612
01:23:04,690 --> 01:23:06,370
will be sent to the coordinator backup,

1613
01:23:06,370 --> 01:23:10,270
before being process complete completely.

1614
01:23:10,690 --> 01:23:13,960
So now with this, although we have that one server,

1615
01:23:14,320 --> 01:23:16,990
the bulk of the traffic for games usually is

1616
01:23:16,990 --> 01:23:20,050
players making moves and sending request process those moves,

1617
01:23:20,170 --> 01:23:23,020
and those are now divided across multiple workers,

1618
01:23:23,020 --> 01:23:25,360
and the coordinator is in charge of just a mapping

1619
01:23:25,360 --> 01:23:28,900
and sending heartbeat to ensure that the workers are still alive

1620
01:23:28,930 --> 01:23:31,270
and can handle any failure cases.

1621
01:23:33,210 --> 01:23:35,640
Yep, so in the case that a worker goes down,

1622
01:23:35,730 --> 01:23:38,730
we have the coordinator handling the reassignment

1623
01:23:38,730 --> 01:23:41,310
of the players who were in that worker,

1624
01:23:41,670 --> 01:23:44,880
and because the coordinator manages only the region mapping,

1625
01:23:44,880 --> 01:23:47,550
it's also easy for us to move around regions

1626
01:23:47,550 --> 01:23:51,070
when one of, say one worker gets overloaded,

1627
01:23:51,070 --> 01:23:53,500
this allows us to perform some amount of load balancing,

1628
01:23:54,380 --> 01:23:55,460
as we mentioned earlier.

1629
01:23:56,400 --> 01:23:59,790
Cool, so I'll move on to what the developer API looks like,

1630
01:23:59,790 --> 01:24:02,550
because another key feature that we wanted was,

1631
01:24:02,580 --> 01:24:04,170
for the framework to be easy to use,

1632
01:24:04,170 --> 01:24:07,290
for a developer trying to code a new game in it,

1633
01:24:07,530 --> 01:24:10,200
so we treat the game as a state machine essentially,

1634
01:24:10,410 --> 01:24:12,720
and so any move that the player makes,

1635
01:24:12,840 --> 01:24:15,060
actually fits into one of two different types of moves,

1636
01:24:15,330 --> 01:24:16,470
so I mentioned earlier,

1637
01:24:16,470 --> 01:24:20,220
that we're trying to balance a bit of latency with fault tolerant,

1638
01:24:20,520 --> 01:24:24,540
so in order to provide a sort of choice for the developer,

1639
01:24:24,540 --> 01:24:28,260
we have two separate commands that we expose to the developer,

1640
01:24:28,350 --> 01:24:30,120
the first is sendFastMove,

1641
01:24:30,240 --> 01:24:32,550
so this fast move make sure that,

1642
01:24:32,550 --> 01:24:34,890
the move gets to the replicas as soon as possible,

1643
01:24:34,890 --> 01:24:38,220
so the move gets processed as fast as possible on the worker,

1644
01:24:39,090 --> 01:24:41,490
on the other hand, we have sendStableMove,

1645
01:24:41,580 --> 01:24:44,520
which actually is a more fault tolerant move,

1646
01:24:44,520 --> 01:24:46,200
that we expose to the developer,

1647
01:24:46,350 --> 01:24:47,580
and this ensures,

1648
01:24:47,580 --> 01:24:50,850
this is mainly used for game critical logic changes,

1649
01:24:50,850 --> 01:24:52,200
such as say transaction,

1650
01:24:52,530 --> 01:24:54,540
so if you're buying something, you don't want,

1651
01:24:55,090 --> 01:24:56,950
like if you've already spent that money,

1652
01:24:56,950 --> 01:24:57,760
you want to make sure that,

1653
01:24:57,760 --> 01:25:00,730
you get whatever you spend that money on your game,

1654
01:25:01,030 --> 01:25:03,790
and so we guarantee that,

1655
01:25:03,790 --> 01:25:07,660
if that move gets fully processed and on the game,

1656
01:25:07,870 --> 01:25:10,800
it's, it's stored on both of the replica,

1657
01:25:10,950 --> 01:25:14,460
which ensures that if the worker that you're talking to goes down,

1658
01:25:14,460 --> 01:25:16,350
and the player gets transferred to a new worker,

1659
01:25:16,620 --> 01:25:19,110
that new worker will be able to reconstruct the game,

1660
01:25:19,110 --> 01:25:20,490
including that transaction,

1661
01:25:20,670 --> 01:25:22,890
this guarantee isn't done for a fast move,

1662
01:25:22,890 --> 01:25:25,290
which prioritizes the latency aspect,

1663
01:25:25,590 --> 01:25:26,940
but you can also see here,

1664
01:25:26,940 --> 01:25:31,560
the Move struct, that the developer defines are pretty general,

1665
01:25:31,560 --> 01:25:33,660
so in the game that will be,

1666
01:25:33,810 --> 01:25:38,010
that we kind of developed as a toy demo for our framework,

1667
01:25:38,310 --> 01:25:40,380
it's kind of a chat penguin like interface.

1668
01:25:40,580 --> 01:25:45,350
So each player is in several different rooms,

1669
01:25:45,650 --> 01:25:47,360
and so within each room,

1670
01:25:47,360 --> 01:25:48,380
there's a chat window,

1671
01:25:48,380 --> 01:25:51,230
that you can talk to to interact with another player,

1672
01:25:51,620 --> 01:25:53,450
so the two types of main moves,

1673
01:25:53,450 --> 01:25:55,940
that you can make in this game are [] move,

1674
01:25:56,030 --> 01:25:58,220
so a developer would just define

1675
01:25:58,250 --> 01:26:01,400
like the X Y and the Username of the player moving,

1676
01:26:01,670 --> 01:26:04,070
and so the ChatMessage is kind of the same words

1677
01:26:04,070 --> 01:26:07,130
just like a chat message that you send into the window.

1678
01:26:07,760 --> 01:26:10,600
And so, in our game,

1679
01:26:10,600 --> 01:26:13,180
we made chat messages a stable move,

1680
01:26:13,180 --> 01:26:14,950
and move as a fast move,

1681
01:26:15,160 --> 01:26:17,530
so even if like say one move gets dropped,

1682
01:26:17,530 --> 01:26:18,040
it's okay,

1683
01:26:18,040 --> 01:26:19,720
if you're a new kind of [],

1684
01:26:19,750 --> 01:26:23,860
but, we don't want chat messages to randomly disappear,

1685
01:26:23,860 --> 01:26:25,420
because they could be important messages.

1686
01:26:26,080 --> 01:26:28,690
So with that, I'll move on to the demo,

1687
01:26:28,720 --> 01:26:30,070
which is a little bit demo,

1688
01:26:30,070 --> 01:26:32,380
but should show off the functionality.

1689
01:26:40,500 --> 01:26:42,960
So we have our minimalist frontend,

1690
01:26:44,690 --> 01:26:46,430
and so when we move around the penguin,

1691
01:26:46,430 --> 01:26:47,270
we can see that,

1692
01:26:47,270 --> 01:26:48,770
it's first send the fast move,

1693
01:26:48,980 --> 01:26:53,630
and the replica, that same move gets sent to the other replica

1694
01:26:53,630 --> 01:26:54,710
is assigned to the main workers,

1695
01:26:54,710 --> 01:26:56,120
so right now, we're on worker 0,

1696
01:26:56,300 --> 01:26:58,340
it gets replicated to worker 1 and 2,

1697
01:26:58,370 --> 01:26:59,480
so we have two copies,

1698
01:26:59,870 --> 01:27:03,140
and the game server then receives that change,

1699
01:27:03,140 --> 01:27:04,790
and so they can process that locally,

1700
01:27:05,300 --> 01:27:08,150
and then if we send a chat message,

1701
01:27:09,460 --> 01:27:11,890
we also have the player username,

1702
01:27:11,890 --> 01:27:13,600
identified with the chat message sent,

1703
01:27:13,660 --> 01:27:15,610
but this is actually a stable move,

1704
01:27:15,790 --> 01:27:18,340
so it's not visible in the logs,

1705
01:27:18,340 --> 01:27:19,840
but the stable moves wait

1706
01:27:19,840 --> 01:27:23,170
until those moves are actually replicated to the workers,

1707
01:27:23,170 --> 01:27:24,280
and it's not easy to see here,

1708
01:27:24,280 --> 01:27:27,820
because normally there might be some amount of lag,

1709
01:27:28,530 --> 01:27:31,320
but when we do introduce some amount of lag into the network,

1710
01:27:31,350 --> 01:27:33,420
that move takes longer than it does,

1711
01:27:34,160 --> 01:27:38,630
and now move it back to some future work that we want to implement.

1712
01:27:44,060 --> 01:27:45,080
In terms of the backend,

1713
01:27:45,080 --> 01:27:47,480
one additional thing that we would like to do is

1714
01:27:47,480 --> 01:27:50,540
to allow the users to move across different rooms,

1715
01:27:50,660 --> 01:27:53,570
so right now, upon a user joining the game,

1716
01:27:53,570 --> 01:27:54,740
it'd be initialized,

1717
01:27:54,740 --> 01:27:56,240
they are assigned to one room,

1718
01:27:56,300 --> 01:27:58,910
but ideally if they want to move across to it,

1719
01:27:58,940 --> 01:28:00,590
if they want to move to a different room,

1720
01:28:00,620 --> 01:28:02,360
then they should be able to talk to the coordinator

1721
01:28:02,360 --> 01:28:05,030
to be like, hey I'm gonna go to this region now,

1722
01:28:05,030 --> 01:28:08,390
can you load up the information of the game state from that region

1723
01:28:08,390 --> 01:28:11,060
and then also now I'm going to start talking to a new worker.

1724
01:28:11,620 --> 01:28:13,900
And additionally we hinted at this earlier,

1725
01:28:14,200 --> 01:28:17,380
where we wanted to deal with region based worker load balancing,

1726
01:28:17,470 --> 01:28:19,150
so the reason why we had that,

1727
01:28:19,180 --> 01:28:23,590
why we did not go for one to one mapping with worker to region

1728
01:28:23,590 --> 01:28:26,320
was to allow for this future work,

1729
01:28:26,320 --> 01:28:27,640
and we hope to be able to do that,

1730
01:28:27,640 --> 01:28:32,890
in order to control how much load each worker will be based with.

1731
01:28:44,260 --> 01:28:46,900
Sorry, I have a question,

1732
01:28:46,960 --> 01:28:48,970
so are your,

1733
01:28:49,880 --> 01:28:54,170
again, you have two actions sending a message and moving,

1734
01:28:54,200 --> 01:28:57,170
so those actions are all atomic, right,

1735
01:28:57,260 --> 01:28:57,920
are they like.

1736
01:28:59,640 --> 01:29:01,170
Yeah.

1737
01:29:03,000 --> 01:29:04,230
They get sequentially processed,

1738
01:29:04,230 --> 01:29:06,120
and since they are like individual moves,

1739
01:29:06,120 --> 01:29:10,260
they most of the time, they only modify some variables,

1740
01:29:10,260 --> 01:29:12,810
and they acquire that locks on those variables before the moves.

1741
01:29:16,350 --> 01:29:17,790
So you said earlier,

1742
01:29:17,790 --> 01:29:20,700
that you have a coordinator and coordinator backup,

1743
01:29:20,880 --> 01:29:24,360
and the replicas can talk to either of them,

1744
01:29:24,660 --> 01:29:27,810
what happens if you have a network partition,

1745
01:29:28,020 --> 01:29:31,260
that separates the coordinator and some set of replicas,

1746
01:29:31,380 --> 01:29:34,080
the coordinator backup and some other set of replicas.

1747
01:29:37,900 --> 01:29:41,320
So the coordinator backup, in case of a network partition,

1748
01:29:41,560 --> 01:29:44,530
the workers will be kind of lost like the coordinator,

1749
01:29:44,530 --> 01:29:48,880
is that a matter of the worker being able to talk to either the coordinator or the coordinator backup,

1750
01:29:48,880 --> 01:29:51,280
they'll only be able to talk to the coordinator,

1751
01:29:51,310 --> 01:29:52,990
and if a coordinator goes down,

1752
01:29:52,990 --> 01:29:57,040
then the backup gets brought up to actually start processing,

1753
01:29:57,070 --> 01:29:58,900
so in that case of network partition,

1754
01:29:58,900 --> 01:30:00,550
I don't think we will be like.

1755
01:30:00,930 --> 01:30:04,950
The workers that are isolated and away from that coordinator

1756
01:30:05,010 --> 01:30:07,020
will not be able to be processed,

1757
01:30:07,020 --> 01:30:09,810
with the like the coordinator itself

1758
01:30:09,900 --> 01:30:11,580
and turns out the user side,

1759
01:30:12,070 --> 01:30:13,150
it can still be processed,

1760
01:30:13,150 --> 01:30:16,270
because the player just needs to continue talking to that worker,

1761
01:30:16,330 --> 01:30:18,940
it's just that, if there's any changes in the region,

1762
01:30:19,270 --> 01:30:21,670
like the state of the game as a whole,

1763
01:30:21,700 --> 01:30:23,080
that will be processed yet.

1764
01:30:23,500 --> 01:30:25,360
Yeah, additionally I want to make that,

1765
01:30:25,570 --> 01:30:27,520
if we have a partition,

1766
01:30:27,550 --> 01:30:29,710
the coordinator backup essentially acts as a coordinator,

1767
01:30:29,710 --> 01:30:31,810
for all of the workers that it can talk to,

1768
01:30:31,990 --> 01:30:33,010
and this is fine,

1769
01:30:33,010 --> 01:30:36,430
because we want the game to be still running for all of the regions,

1770
01:30:36,430 --> 01:30:38,620
in the workers at the coordinator backup is talking to,

1771
01:30:38,740 --> 01:30:40,510
this mainly becomes a problem,

1772
01:30:40,510 --> 01:30:42,340
when they do reunite,

1773
01:30:42,370 --> 01:30:48,520
and in this case, the coordinator backup then takes all of its data,

1774
01:30:48,520 --> 01:30:49,990
and it can send it to the coordinator,

1775
01:30:49,990 --> 01:30:51,880
and the coordinator can locally resolve it,

1776
01:30:51,910 --> 01:30:55,720
because there is kind of an original coordinator and the coordinator backup,

1777
01:30:55,720 --> 01:30:59,410
and they know that the backup was, backup of the coordinator,

1778
01:30:59,440 --> 01:31:00,700
because it's stored locally.

1779
01:31:02,330 --> 01:31:05,690
But if the coordinator backup becomes a coordinator,

1780
01:31:05,720 --> 01:31:07,040
then wouldn't it,

1781
01:31:07,280 --> 01:31:09,800
for example say, oh I need to make sure that,

1782
01:31:09,800 --> 01:31:13,610
we have active replicas for all these rooms,

1783
01:31:13,610 --> 01:31:16,190
that are that are on the other side of the partition,

1784
01:31:16,250 --> 01:31:19,370
wouldn't you have the same room hosts on both sides the partition

1785
01:31:19,370 --> 01:31:20,810
and be able to diverge.

1786
01:31:23,820 --> 01:31:28,560
No, because each room belongs only to like one worker,

1787
01:31:28,830 --> 01:31:30,210
so like,

1788
01:31:30,920 --> 01:31:34,220
so I guess, like each room, like can't,

1789
01:31:34,960 --> 01:31:37,840
like the replicas for the rooms would get abandoned,

1790
01:31:37,840 --> 01:31:39,250
so essentially what happens is,

1791
01:31:39,250 --> 01:31:40,420
if a worker,

1792
01:31:40,600 --> 01:31:41,860
like in the case of a partition,

1793
01:31:41,860 --> 01:31:44,980
the coordinator wouldn't be able to access a worker that's in the other partition,

1794
01:31:45,130 --> 01:31:46,810
so what happens is,

1795
01:31:47,290 --> 01:31:49,930
I think they like move their replicas over,

1796
01:31:50,220 --> 01:31:53,760
but because the players also can't contact worker,

1797
01:31:54,590 --> 01:31:56,510
none of the moves would be processed,

1798
01:31:56,570 --> 01:32:01,910
and so the more recent replicas after the partition heals would be prioritized,

1799
01:32:01,910 --> 01:32:03,140
when healing that network.

1800
01:32:08,590 --> 01:32:12,280
Why did you decide on that API with move

1801
01:32:12,310 --> 01:32:14,380
and sending a message?

1802
01:32:16,330 --> 01:32:19,660
So specifically for this API,

1803
01:32:19,690 --> 01:32:22,510
we wanted two different types of moves,

1804
01:32:22,510 --> 01:32:23,590
two distinct types of moves,

1805
01:32:23,860 --> 01:32:26,980
demonstrate one with a fast move, one with a stable move,

1806
01:32:26,980 --> 01:32:30,850
ideally, stable move is used more like raraly

1807
01:32:30,850 --> 01:32:32,590
and used more for transactions,

1808
01:32:32,590 --> 01:32:34,630
that where it's okay for it to take longer,

1809
01:32:34,630 --> 01:32:36,850
but we wanted to not be dropped at all,

1810
01:32:37,090 --> 01:32:41,200
the easiest way to replicate this in a simple frontend was with a chat message,

1811
01:32:41,650 --> 01:32:42,610
so it's kind of arbitrary,

1812
01:32:43,670 --> 01:32:45,170
moves for sure should be fast,

1813
01:32:45,170 --> 01:32:48,380
because we are, like we don't want it to be,

1814
01:32:48,380 --> 01:32:49,760
because players move a lot.

1815
01:32:52,390 --> 01:32:53,200
Thank you.

1816
01:32:59,650 --> 01:33:00,700
Awesome, thanks so much,

1817
01:33:00,730 --> 01:33:02,500
that concludes the presentations,

1818
01:33:02,500 --> 01:33:03,190
great job everyone,

1819
01:33:03,190 --> 01:33:04,450
this was a pretty exciting.

1820
01:33:05,800 --> 01:33:10,330
I have one more question for [] presentation, if that's possible.

1821
01:33:10,820 --> 01:33:11,750
Yep, go ahead.

1822
01:33:12,420 --> 01:33:14,040
So for the leader,

1823
01:33:14,070 --> 01:33:17,490
sorry, for the distributed election system,

1824
01:33:19,680 --> 01:33:22,110
I'm not familiar lot with cryptography,

1825
01:33:22,110 --> 01:33:28,230
but I guess the system where you sum up all the results of the election,

1826
01:33:28,590 --> 01:33:31,080
on a vote on a counter server,

1827
01:33:31,680 --> 01:33:34,110
this wouldn't that [] group attacks,

1828
01:33:34,110 --> 01:33:36,480
for example if I have two servers,

1829
01:33:36,480 --> 01:33:39,750
and then I vote for different people on both servers,

1830
01:33:39,750 --> 01:33:41,850
but then I coordinate with someone else,

1831
01:33:42,000 --> 01:33:44,520
to also vote in the other way around,

1832
01:33:44,610 --> 01:33:47,100
will eventually get the same vote factor,

1833
01:33:47,550 --> 01:33:50,460
but I would have thought it maliciously against,

1834
01:33:50,460 --> 01:33:55,420
I guess in this case wouldn't change the vote results,

1835
01:33:55,420 --> 01:33:56,830
or like the election result,

1836
01:33:56,830 --> 01:33:59,350
but I guess I would have acted incorrectly,

1837
01:33:59,350 --> 01:34:03,310
so are there checks to make sure everybody voted correctly at each server.

1838
01:34:03,860 --> 01:34:05,930
Yeah.

1839
01:34:07,430 --> 01:34:11,840
Sorry, so we actually don't handle malicious voting,

1840
01:34:11,840 --> 01:34:14,420
which was which is which is pretty big,

1841
01:34:14,870 --> 01:34:19,640
and you know I used to be pretty important for for real-world voting system,

1842
01:34:20,130 --> 01:34:25,950
but yeah, I think like you know the scope of the project that we had

1843
01:34:25,950 --> 01:34:26,940
and that we set out,

1844
01:34:27,210 --> 01:34:30,630
it was just a little too complicated,

1845
01:34:31,090 --> 01:34:33,850
so we, yeah.

1846
01:34:33,880 --> 01:34:37,540
I think we focus more on the distributed systems part,

1847
01:34:37,600 --> 01:34:40,360
but if we wanted to provide more security,

1848
01:34:40,360 --> 01:34:41,470
like for security,

1849
01:34:41,800 --> 01:34:44,320
using for example like an idea that we thought,

1850
01:34:44,320 --> 01:34:46,390
but then decided to not,

1851
01:34:46,390 --> 01:34:48,220
I was having a public ledger,

1852
01:34:48,220 --> 01:34:51,220
where you can make, give us your knowledge proofs,

1853
01:34:51,220 --> 01:34:54,340
that they what you're posting like adds up

1854
01:34:54,340 --> 01:34:56,080
and is what you're saying that it is,

1855
01:34:56,080 --> 01:34:59,800
and thinks of these things to handle malicious participants.

1856
01:35:02,170 --> 01:35:03,340
Yeah.

1857
01:35:03,550 --> 01:35:05,170
So we're running a little bit late,

1858
01:35:05,170 --> 01:35:07,540
yeah, it's in the class in principle,

1859
01:35:07,540 --> 01:35:09,130
and anybody want to take around a question,

1860
01:35:09,130 --> 01:35:10,120
feel free to stick around,

1861
01:35:10,390 --> 01:35:12,280
as I want to say one of two things

1862
01:35:12,280 --> 01:35:14,530
before closing since our last class meeting,

1863
01:35:14,890 --> 01:35:17,530
first of all, I want to thank you for participating,

1864
01:35:17,530 --> 01:35:19,360
even though it's another covid semester,

1865
01:35:19,540 --> 01:35:21,220
I feel I've interacted with many of you,

1866
01:35:21,220 --> 01:35:24,400
you know for email, we're indirectly

1867
01:35:24,400 --> 01:35:25,900
and exchange lots of information,

1868
01:35:25,900 --> 01:35:27,820
and I'd love to see you at some point in person,

1869
01:35:28,270 --> 01:35:29,770
actually know who you are,

1870
01:35:29,980 --> 01:35:33,940
the but, I appreciate all the participation.

1871
01:35:34,410 --> 01:35:36,150
The second thing I want to thank TAs,

1872
01:35:36,180 --> 01:35:39,270
it's an awesome set of TAs,

1873
01:35:39,270 --> 01:35:42,360
you probably realized, probably for many you,

1874
01:35:42,360 --> 01:35:43,650
they figured out some bugs

1875
01:35:43,680 --> 01:35:45,390
and helped you get through the labs,

1876
01:35:46,260 --> 01:35:49,740
and so I ran applause for the TAs,

1877
01:35:50,250 --> 01:35:53,400
very fortunate with this kind of quality.

1878
01:35:54,460 --> 01:35:57,280
And, I guess last thing I was to say,

1879
01:35:57,280 --> 01:35:58,060
I guess good luck on the final,

1880
01:35:58,630 --> 01:36:01,210
and hopefully not too bad,

1881
01:36:01,210 --> 01:36:04,540
and I hope you had learned something in 6.824,

1882
01:36:04,540 --> 01:36:06,130
and enjoyed it at the same time.

1883
01:36:07,320 --> 01:36:08,460
And anyone who wants to stick around,

1884
01:36:08,460 --> 01:36:09,270
please stick around,

1885
01:36:09,270 --> 01:36:13,230
you know more questions you want to ask to the different teams,

1886
01:36:13,230 --> 01:36:15,930
if the teams can stick around to be wonderful,

1887
01:36:15,930 --> 01:36:17,580
otherwise, well, this is the end,

1888
01:36:17,970 --> 01:36:19,980
at least for the class meetings for 6.824.

1889
01:36:20,950 --> 01:36:21,640
Thank you all.

1890
01:36:24,180 --> 01:36:24,900
Thank you.

1891
01:36:25,080 --> 01:36:26,550
Thank you so much.

1892
01:36:26,550 --> 01:36:27,090
Thank you.

1893
01:36:27,270 --> 01:36:28,170
Thank you.

1894
01:36:28,530 --> 01:36:29,700
Thank you so much.

1895
01:36:40,260 --> 01:36:41,850
Sorry, a quick question.

1896
01:36:42,180 --> 01:36:43,350
Okay.

1897
01:36:43,380 --> 01:36:47,220
One last question now for real,

1898
01:36:48,480 --> 01:36:53,880
oh, I I oh I was wondering actually for for logistics for the exam,

1899
01:36:54,630 --> 01:36:56,970
I emailed you.

1900
01:36:57,000 --> 01:37:00,600
Yeah, yeah, we haven't gotten to the point, yet,

1901
01:37:00,600 --> 01:37:02,430
we're dealing with the logistics of the exam.

1902
01:37:02,460 --> 01:37:02,910
Okay.

1903
01:37:03,000 --> 01:37:07,440
A couple, aware of you two three [] to.

1904
01:37:07,530 --> 01:37:11,070
We have plan, we haven't executed yet,

1905
01:37:11,070 --> 01:37:12,690
nor shared any details.

1906
01:37:13,340 --> 01:37:13,910
Okay.

1907
01:37:14,060 --> 01:37:14,810
But it will happen.

1908
01:37:15,260 --> 01:37:16,730
Sounds good.

1909
01:37:16,970 --> 01:37:20,030
You don't need me, make sure you reach in chat.

1910
01:37:20,030 --> 01:37:25,040
Alright, perfect,

1911
01:37:25,040 --> 01:37:26,750
thank you so much for everything,

1912
01:37:26,780 --> 01:37:28,970
for the class and you know for the TAs.

1913
01:37:30,740 --> 01:37:33,970
Thank you very much for all the class,

1914
01:37:33,970 --> 01:37:37,530
so very fun I learned a lot.

1915
01:37:38,060 --> 01:37:39,860
Thank you, thank you for participating,

1916
01:37:40,100 --> 01:37:41,180
asking all these questions,

1917
01:37:41,300 --> 01:37:41,870
I appreciate it.

1918
01:37:44,080 --> 01:37:44,620
Yes, thank you,

1919
01:37:44,620 --> 01:37:45,790
this is this is an awesome class,

1920
01:37:45,790 --> 01:37:46,540
I really appreciate.

1921
01:37:47,860 --> 01:37:50,710
Things were being active during the class.

1922
01:37:53,920 --> 01:37:55,840
Okay, I guess that's probably,

1923
01:37:55,870 --> 01:37:59,440
so I guess let's stop the recording.

