1
00:00:09,730 --> 00:00:13,000
So feel free to get started, whenever you want to be ready.

2
00:00:19,430 --> 00:00:21,470
Alright, should we start then.

3
00:00:22,560 --> 00:00:23,130
Please.

4
00:00:24,420 --> 00:00:27,240
Alright, hey everyone, my name is Felipe,

5
00:00:27,240 --> 00:00:28,770
I'm working with Caralina

6
00:00:28,770 --> 00:00:33,750
and today we're going to be presenting on our project on distributed private electronic voting.

7
00:00:34,400 --> 00:00:39,260
So motivating this project was you know actually sort of sort of easy,

8
00:00:39,290 --> 00:00:44,090
given the current events and elections that had to happen with [covert] restrictions,

9
00:00:44,330 --> 00:00:49,190
so we ask the question how would Internet voting work.

10
00:00:49,760 --> 00:00:55,480
And, we're we're focusing specifically on maintaining voter privacy,

11
00:00:55,970 --> 00:00:58,460
or you know keeping votes private,

12
00:00:59,010 --> 00:01:03,150
and so here's a sketch of how a voting system might work.

13
00:01:03,520 --> 00:01:06,100
You have a bunch of voters,

14
00:01:06,160 --> 00:01:07,840
in this case, you know five

15
00:01:08,110 --> 00:01:10,600
and then a vote counter,

16
00:01:10,600 --> 00:01:13,450
the voters send their votes to the vote counter,

17
00:01:14,060 --> 00:01:16,520
and the vote counter is

18
00:01:16,520 --> 00:01:20,840
you know maybe they send them encrypted or some sort of security,

19
00:01:20,870 --> 00:01:23,060
the vote counter decrypts them,

20
00:01:23,060 --> 00:01:25,370
make sure that each voter votes at most once

21
00:01:25,670 --> 00:01:26,870
and computes a winner.

22
00:01:27,590 --> 00:01:32,540
And so you know key thing to notice here is that,

23
00:01:32,600 --> 00:01:38,360
in order for the vote counter to determine that every voter vote [] at most once,

24
00:01:38,360 --> 00:01:42,170
each vote has to be in some way linked to the voter,

25
00:01:42,500 --> 00:01:43,940
which which is dangerous,

26
00:01:43,970 --> 00:01:45,740
so we're going to explain our threat model here.

27
00:01:46,040 --> 00:01:50,570
Which is we're giving the attacker two sort of capacity

28
00:01:50,570 --> 00:01:52,400
or two two sort of powers,

29
00:01:52,490 --> 00:01:55,940
the first is to create fell stop failures,

30
00:01:56,000 --> 00:01:57,800
but not byzantine failures,

31
00:01:57,800 --> 00:02:00,530
so the attacker can crash server,

32
00:02:00,620 --> 00:02:04,580
but it cannot sort of make it misbehave.

33
00:02:04,930 --> 00:02:08,380
And you know whether there is a reasonable assumption is a question for another day,

34
00:02:08,830 --> 00:02:11,890
but we think there are other protocols that deal with this issues,

35
00:02:11,980 --> 00:02:14,590
we're not dealing with byzantine failures.

36
00:02:15,090 --> 00:02:18,240
The the second power we give them is

37
00:02:18,240 --> 00:02:21,720
to passively spy on a on a vote counter

38
00:02:21,840 --> 00:02:23,280
and so this is problematic,

39
00:02:23,280 --> 00:02:26,970
because as we said votes are linked with their voters

40
00:02:26,970 --> 00:02:28,530
and so a passive attacker,

41
00:02:28,530 --> 00:02:33,600
that's spying on the server, can sort of deanonymized those votes.

42
00:02:33,840 --> 00:02:36,900
And so here's where we come in

43
00:02:36,900 --> 00:02:39,330
and present our distributed voting design,

44
00:02:39,510 --> 00:02:42,360
we're going to deal with the first problem,

45
00:02:42,450 --> 00:02:46,500
which is you know the [adviser] crashing the servers,

46
00:02:46,500 --> 00:02:50,370
and so we are going to have several vote counters,

47
00:02:50,460 --> 00:02:52,380
and the idea is that,

48
00:02:52,410 --> 00:02:56,700
each voter will send other votes to all the vote counters,

49
00:02:57,030 --> 00:03:03,630
and the vote counters will use the same protocols before and compute winners,

50
00:03:04,170 --> 00:03:06,330
[other] winners, sorry,

51
00:03:06,480 --> 00:03:08,760
and it's this is good,

52
00:03:08,760 --> 00:03:11,280
because you add a second crash,

53
00:03:11,310 --> 00:03:14,310
sort of like n-1 vote counters

54
00:03:14,310 --> 00:03:16,890
as long as one of them is up and running,

55
00:03:17,070 --> 00:03:18,810
we will be able to compute a winner.

56
00:03:19,320 --> 00:03:26,340
However it's very you know arguably more unsafe for the second type of attack,

57
00:03:26,340 --> 00:03:27,990
which was passively [fine],

58
00:03:28,020 --> 00:03:32,460
because as long as you know as as a adversary,

59
00:03:32,460 --> 00:03:36,000
the attacker can compromise, even one server,

60
00:03:36,090 --> 00:03:38,490
you know it can deanonymized votes,

61
00:03:38,490 --> 00:03:40,710
so that's where we're introducing Shamir Secret Sharing,

62
00:03:41,370 --> 00:03:44,310
for Shamir sharing, we have a voter,

63
00:03:44,460 --> 00:03:48,450
that's gonna, you know choose a vote either 0 or 1,

64
00:03:48,820 --> 00:03:52,750
and we're gonna, you know I'm not going to explain how Shamir actually works,

65
00:03:52,750 --> 00:03:54,610
it's a cryptographic protocol,

66
00:03:54,640 --> 00:03:58,180
but we're gonna just you know show what it allows us to do,

67
00:03:58,420 --> 00:04:02,980
you pass vote through Shamir, where you give it two parameters, n and k,

68
00:04:03,070 --> 00:04:06,730
and it's going to create n part,

69
00:04:06,880 --> 00:04:10,420
which allow you to re-compute that vote,

70
00:04:10,450 --> 00:04:12,250
the n parts could completely random,

71
00:04:12,280 --> 00:04:14,710
and in fact, what is sort of powerful,

72
00:04:14,710 --> 00:04:21,550
but Shamir is even k-1 shares give you no information about the original vote,

73
00:04:21,580 --> 00:04:24,340
but given k or more shares,

74
00:04:24,370 --> 00:04:27,070
you can use Shamir to re-compute that vote.

75
00:04:29,780 --> 00:04:31,700
So we're going to show Shamir voting scheme.

76
00:04:32,570 --> 00:04:36,290
so, yeah, now for the Shamir voting voting scheme,

77
00:04:36,290 --> 00:04:39,410
so first all the voters are going to choose their vote

78
00:04:39,410 --> 00:04:41,690
and share parts of the vote counters

79
00:04:41,690 --> 00:04:44,330
and the complete vote on the different parts

80
00:04:44,720 --> 00:04:47,990
and so the both counters are going to receive the parts

81
00:04:47,990 --> 00:04:50,180
and when they have the parts of all the voters,

82
00:04:50,610 --> 00:04:52,410
they're going to some these parts

83
00:04:52,650 --> 00:04:54,690
and share the sum with the vote counters,

84
00:04:54,780 --> 00:04:56,820
so here it's important to note that,

85
00:04:56,820 --> 00:04:59,190
this summit looks like completely at random

86
00:04:59,280 --> 00:05:01,440
and so by sharing it with the both counters,

87
00:05:01,560 --> 00:05:07,440
the other both counters, they cannot learn anything about the parts that a received,

88
00:05:07,680 --> 00:05:12,450
so, this, ensures that the privacy of the voters is maintained,

89
00:05:13,970 --> 00:05:16,010
so they exchange their votes

90
00:05:16,100 --> 00:05:19,100
and when a vote counter has received case sums

91
00:05:19,370 --> 00:05:22,700
from other voters are including their part,

92
00:05:22,730 --> 00:05:24,740
they can finally compute the winner,

93
00:05:24,980 --> 00:05:28,340
so using a Shamir secret sharing again, black box,

94
00:05:28,460 --> 00:05:31,520
it will like a re-compute the sum

95
00:05:31,760 --> 00:05:35,960
and if we get something that is greater than half of the number of voters, than the winner is 1,

96
00:05:35,960 --> 00:05:38,120
and if it is less than, the winner would be 0,

97
00:05:38,420 --> 00:05:40,910
and yes so we finally have a winner.

98
00:05:41,680 --> 00:05:44,860
Now, some of the assumptions of our scheme are

99
00:05:44,860 --> 00:05:47,890
first that the voters and vote counters are well behaved

100
00:05:47,890 --> 00:05:53,920
and follow the protocol or their parts some to what they say sounds like good intentions

101
00:05:53,950 --> 00:05:59,470
and we only handle fail stop failures with this scheme,

102
00:05:59,920 --> 00:06:02,410
so yeah now to handle some of the scenarios,

103
00:06:02,410 --> 00:06:07,620
first if we have an unreliable network scenario,

104
00:06:07,620 --> 00:06:12,240
all of the RPCs that we send in our servers are going to be sent periodically

105
00:06:12,240 --> 00:06:15,450
until we receive an acknowledgement that it has been received,

106
00:06:17,130 --> 00:06:19,620
then to handle both failures,

107
00:06:19,710 --> 00:06:22,980
so we we need to persist all of the voters,

108
00:06:23,010 --> 00:06:26,040
like persist the parts they computed and their vote,

109
00:06:26,130 --> 00:06:29,280
because if we re-compute parts,

110
00:06:29,280 --> 00:06:31,380
and then the vote counters had different parts,

111
00:06:31,380 --> 00:06:33,180
then the correctness of the scheme will be gone

112
00:06:33,180 --> 00:06:34,800
and they would not be able to correct the sum,

113
00:06:34,950 --> 00:06:39,720
so it's important to always share like parts from the same computation

114
00:06:39,720 --> 00:06:41,660
and not like change parts

115
00:06:42,500 --> 00:06:45,560
and finally to handle the vote counter failures,

116
00:06:45,560 --> 00:06:49,580
here we rely on Shamir secret sharing scheme

117
00:06:49,670 --> 00:06:51,140
and so as we mentioned before,

118
00:06:51,140 --> 00:06:54,260
we only need k servers to compute the winner,

119
00:06:54,530 --> 00:06:59,540
so, the system is referring to n-k crashes of vote counters.

120
00:07:00,480 --> 00:07:01,950
And now it is demo time,

121
00:07:01,980 --> 00:07:05,580
so I'll stop sharing screen and share another screen.

122
00:07:08,520 --> 00:07:11,190
And so here is our demo,

123
00:07:11,310 --> 00:07:13,920
basically we have 5 voters here,

124
00:07:13,920 --> 00:07:16,470
so we have 3 vote counters, 5 voters

125
00:07:16,470 --> 00:07:18,120
and k is equal to 2,

126
00:07:18,710 --> 00:07:22,100
this means that network is unreliable,

127
00:07:22,280 --> 00:07:23,480
and now if we run it,

128
00:07:26,000 --> 00:07:27,860
here, so we got 3

129
00:07:27,860 --> 00:07:30,470
and we get the winner is 1,

130
00:07:30,860 --> 00:07:32,900
we can also crash one of the servers,

131
00:07:33,530 --> 00:07:35,060
since k is equal to 2,

132
00:07:35,060 --> 00:07:39,050
we can still compute a winner by having only two servers up,

133
00:07:39,230 --> 00:07:40,640
and so now if we run it,

134
00:07:40,970 --> 00:07:44,090
we get the winner of the election is say one again,

135
00:07:44,210 --> 00:07:46,370
and this is the end of our presentation,

136
00:07:46,370 --> 00:07:47,810
thank you very much for listening

137
00:07:47,810 --> 00:07:49,130
and we'll take some questions.

138
00:08:03,940 --> 00:08:05,320
Feel free to ask questions.

139
00:08:12,130 --> 00:08:15,160
Curious, how extensively you tested this system,

140
00:08:15,430 --> 00:08:19,660
like do you have a try in much different other configurations,

141
00:08:19,720 --> 00:08:21,430
do you have performance numbers?

142
00:08:22,680 --> 00:08:29,190
Sure, so, we we did have like different sizes of,

143
00:08:29,220 --> 00:08:30,660
we create a whole test suite

144
00:08:30,690 --> 00:08:36,120
and make sure to test like voter failures, server failures have different sizes

145
00:08:36,360 --> 00:08:39,990
and we don't have performance numbers,

146
00:08:40,110 --> 00:08:41,160
we didn't test that,

147
00:08:41,190 --> 00:08:43,620
but in terms of like failures

148
00:08:43,620 --> 00:08:47,220
and and different like numbers of voters vote counters,

149
00:08:47,220 --> 00:08:49,410
yeah we have a full test suite

150
00:08:49,410 --> 00:08:52,440
and that we should have put the link and get for github,

151
00:08:52,440 --> 00:08:55,810
but we can you know send it to you, if you, you want,

152
00:08:55,810 --> 00:09:00,690
so you can check like look over implementation, under testing.

153
00:09:09,490 --> 00:09:13,070
There, more questions?

154
00:09:13,070 --> 00:09:16,220
Can you say something maybe a little bit like what ideas from 6.824

155
00:09:16,220 --> 00:09:18,290
were you able to apply in the in this system,

156
00:09:20,390 --> 00:09:22,100
other than using the testing framework.

157
00:09:24,170 --> 00:09:26,810
Sure, kind of do you want to take this one or should I take?

158
00:09:27,340 --> 00:09:30,730
Sure, I'm, so here,

159
00:09:30,730 --> 00:09:35,830
I guess, yeah this game actually comes from a pizza from the cryptography class

160
00:09:35,860 --> 00:09:39,310
and so it was fun to look at it from a different perspective,

161
00:09:39,310 --> 00:09:43,150
so now we're not like focusing that much on the security,

162
00:09:43,150 --> 00:09:46,720
I need but more like what happens if the individual like servers,

163
00:09:46,720 --> 00:09:47,620
if they fail,

164
00:09:47,650 --> 00:09:55,690
so both handling voter failures, handling vote counter failures, problems with the network,

165
00:09:55,720 --> 00:09:59,020
I guess we didn't address specific partitions,

166
00:09:59,020 --> 00:10:01,900
because it would work similar to both counter crushed,

167
00:10:02,290 --> 00:10:08,740
but yeah it was it was fun to look at this problem from a different perspective.

168
00:10:11,300 --> 00:10:13,460
Yeah, thanks.

169
00:10:14,300 --> 00:10:15,470
Yeah, you have something to say.

170
00:10:15,500 --> 00:10:16,610
We're out of time a bit, so,

171
00:10:17,940 --> 00:10:21,000
yeah, cool awesome, nice job, thanks for sharing.

172
00:10:21,360 --> 00:10:24,960
Okay, we have a presentation on private analytics.

173
00:10:27,830 --> 00:10:30,380
If Kevin is ready to share, awesome,

174
00:10:30,900 --> 00:10:31,950
take it away, I'll be ready.

175
00:10:34,150 --> 00:10:36,940
Okay, you guys can hear me and you guys can see my pointer?

176
00:10:37,420 --> 00:10:37,930
Yes.

177
00:10:38,440 --> 00:10:39,940
Okay, cool,

178
00:10:40,390 --> 00:10:41,470
everyone, I'm Kevin

179
00:10:41,470 --> 00:10:45,010
and today I'm going to be presenting this very creatively named system called Sys,

180
00:10:45,310 --> 00:10:49,750
and this is a system for collecting aggregate statistics in a privacy preserving way,

181
00:10:50,440 --> 00:10:53,320
and so the last presentation was good leading to my project,

182
00:10:53,350 --> 00:10:55,480
so for the most part in 6.824,

183
00:10:55,780 --> 00:11:00,880
we've talked about how we can build reliable systems in the presence of byzantine or [] fails,

184
00:11:01,030 --> 00:11:02,080
so servers can crash

185
00:11:02,080 --> 00:11:03,820
and clients are generally well behaved,

186
00:11:04,030 --> 00:11:05,110
but from this talk,

187
00:11:05,110 --> 00:11:06,610
I hope you learn some new concepts

188
00:11:06,610 --> 00:11:11,200
on how we can build systems with strong guarantees in the presence of byzantine failures

189
00:11:11,440 --> 00:11:13,690
on behalf of both clients and servers within the system.

190
00:11:14,450 --> 00:11:18,770
And the main tools that we're going to use to achieve these guarantees are cryptographic primitives,

191
00:11:18,770 --> 00:11:21,770
such as multiparty computation and zero-knowledge proofs

192
00:11:21,860 --> 00:11:24,470
and also distributed computing primitives, such as broadcast.

193
00:11:27,040 --> 00:11:28,210
Okay, so for simplicity,

194
00:11:28,240 --> 00:11:30,670
let's say we just want to build a system that computes sums,

195
00:11:30,820 --> 00:11:32,350
so we're going to have an aggregation server,

196
00:11:32,350 --> 00:11:34,210
that stores key value store,

197
00:11:34,450 --> 00:11:37,720
so the keys are going to be indices for some statistics

198
00:11:37,780 --> 00:11:39,910
and the values are going to be tuples of sums,

199
00:11:40,850 --> 00:11:42,350
and we're gonna have a bunch of clients,

200
00:11:42,440 --> 00:11:44,330
so each client is gonna have some identity,

201
00:11:44,330 --> 00:11:45,980
say its client IP addresses,

202
00:11:46,380 --> 00:11:48,930
it's going to have the index of the statistic they wanna bump

203
00:11:49,230 --> 00:11:51,570
and it's also going to have its private inputs,

204
00:11:52,040 --> 00:11:53,480
so the most straightforward thing to do is,

205
00:11:53,480 --> 00:11:57,230
we can have all the clients send their inputs to the server as is

206
00:11:57,230 --> 00:11:58,430
and they can compete the sums,

207
00:11:59,120 --> 00:11:59,990
but obviously this is bad,

208
00:11:59,990 --> 00:12:02,180
because now this leads everything, right,

209
00:12:02,330 --> 00:12:04,010
the server will learn the client's identity,

210
00:12:04,130 --> 00:12:05,570
it'll learn the index being bumped

211
00:12:05,750 --> 00:12:07,580
and they'll learn their private input.

212
00:12:09,130 --> 00:12:10,330
So we can do a little bit better,

213
00:12:10,420 --> 00:12:12,220
we can compute these sums privately,

214
00:12:12,340 --> 00:12:14,800
if we deploy to non colluding servers,

215
00:12:15,070 --> 00:12:19,180
and then we're going to have each client secret share their input to each of these servers,

216
00:12:19,620 --> 00:12:21,570
so as we said in the previous presentation,

217
00:12:21,570 --> 00:12:26,730
so each server, each signature alone leaks no information about the client's private input,

218
00:12:27,000 --> 00:12:29,760
but still the servers can add up these shares

219
00:12:29,760 --> 00:12:33,000
and compute a local version of the key value store,

220
00:12:33,660 --> 00:12:36,720
and then later when the servers want to recover the actual sums,

221
00:12:36,810 --> 00:12:38,760
they can combine their local key value stores

222
00:12:38,760 --> 00:12:40,860
to reconstruct the global key value store,

223
00:12:42,120 --> 00:12:43,140
and this is a little bit better,

224
00:12:43,140 --> 00:12:45,660
at least if one of these servers is honest,

225
00:12:45,810 --> 00:12:48,030
then the servers are still gonna learn the client's identity,

226
00:12:48,060 --> 00:12:50,310
they're still going to learn the client's index,

227
00:12:50,370 --> 00:12:53,040
but now instead of learning each individual client's input,

228
00:12:53,130 --> 00:12:56,220
they're going to learn the sums of all clients and inputs,

229
00:12:56,880 --> 00:12:57,780
okay, so that's better,

230
00:12:57,780 --> 00:12:59,220
but still a problem,

231
00:12:59,220 --> 00:13:03,690
namely that this identity index relation can still leak a lot of information.

232
00:13:04,510 --> 00:13:05,620
So how can we fix this,

233
00:13:05,680 --> 00:13:07,120
we can make things anonymous,

234
00:13:07,330 --> 00:13:10,300
so we're still going to adapt setup from the [floor],

235
00:13:10,510 --> 00:13:13,990
but now we're going to give each server a public key for encryption,

236
00:13:14,840 --> 00:13:17,840
and that each client is going to encrypt each of their shares,

237
00:13:18,380 --> 00:13:21,380
and instead of having the clients send their shares directly to servers,

238
00:13:21,410 --> 00:13:24,050
now we're going to have a layer of forwarding proxies in between,

239
00:13:24,980 --> 00:13:25,880
and so what's going to happen is,

240
00:13:25,880 --> 00:13:31,520
the clients going to send their encrypted shares via broadcast to those proxies

241
00:13:31,700 --> 00:13:35,570
and the proxies will route each share to their respective servers,

242
00:13:36,160 --> 00:13:39,490
and then the aggregation can go as as explained,

243
00:13:40,720 --> 00:13:42,370
and so what are the privacy guarantees here,

244
00:13:42,670 --> 00:13:44,860
if at least one of these properties is honest,

245
00:13:44,950 --> 00:13:47,380
then the proxy will still learn the client's identity

246
00:13:47,650 --> 00:13:49,210
and it'll learn some timing information,

247
00:13:49,210 --> 00:13:51,430
based on when the client set the share,

248
00:13:52,100 --> 00:13:52,940
but nothing else,

249
00:13:52,940 --> 00:13:55,400
because the shares are encrypted to the server,

250
00:13:55,400 --> 00:13:56,930
so they learn nothing from that,

251
00:13:58,100 --> 00:14:00,980
and then if at least one of the servers is honest,

252
00:14:01,010 --> 00:14:04,700
then the servers will also learn some timing information based on the proxy forward it,

253
00:14:05,180 --> 00:14:07,400
it will certainly learn the index of the statistic

254
00:14:07,490 --> 00:14:08,510
and learn the sum,

255
00:14:09,260 --> 00:14:15,470
but most importantly as long as not both proxy and server are compromised at the same time,

256
00:14:15,590 --> 00:14:18,830
then this design unlinks the identity from the index being bumped,

257
00:14:18,830 --> 00:14:21,000
which is exactly what we wanted, right,

258
00:14:21,000 --> 00:14:21,840
so this is great,

259
00:14:21,960 --> 00:14:24,210
but this also leads to another problem,

260
00:14:24,300 --> 00:14:27,540
namely that now clients can hide behind the privacy

261
00:14:27,540 --> 00:14:31,230
and anonymity guarantees of the system to send bad inputs, right,

262
00:14:31,230 --> 00:14:34,590
so let's say the system expected client inputs to be zeros and ones,

263
00:14:34,710 --> 00:14:36,300
well behind this [] of privacy,

264
00:14:36,330 --> 00:14:39,600
now the client can send the secret share of a billion through the system,

265
00:14:39,750 --> 00:14:42,450
and now it can undetectably skew this sum,

266
00:14:42,840 --> 00:14:44,190
so clearly this is bad.

267
00:14:45,280 --> 00:14:45,970
Let's fix this,

268
00:14:45,970 --> 00:14:47,680
we want to make the system more robust,

269
00:14:47,830 --> 00:14:49,510
so what we're going to do is,

270
00:14:50,320 --> 00:14:51,640
we're going to have each client generate,

271
00:14:51,640 --> 00:14:54,610
what's called a zero-knowledge proof over their shares

272
00:14:54,610 --> 00:14:55,930
and send these to the servers,

273
00:14:56,680 --> 00:14:59,500
and the many servers collect these zero-knowledge proofs,

274
00:14:59,590 --> 00:15:03,250
they can interactively check that the client's inputs,

275
00:15:03,250 --> 00:15:07,930
actually the client shares actually reconstruct to some well formed input,

276
00:15:08,640 --> 00:15:10,920
and because this proof is in zero-knowledge,

277
00:15:11,010 --> 00:15:15,420
it leaves nothing about input, other than that it's well formed,

278
00:15:16,160 --> 00:15:18,080
and so again our privacy properties stay the same,

279
00:15:18,080 --> 00:15:20,120
the property still learns that client's identity,

280
00:15:20,210 --> 00:15:21,950
is still learns some timing information,

281
00:15:22,340 --> 00:15:23,600
the server learns timing information,

282
00:15:23,600 --> 00:15:25,130
it learns the index and learns the sums,

283
00:15:25,510 --> 00:15:28,330
but now we've protected the system against malicious clients,

284
00:15:28,480 --> 00:15:30,670
because it'll only accept well formed inputs,

285
00:15:31,610 --> 00:15:32,570
and so this is great,

286
00:15:32,720 --> 00:15:34,460
still there's another problem,

287
00:15:34,460 --> 00:15:36,050
which is that servers can crash

288
00:15:36,260 --> 00:15:37,190
and we can lose data,

289
00:15:37,220 --> 00:15:39,290
so we obviously need both servers to be online

290
00:15:39,290 --> 00:15:42,200
in order to reconstruct global key value store.

291
00:15:42,840 --> 00:15:44,550
And so if we want to make the system more reliable,

292
00:15:44,550 --> 00:15:47,700
we can do what we know best which is to replicate the servers

293
00:15:47,850 --> 00:15:49,890
and so we can use raft style replication,

294
00:15:49,890 --> 00:15:52,500
or we can also use primary backup style replication.

295
00:15:53,340 --> 00:15:54,240
Now the question is,

296
00:15:54,240 --> 00:15:57,090
okay with all this replication, all this cryptographic machinery

297
00:15:57,120 --> 00:15:58,110
and on this message routing,

298
00:15:58,110 --> 00:16:00,090
can we still achieve good throughput,

299
00:16:01,180 --> 00:16:03,970
and it turns out that we can actually paralyze the server step here,

300
00:16:04,000 --> 00:16:05,740
that does proof checking navigation,

301
00:16:05,770 --> 00:16:07,900
which is likely to be the bottleneck of the system

302
00:16:08,260 --> 00:16:09,490
and what's going to happen is that,

303
00:16:09,490 --> 00:16:11,380
proxies are going to hash partition,

304
00:16:11,410 --> 00:16:13,030
their inputs to each of these servers

305
00:16:13,300 --> 00:16:15,580
and then each of these servers in reduce step,

306
00:16:15,610 --> 00:16:17,740
will combine their intermediate key value stores

307
00:16:17,800 --> 00:16:20,950
to reconstruct the global key value store containing all the sums.

308
00:16:21,910 --> 00:16:22,420
Okay?

309
00:16:23,200 --> 00:16:24,400
And so now the final question is,

310
00:16:24,400 --> 00:16:26,230
did I implement all this before the due date,

311
00:16:27,040 --> 00:16:27,730
unfortunately, no,

312
00:16:27,730 --> 00:16:29,950
but I did make it much of the way there,

313
00:16:29,950 --> 00:16:34,090
so I'm gonna show a quick demo of the non replicated, non parallel version of it.

314
00:16:34,700 --> 00:16:37,580
So I'm going to quickly switch to my other laptop,

315
00:16:42,520 --> 00:16:44,080
I gotta stop sharing this one first.

316
00:16:46,630 --> 00:16:53,040
Yes, cool,

317
00:16:53,070 --> 00:16:54,840
okay, so great,

318
00:16:55,430 --> 00:16:57,800
so, on these right two terminals are going to be with servers,

319
00:16:57,800 --> 00:16:58,700
so I'm going to run them,

320
00:17:00,640 --> 00:17:02,080
it is implemented in Rust,

321
00:17:02,200 --> 00:17:05,020
now I'm going to hook up these two properties in the middle,

322
00:17:07,420 --> 00:17:08,410
and then on the left terminal,

323
00:17:08,410 --> 00:17:10,720
I'm just gonna simulate a thousand honest clients.

324
00:17:11,820 --> 00:17:14,970
And what's happening is all the clients are generating their input sharing

325
00:17:14,970 --> 00:17:16,380
and generating zero-knowledge proofs

326
00:17:16,440 --> 00:17:17,910
and sending them with proxies,

327
00:17:18,320 --> 00:17:20,780
and the proxies are simply forwarding them to the servers,

328
00:17:21,280 --> 00:17:22,930
and then here finally on the server side,

329
00:17:22,930 --> 00:17:24,520
they're going to be checking all the proofs

330
00:17:24,610 --> 00:17:26,830
and if the inputs are in fact well formed,

331
00:17:26,830 --> 00:17:29,170
it's going to add it to its local key value store

332
00:17:29,560 --> 00:17:30,490
and then at some time later,

333
00:17:30,490 --> 00:17:33,040
when the servers want to reconstruct the final statistics,

334
00:17:33,040 --> 00:17:35,620
they can just combine their key value source to recover the sums.

335
00:17:36,760 --> 00:17:38,740
And that's it for my presentation,

336
00:17:38,740 --> 00:17:40,090
I'm happy to take any questions.

337
00:17:45,830 --> 00:17:47,300
Do you have any questions from the audience?

338
00:17:54,380 --> 00:17:55,490
I guess I have a question,

339
00:17:55,550 --> 00:17:58,130
so like with what you implemented so far,

340
00:17:58,220 --> 00:18:03,020
what sort of, like which at what point do you accept,

341
00:18:03,230 --> 00:18:04,820
what point do tolerate failures

342
00:18:04,820 --> 00:18:10,730
and like stuff like that I can talk about the reliability of your current implementation.

343
00:18:11,640 --> 00:18:15,690
Yeah, so the reliability of the current implementation is not great,

344
00:18:16,050 --> 00:18:18,270
mostly because the servers aren't replicated,

345
00:18:18,720 --> 00:18:19,770
so for the proxies,

346
00:18:19,770 --> 00:18:23,100
because, the clients broadcast the proxies,

347
00:18:23,100 --> 00:18:25,830
all you require is that one of the proxies is up,

348
00:18:26,540 --> 00:18:27,560
so if we have two proxies,

349
00:18:27,650 --> 00:18:29,990
we can tolerate one failure of the proxies

350
00:18:29,990 --> 00:18:33,110
and will still get the messages to the servers,

351
00:18:33,380 --> 00:18:35,270
but if any of the servers goes down,

352
00:18:35,300 --> 00:18:38,030
then you're just not going to be reconstruct on the data for that.

353
00:18:41,670 --> 00:18:43,410
Is it only for sums

354
00:18:43,410 --> 00:18:48,630
or did you implemented for any general function that operates on the all those inputs.

355
00:18:49,620 --> 00:18:52,770
Yeah, for now, I've only implemented for sums,

356
00:18:52,770 --> 00:18:55,320
but basically with this additive secret sharing scheme,

357
00:18:55,350 --> 00:18:58,260
you can compute any linear function you want,

358
00:18:59,000 --> 00:19:00,770
and maybe more complex ones are possible,

359
00:19:00,770 --> 00:19:02,960
but I'm still haven't explored those yet,

360
00:19:04,100 --> 00:19:06,020
it turns out at least in practice,

361
00:19:06,020 --> 00:19:08,540
sums probably get like 90% of the way there.

362
00:19:10,350 --> 00:19:13,830
So what do you performance numbers looked like?

363
00:19:14,790 --> 00:19:15,990
Performance numbers, yeah,

364
00:19:16,080 --> 00:19:18,330
so the main things we want to measure are,

365
00:19:18,420 --> 00:19:21,750
for the client side clients on computation and client bandwidth,

366
00:19:22,080 --> 00:19:23,310
I have some numbers,

367
00:19:23,310 --> 00:19:24,600
at least for client computation,

368
00:19:24,690 --> 00:19:28,650
I'm generating these shares and these proofs takes less than a few milliseconds,

369
00:19:28,650 --> 00:19:30,270
so it's very lightweight,

370
00:19:30,450 --> 00:19:32,340
the bandwidth is just a few kilobytes,

371
00:19:32,840 --> 00:19:35,510
and then for the throughput on the server side,

372
00:19:35,540 --> 00:19:37,400
I'm actually ran on EC2,

373
00:19:37,400 --> 00:19:40,880
but I only allocated four cores to each server,

374
00:19:41,090 --> 00:19:42,350
because I only had 64 cores

375
00:19:42,350 --> 00:19:43,820
and I wanted most of them to be on the client,

376
00:19:43,820 --> 00:19:45,530
so I could remove that bottleneck

377
00:19:45,680 --> 00:19:46,790
and so at four cores,

378
00:19:46,790 --> 00:19:49,930
I think, what is that,

379
00:19:49,930 --> 00:19:52,420
probably like a thousand queries per second

380
00:19:52,780 --> 00:19:58,300
and then estimating I guess if you paralyze by twenty servers for each logical machine,

381
00:19:58,360 --> 00:20:02,680
they can probably achieve close to 22000 queries per second,

382
00:20:03,110 --> 00:20:05,390
but this is all run on the same data center,

383
00:20:05,390 --> 00:20:07,520
so it doesn't factor into latency,

384
00:20:07,550 --> 00:20:10,100
so the actual numbers will probably be a little bit lower than that.

385
00:20:12,040 --> 00:20:14,050
I had a question about your implementation,

386
00:20:14,170 --> 00:20:21,080
how do you do, so how do you actually implement your knowledge proofs and code not theoretical.

387
00:20:21,770 --> 00:20:23,780
Yeah, that's a great question,

388
00:20:24,170 --> 00:20:28,130
I can send you the paper, that I implemented it out,

389
00:20:28,950 --> 00:20:31,350
but it's it's not too complicated,

390
00:20:31,350 --> 00:20:33,870
basically it's just a bunch of finite field operations

391
00:20:33,870 --> 00:20:35,400
and so if you find your file library,

392
00:20:35,400 --> 00:20:37,560
just follow the paper and follow the steps

393
00:20:37,920 --> 00:20:40,510
and it's so much straightforward from that point,

394
00:20:40,660 --> 00:20:43,300
as long as you can decrypt the paper.

395
00:20:44,020 --> 00:20:47,560
Okay, and is that and is it possible to like test that,

396
00:20:48,100 --> 00:20:51,620
like, like, how would you know that it's working or not working.

397
00:20:52,070 --> 00:20:52,850
Yeah, so I guess,

398
00:20:52,880 --> 00:20:55,760
yeah, I only showed the simulation with a honest clients,

399
00:20:55,760 --> 00:20:59,990
but you can also generate a simulation with clients that submit bad proofs

400
00:20:59,990 --> 00:21:02,300
and then you can see them being rejected.

401
00:21:02,960 --> 00:21:04,520
Okay, yeah, that makes sense, thank you.

402
00:21:04,550 --> 00:21:04,940
Yeah.

403
00:21:10,280 --> 00:21:12,530
Great, thanks, pretty sweet,

404
00:21:12,920 --> 00:21:15,170
is the next group ready to go?

405
00:21:22,300 --> 00:21:27,280
Hello, I'm Shannon and Nik and Johan on here,

406
00:21:27,280 --> 00:21:29,680
so Johan take it away.

407
00:21:32,850 --> 00:21:33,750
Thank you, Shannon,

408
00:21:33,900 --> 00:21:35,520
so we'll be talking about BukaDocs,

409
00:21:35,550 --> 00:21:38,010
BukaDocs is a distributive collaborative editor,

410
00:21:38,340 --> 00:21:39,540
it's similar to Google docs,

411
00:21:39,540 --> 00:21:40,560
just a little bit better,

412
00:21:41,130 --> 00:21:44,400
so to move onto the next slide,

413
00:21:45,090 --> 00:21:46,410
can you look next for them.

414
00:21:47,140 --> 00:21:49,390
Okay, so as you've seen in this class,

415
00:21:49,390 --> 00:21:52,720
achieving consistency is very hard to do in the distributed system,

416
00:21:53,140 --> 00:21:56,050
there are many ways that consistency could go wrong,

417
00:21:56,440 --> 00:21:58,360
so a very simple example is,

418
00:21:58,360 --> 00:22:02,800
if the order that you received RPCs is different in each peer,

419
00:22:02,950 --> 00:22:04,990
you might end up with an inconsistent state,

420
00:22:06,100 --> 00:22:10,600
and there's a data structure called CRDTs,

421
00:22:10,600 --> 00:22:14,860
which we use in our system to mitigate this issue,

422
00:22:14,890 --> 00:22:18,850
CRDTs achieve eventually consistency,

423
00:22:19,300 --> 00:22:22,810
by making every single operation that you make on the document,

424
00:22:22,930 --> 00:22:24,070
globally unique,

425
00:22:24,100 --> 00:22:27,160
not just unique to every single peer, but globally unique,

426
00:22:27,250 --> 00:22:29,560
so if I press the letter a on my editor,

427
00:22:29,860 --> 00:22:33,160
it's different from Shannon or Nik pressing the letter a on their editor,

428
00:22:34,280 --> 00:22:38,550
so, for example here even if the bottom here,

429
00:22:38,550 --> 00:22:42,300
for example here adds a new turtle to a document,

430
00:22:43,260 --> 00:22:48,030
even if they receive remove request from the other two peers,

431
00:22:48,090 --> 00:22:50,460
they will never remove the golden turtle,

432
00:22:50,790 --> 00:22:54,330
because the operation is itself different from remove,

433
00:22:54,510 --> 00:22:58,740
so remove green turtle is different from remove gold turtle.

434
00:22:59,980 --> 00:23:02,860
And this, this is how we achieve eventual consistency,

435
00:23:03,220 --> 00:23:07,510
so from here, I believe Nik is going to talk a little bit more about what is here,

436
00:23:07,510 --> 00:23:08,890
how we implement [] [entities].

437
00:23:09,600 --> 00:23:15,960
Yeah, so for Bukadocs, we chose to use a CRDT called LSEQ,

438
00:23:16,020 --> 00:23:20,250
which represents a sequence of elements with variable length keys,

439
00:23:20,430 --> 00:23:21,780
so the goal is,

440
00:23:21,900 --> 00:23:24,720
let's say we want a sequence that represents the alphabet

441
00:23:24,720 --> 00:23:26,850
and so far we have the letters a and c,

442
00:23:27,460 --> 00:23:31,480
so one editor might choose to try adding the letter b between them

443
00:23:31,720 --> 00:23:35,470
and another editor may choose to try adding the letter d after c,

444
00:23:35,890 --> 00:23:38,770
and the goal is that with eventual consistency,

445
00:23:38,770 --> 00:23:42,010
will eventually reach the state a b c d,

446
00:23:42,040 --> 00:23:43,930
so the way that LSEQ achieves this is,

447
00:23:43,930 --> 00:23:45,880
by using a START and END token,

448
00:23:46,350 --> 00:23:50,670
and then it gives every character in the document an individual token,

449
00:23:50,670 --> 00:23:52,260
that is between the start and the end,

450
00:23:52,560 --> 00:23:55,020
so we can insert h between start end,

451
00:23:55,350 --> 00:23:57,540
if we want the letter i after h,

452
00:23:57,630 --> 00:24:00,600
then we can insert that at 7, which is between 4 and end,

453
00:24:01,060 --> 00:24:05,440
now, if we want to insert an exclamation point between i and the end of the document,

454
00:24:05,680 --> 00:24:08,230
we can insert it with the key 7 2,

455
00:24:08,410 --> 00:24:14,440
so we increase key length or two to create key between two other keys that are adjacent,

456
00:24:14,890 --> 00:24:18,790
so in this way, we can always create a key between any two other keys,

457
00:24:19,000 --> 00:24:20,800
so we can always insert,

458
00:24:21,540 --> 00:24:24,780
now LSEQ forms well,

459
00:24:24,780 --> 00:24:28,860
in that it reaches eventual consistency with very little effort for coordination,

460
00:24:29,280 --> 00:24:35,010
and it has some optimizations that cause the length of the keys to grow relatively slowly.

461
00:24:36,000 --> 00:24:41,370
However, some cons are that in order to support deletion of these elements,

462
00:24:41,370 --> 00:24:45,120
it relies on causal delivery and exactly-once delivery,

463
00:24:45,120 --> 00:24:46,710
and we didn't really want to implement this,

464
00:24:46,710 --> 00:24:49,050
since it was based off a number of other works,

465
00:24:49,420 --> 00:24:53,140
so we use a slightly simpler approach, which was a deletion set,

466
00:24:53,660 --> 00:24:56,300
so this is a grow only set, where we add in elements,

467
00:24:56,510 --> 00:24:59,090
so for instance to delete letters h and i,

468
00:24:59,090 --> 00:25:01,850
we would add in 4 7 into this deletion set,

469
00:25:02,580 --> 00:25:07,440
then this whole state becomes equivalent to just having to start and end tokens

470
00:25:07,440 --> 00:25:09,810
and the exclamation point at key 7 2.

471
00:25:14,820 --> 00:25:16,740
So we built the Buka docs service

472
00:25:16,770 --> 00:25:19,290
similar to how we implemented kv raft,

473
00:25:19,290 --> 00:25:21,600
we have multiple servers, multiple clients

474
00:25:21,600 --> 00:25:25,080
and multiple clients, they only talk to one server at a time

475
00:25:25,080 --> 00:25:31,020
and they continuously try the operations until they get a successful reply from the server,

476
00:25:34,320 --> 00:25:39,450
clients and servers both maintain an AVL tree of characters in the document,

477
00:25:39,450 --> 00:25:43,260
as well as our own deletion set of removed keys,

478
00:25:43,260 --> 00:25:46,710
we chose to store characters in AVL tree,

479
00:25:46,710 --> 00:25:48,870
for I guess performance reasons,

480
00:25:50,340 --> 00:25:52,290
the chain of events goes something like this,

481
00:25:52,290 --> 00:25:55,530
so the client will send an insertion and deletion to the server,

482
00:25:55,860 --> 00:26:00,240
a server will update its own AVL tree and deletion set and persist that,

483
00:26:00,630 --> 00:26:03,300
and for those updates to all the other servers and clients

484
00:26:03,300 --> 00:26:06,630
and then the server will respond success to the client.

485
00:26:07,870 --> 00:26:10,660
So we're going to demo it,

486
00:26:11,200 --> 00:26:15,640
we build a very simple UI for it here,

487
00:26:15,940 --> 00:26:17,350
so let me give it,

488
00:26:18,350 --> 00:26:23,900
Johan and Nik are also accessing this from different clients right now,

489
00:26:24,230 --> 00:26:27,170
so you can see them typing,

490
00:26:27,170 --> 00:26:28,580
you can type something else,

491
00:26:29,330 --> 00:26:30,740
I'm typing here,

492
00:26:34,210 --> 00:26:37,750
I think Johan is typing hi,

493
00:26:38,710 --> 00:26:40,810
Nik is typing something here,

494
00:26:53,360 --> 00:26:57,310
and we can edit each others text as well.

495
00:27:02,930 --> 00:27:08,010
Then, yeah, that is Buka Docs,

496
00:27:12,770 --> 00:27:16,700
we're happy to answer any questions, I guess.

497
00:27:21,550 --> 00:27:22,600
I have a question here,

498
00:27:23,050 --> 00:27:25,150
I really like the turtle theme,

499
00:27:25,150 --> 00:27:26,380
that you guys came up with this

500
00:27:26,380 --> 00:27:29,680
and I was just wondering where the, where the turtle theme came from.

501
00:27:30,930 --> 00:27:37,170
Oh yeah, so the turtle is Nik Johan and I also taught the class over IP

502
00:27:37,170 --> 00:27:40,170
and the turtle is our mascot for the class

503
00:27:40,170 --> 00:27:42,510
and we are inspired to build Buka docs,

504
00:27:42,510 --> 00:27:47,100
because we use Google docs is a question document for the students ask questions,

505
00:27:47,100 --> 00:27:51,270
but it couldn't handle a load of over 75 users typing at the same time

506
00:27:51,570 --> 00:27:53,610
and that's how Buka docs was born.

507
00:27:54,560 --> 00:27:55,670
Thank you for ask.

508
00:27:56,360 --> 00:27:56,840
That's cool.

509
00:27:58,060 --> 00:27:59,560
I had a quick question,

510
00:27:59,740 --> 00:28:02,590
and so first of all, I got called little presentation,

511
00:28:02,590 --> 00:28:04,180
sorry, if you mentioned this, you may have,

512
00:28:04,210 --> 00:28:09,280
but so first of all, first question just to make sure I understood correctly,

513
00:28:09,280 --> 00:28:11,800
is the are these data structures which are very interesting,

514
00:28:11,800 --> 00:28:13,390
they stored on the client or the server,

515
00:28:14,090 --> 00:28:16,850
if they're stored, on I imagine right now the server is not replicated,

516
00:28:16,850 --> 00:28:18,260
but that's something you could easily do,

517
00:28:18,290 --> 00:28:21,050
but if they if they are stored on the client,

518
00:28:21,500 --> 00:28:23,180
what happens if one of the clients fails.

519
00:28:24,500 --> 00:28:28,820
So the data structures are actually stored on all the clients and all the servers,

520
00:28:29,660 --> 00:28:33,020
and so right now, we're assuming the clients are fully trustworthy,

521
00:28:33,230 --> 00:28:38,330
and so if client fails, any edits they haven't sent to a server,

522
00:28:38,360 --> 00:28:41,690
will just be on their end until they come back online,

523
00:28:41,690 --> 00:28:43,580
in which case they can send it

524
00:28:43,580 --> 00:28:45,890
and it will achieve eventual consistency.

525
00:28:47,180 --> 00:28:48,380
Interesting, thank you.

526
00:28:49,850 --> 00:28:50,930
Oh, I had a question,

527
00:28:50,990 --> 00:28:53,840
why LSEQ over other CRDTs.

528
00:28:58,460 --> 00:29:01,520
Yeah, we chose LSEQ mainly just because,

529
00:29:01,520 --> 00:29:03,260
it was one of the first ones we found

530
00:29:03,260 --> 00:29:04,400
and we wanted to get started

531
00:29:04,580 --> 00:29:08,900
and because after implementing the logic of the variable length keys,

532
00:29:08,900 --> 00:29:10,250
there was really not much more,

533
00:29:10,250 --> 00:29:11,900
we have to do in order to make sure,

534
00:29:11,900 --> 00:29:13,880
that the clients and servers stayed consistent.

535
00:29:15,610 --> 00:29:16,930
Oh, okay, thank you.

536
00:29:16,930 --> 00:29:19,060
So beyond that it was just like message passing

537
00:29:19,060 --> 00:29:20,860
and making sure everyone gets all the messages.

538
00:29:22,100 --> 00:29:23,300
Okay, thanks.

539
00:29:24,780 --> 00:29:27,120
So, how many servers are you running within this example,

540
00:29:27,120 --> 00:29:28,590
was it just one or was it multiple?

541
00:29:30,080 --> 00:29:32,480
There were three servers and three clients,

542
00:29:32,510 --> 00:29:35,960
each one of the clients was connected to known to its own server.

543
00:29:37,070 --> 00:29:43,760
So you are using the servers for scaling or for fault tolerance, or both?

544
00:29:44,900 --> 00:29:46,190
A little bit for both,

545
00:29:46,190 --> 00:29:48,530
when we ran the performance metrics,

546
00:29:49,040 --> 00:29:53,510
by distributing the load of the client requests across many servers,

547
00:29:53,510 --> 00:29:56,120
you are able to handle a little bit more bandwidth request,

548
00:29:56,740 --> 00:29:58,780
however, this is a trade-off between,

549
00:29:58,810 --> 00:30:01,390
because eventual consistency will take a little bit more time,

550
00:30:01,390 --> 00:30:03,730
because the servers will have to send RPCs,

551
00:30:04,400 --> 00:30:05,780
every single other servers,

552
00:30:06,680 --> 00:30:09,890
so our paper, we have a whole diagram everything, but.

553
00:30:12,770 --> 00:30:14,900
Yeah, how well does it scale,

554
00:30:14,930 --> 00:30:16,640
as you add more servers.

555
00:30:17,220 --> 00:30:20,460
Yeah, so have the metrics on my other screen,

556
00:30:20,640 --> 00:30:24,630
so it's able to handle,

557
00:30:25,920 --> 00:30:32,100
if we gave it to more or less 3000 requests across five clients and five servers,

558
00:30:32,550 --> 00:30:34,980
it achieve eventual consistency within three seconds,

559
00:30:34,980 --> 00:30:38,820
but this was mainly because it was also on a computer,

560
00:30:38,820 --> 00:30:41,010
that's on local host with RPCs were really fast.

561
00:30:42,460 --> 00:30:45,730
However, that also means that all the computers happening on one machine,

562
00:30:45,730 --> 00:30:49,300
so this can go either way when we put it on real hardware.

563
00:30:54,080 --> 00:30:56,570
Yeah, another thing is that,

564
00:30:57,170 --> 00:31:00,290
if you make too many requests at once,

565
00:31:00,680 --> 00:31:04,460
eventually consistency kind of hard to achieve,

566
00:31:04,520 --> 00:31:10,730
so for example we tried to do 19000 edits at once,

567
00:31:10,760 --> 00:31:13,070
across a hundred clients and five servers

568
00:31:13,400 --> 00:31:16,670
and it took around 32 seconds to reach eventual consistency,

569
00:31:17,480 --> 00:31:21,260
however, if we only did 3000 edits across a hundred clients and five servers,

570
00:31:21,260 --> 00:31:22,670
it only took around three seconds.

571
00:31:25,740 --> 00:31:28,860
When you say you do whatever the 19000 or whatever that was,

572
00:31:28,860 --> 00:31:30,660
is that like sending all those at once,

573
00:31:30,660 --> 00:31:32,520
or is it spread out over time.

574
00:31:33,210 --> 00:31:37,160
Every single one is sent, every single edit sent in parallel, and at least at test.

575
00:31:38,990 --> 00:31:39,440
Interesting.

576
00:31:42,580 --> 00:31:43,750
I had a quick question,

577
00:31:43,750 --> 00:31:45,460
so one of the motivators

578
00:31:45,460 --> 00:31:52,840
that collaborative text editors like Google docs can't support high scale updates concurrent updates,

579
00:31:52,960 --> 00:31:56,590
so why don't you think they use an approach like what you guys are proposing.

580
00:31:59,650 --> 00:32:03,610
I think Google docs uses a similar approach called operational transform,

581
00:32:03,640 --> 00:32:07,150
which I'm not sure, if it can be parallelized as well,

582
00:32:07,600 --> 00:32:08,740
and they probably don't want to,

583
00:32:08,740 --> 00:32:10,630
because they don't want to give us that much server power,

584
00:32:12,320 --> 00:32:15,260
it's probably just like for cost effectiveness,

585
00:32:15,290 --> 00:32:15,890
I'm guessing.

586
00:32:18,950 --> 00:32:21,470
Yeah, I'm just, how [] say,

587
00:32:21,980 --> 00:32:25,100
I don't think they use case a hundred people editing one document,

588
00:32:25,100 --> 00:32:26,360
comes up too often,

589
00:32:26,960 --> 00:32:30,170
at least in the Google Google docs usage,

590
00:32:31,290 --> 00:32:33,930
so it's probably not something you want to spend extra engineering time on.

591
00:32:39,200 --> 00:32:41,420
Great, thank you, cool demo,

592
00:32:41,990 --> 00:32:44,150
so up next, we have the eggscrambler,

593
00:32:44,150 --> 00:32:44,780
if you all ready.

594
00:32:51,910 --> 00:32:54,580
Cool, hello everyone, my name is Arvid

595
00:32:54,580 --> 00:32:56,860
and together with Arman and Wendy,

596
00:32:56,860 --> 00:32:58,360
we have been working on eggscrambler,

597
00:32:58,360 --> 00:33:02,350
which is anonymous broadcasting using community encryption.

598
00:33:02,350 --> 00:33:05,020
So let's start with a motivating problem,

599
00:33:05,470 --> 00:33:07,840
we all know we all love MIT Confessions,

600
00:33:07,840 --> 00:33:11,560
so let's say that one person in this class wants to submit a confession,

601
00:33:11,560 --> 00:33:13,840
telling people how much they love this class,

602
00:33:13,930 --> 00:33:14,890
and so what do they do,

603
00:33:14,890 --> 00:33:17,890
well they submit this convention to a Google form,

604
00:33:18,010 --> 00:33:21,860
and then that goes to the MIT Confessions admin

605
00:33:21,860 --> 00:33:24,380
and then that hopefully goes to the Facebook page,

606
00:33:24,560 --> 00:33:26,360
but now let's consider a scenario,

607
00:33:26,360 --> 00:33:29,420
where the confessions admin is actually an MIT professor,

608
00:33:29,570 --> 00:33:32,300
particularly for another classic course,

609
00:33:32,300 --> 00:33:35,270
so obviously they would not want this confession to be posted,

610
00:33:35,670 --> 00:33:37,560
so they would [censor] it,

611
00:33:37,650 --> 00:33:38,370
because they can,

612
00:33:38,670 --> 00:33:39,900
but it's like even worse,

613
00:33:39,900 --> 00:33:42,810
because they could they could there like an MIT professor,

614
00:33:42,810 --> 00:33:45,900
so they could go to Google look at their network traffic,

615
00:33:45,900 --> 00:33:47,790
they could look at the MIT WiFi network traffic,

616
00:33:47,880 --> 00:33:50,220
so they could figure out who actually sent this,

617
00:33:50,610 --> 00:33:54,330
and then obviously they would like to give that person an f minus in their class

618
00:33:54,600 --> 00:33:55,950
and that's terrible,

619
00:33:55,950 --> 00:33:56,970
so what do we want to do,

620
00:33:56,970 --> 00:34:00,240
well, we want anonymous broadcasting

621
00:34:00,240 --> 00:34:01,980
and that's like a common theme today,

622
00:34:01,980 --> 00:34:04,680
it seems like and so what is anonymous broadcasting,

623
00:34:04,680 --> 00:34:09,360
well, we define it as a protocol to broadcast a message to everyone,

624
00:34:09,450 --> 00:34:12,510
all the users without revealing who the message comes from,

625
00:34:12,790 --> 00:34:14,590
so without further do,

626
00:34:14,590 --> 00:34:18,250
I will let Wendy talk about how we do that.

627
00:34:20,540 --> 00:34:23,720
So yeah, the two main features of our design are that,

628
00:34:23,720 --> 00:34:26,600
it is decentralized and uses cryptography,

629
00:34:26,630 --> 00:34:29,960
in particular, community of encryption is necessary,

630
00:34:29,960 --> 00:34:34,850
because otherwise the description order will reveal the original identities of message senders.

631
00:34:36,880 --> 00:34:40,150
Yeah, so here's the protocol be implemented,

632
00:34:40,210 --> 00:34:42,430
ensure participants join the network

633
00:34:42,580 --> 00:34:44,560
and encrypt and submit their message,

634
00:34:44,560 --> 00:34:48,880
which [winds] up encrypted by all participants exactly once in the encrypt phase,

635
00:34:49,060 --> 00:34:51,100
the scramble phase provides anonymity,

636
00:34:51,100 --> 00:34:52,870
because as highlighted,

637
00:34:53,110 --> 00:34:56,940
because every participant encrypts the messages

638
00:34:56,940 --> 00:34:58,620
and randomize is there order,

639
00:34:59,070 --> 00:35:04,950
finally decryption results in the desired list of messages from all participants with no connection to identity.

640
00:35:08,340 --> 00:35:11,100
So yeah, if all participants are honest,

641
00:35:11,100 --> 00:35:14,070
this protocol successfully results in anonymity,

642
00:35:14,940 --> 00:35:18,570
while it is possible for a malicious user to determine the sender of a message,

643
00:35:18,570 --> 00:35:21,690
we believe that such actions are detectable upon massacre reveals,

644
00:35:21,810 --> 00:35:23,430
if people are interested,

645
00:35:23,430 --> 00:35:25,320
there exist other solutions to this problems,

646
00:35:25,320 --> 00:35:27,270
such as mix-nets and DC-nets,

647
00:35:27,390 --> 00:35:31,050
next Armand will describe the structure of the implementation.

648
00:35:34,800 --> 00:35:38,040
So, first of all, look at this assuming that,

649
00:35:38,840 --> 00:35:41,000
we're looking at the high-level interface for,

650
00:35:41,030 --> 00:35:42,230
someone trying to submit a message

651
00:35:42,230 --> 00:35:43,610
and our interaction with our servers,

652
00:35:43,640 --> 00:35:46,340
so let's say you're the person trying to submit a confession,

653
00:35:46,370 --> 00:35:47,750
you're this application client,

654
00:35:48,170 --> 00:35:51,680
so the first thing you're going to do is send a start message,

655
00:35:51,710 --> 00:35:55,070
so that indicates that you want to start a new broadcast round,

656
00:35:55,820 --> 00:35:58,610
and the service is going to ask all the people

657
00:35:58,610 --> 00:36:01,160
who are participating in the round for a message,

658
00:36:02,050 --> 00:36:03,730
and then it's going to run this protocol

659
00:36:04,090 --> 00:36:07,450
and then it's going to send the results to everyone who participated

660
00:36:07,480 --> 00:36:10,270
or if it's something like confessions going to publish it,

661
00:36:11,140 --> 00:36:13,570
and I feel at this point, it would be anonymized,

662
00:36:14,440 --> 00:36:16,120
but this is a very simple view,

663
00:36:16,120 --> 00:36:21,970
in reality, we're actually building this on top of our raft implementation from the labs,

664
00:36:22,390 --> 00:36:24,610
so the reason we use lab raft is two [],

665
00:36:24,700 --> 00:36:28,210
so one, we implement the protocol as a state machine,

666
00:36:28,980 --> 00:36:34,260
and so raft is among many instances that are running the protocol

667
00:36:34,350 --> 00:36:36,540
and so raft is used to replicate this state

668
00:36:37,020 --> 00:36:38,940
and then two, when we're doing the scramble phase,

669
00:36:38,940 --> 00:36:42,630
we need to make sure that each participant is scrambling in a specific order,

670
00:36:42,630 --> 00:36:45,660
so we actually use raft as a coordination mechanism

671
00:36:45,660 --> 00:36:46,830
to do this test and set,

672
00:36:47,220 --> 00:36:49,350
where they all proceed in a single order,

673
00:36:49,980 --> 00:36:52,440
so similar to the kv store,

674
00:36:52,440 --> 00:36:54,030
we have the state machine server,

675
00:36:54,030 --> 00:36:55,440
that's reading the raft updates

676
00:36:55,440 --> 00:36:59,490
and updating the state machine with whatever they agreed upon state is,

677
00:36:59,970 --> 00:37:02,310
and we also add in the state machine client,

678
00:37:02,310 --> 00:37:04,410
which is reading how the state machine is changing

679
00:37:04,770 --> 00:37:07,410
and then this client is what's actually taking the actions

680
00:37:07,410 --> 00:37:10,290
to progress the protocol in some way,

681
00:37:11,640 --> 00:37:14,430
and then again just as in kv store,

682
00:37:14,980 --> 00:37:17,680
in order to progress the protocol,

683
00:37:17,680 --> 00:37:21,100
this client is going to contact the leader server,

684
00:37:21,370 --> 00:37:24,280
with a request to the state machine updates,

685
00:37:24,900 --> 00:37:28,410
and then we also implement the raft configuration change RPCs,

686
00:37:28,870 --> 00:37:32,710
so servers can add or remove themselves to the configuration,

687
00:37:32,770 --> 00:37:33,700
and this means that,

688
00:37:34,090 --> 00:37:37,180
new people can join subsequent broadcast rounds

689
00:37:37,570 --> 00:37:38,920
and then also if there's a failure,

690
00:37:38,920 --> 00:37:40,870
we can cut someone out of the configuration,

691
00:37:40,900 --> 00:37:43,450
so that the next broadcast round can proceed,

692
00:37:43,780 --> 00:37:49,580
because our protocol requires that everyone is actually active in order for the round to complete,

693
00:37:51,380 --> 00:37:52,940
and then finally we have the application,

694
00:37:52,940 --> 00:37:56,840
where the client is like something like confessions running on your phone,

695
00:37:57,200 --> 00:37:59,750
will send these broadcast messages to the clients

696
00:38:00,080 --> 00:38:02,960
and then it will receive the update,

697
00:38:02,960 --> 00:38:04,850
once the protocol has completed.

698
00:38:05,950 --> 00:38:09,190
So we're going to show a quick demo recorded,

699
00:38:09,220 --> 00:38:11,950
just let me know if you can't hear this,

700
00:38:16,730 --> 00:38:17,810
So now onto the demo,

701
00:38:17,810 --> 00:38:20,750
we start working at Wendy's computer right now

702
00:38:21,080 --> 00:38:23,870
and she's going to start the application,

703
00:38:23,870 --> 00:38:26,990
so she's basically gonna start an instance on her computer and try

704
00:38:27,080 --> 00:38:32,860
and she's currently sending the link for her configuration into the zoom chat

705
00:38:32,890 --> 00:38:33,850
as we can see here,

706
00:38:35,210 --> 00:38:37,670
so now we're gonna go to Arvid's computer,

707
00:38:37,670 --> 00:38:41,720
he's gonna take that link and enjoy the same configuration as Wendy,

708
00:38:43,110 --> 00:38:44,130
so he does that,

709
00:38:44,520 --> 00:38:47,310
and now he's going to start the round,

710
00:38:48,540 --> 00:38:50,700
will can send their messages,

711
00:38:53,220 --> 00:38:54,990
and we have the round succeeded,

712
00:38:55,470 --> 00:38:58,740
so now are we going to show that we can seamlessly add a new client,

713
00:38:58,740 --> 00:39:00,780
so this is both someone is going to send a message

714
00:39:00,810 --> 00:39:02,970
and an entirely new raft instance,

715
00:39:03,390 --> 00:39:04,170
so we see here,

716
00:39:04,170 --> 00:39:08,510
this new instance quickly catches up on the left there,

717
00:39:08,660 --> 00:39:12,290
and now all three of them are going to send a message for this next round,

718
00:39:15,980 --> 00:39:18,230
and we get all three messages,

719
00:39:18,260 --> 00:39:21,500
so we demonstrated that we can broadcast over the network

720
00:39:21,950 --> 00:39:25,400
and also add in clients at will.

721
00:39:30,260 --> 00:39:33,670
Yes, that's a demo of eggscrambler,

722
00:39:33,760 --> 00:39:36,010
does anyone have any questions?

723
00:39:39,610 --> 00:39:41,140
This is this is a really fast one,

724
00:39:41,170 --> 00:39:44,620
this first of all, this is a really cool system,

725
00:39:44,920 --> 00:39:52,780
well, so the application is separate from the client raft state machine, correct.

726
00:39:53,660 --> 00:39:54,140
Yes.

727
00:39:54,260 --> 00:39:56,150
And it's like two different devices,

728
00:39:56,740 --> 00:39:58,600
so I guess my question sort of is,

729
00:39:58,750 --> 00:40:01,570
like in the overall picture of how you would deploy this thing,

730
00:40:01,570 --> 00:40:02,410
why is that,

731
00:40:02,710 --> 00:40:07,480
I mean I imagine you could run a like a raft state machine with a bunch of phones,

732
00:40:07,940 --> 00:40:10,670
it might be poor availability,

733
00:40:10,670 --> 00:40:13,220
but you've implemented the configuration changes,

734
00:40:13,220 --> 00:40:14,420
so in principle could that,

735
00:40:15,290 --> 00:40:17,390
really, I'm just trying to figure out what the role of the client is,

736
00:40:17,390 --> 00:40:19,580
in this whole thing, the server is a cluster of servers that makes sense,

737
00:40:19,580 --> 00:40:21,320
the applications that make sense, but.

738
00:40:22,270 --> 00:40:23,650
So basically the client,

739
00:40:23,680 --> 00:40:28,660
it's basically just like a nice way to like write the implementation,

740
00:40:28,930 --> 00:40:30,460
so the server and the client,

741
00:40:30,460 --> 00:40:32,110
you can basically think of that as a server,

742
00:40:32,260 --> 00:40:35,080
is just like in our implementation,

743
00:40:35,080 --> 00:40:38,500
it made sense to create this abstraction, yeah.

744
00:40:38,940 --> 00:40:40,110
Okay, thanks.

745
00:40:43,760 --> 00:40:45,500
In order for the round to progress,

746
00:40:45,500 --> 00:40:50,090
all peers must receive the message from all the other peers where n-1 peers

747
00:40:50,300 --> 00:40:52,940
and so if you have some very weird partition,

748
00:40:52,940 --> 00:40:56,660
where like some subset is able to talk to the rest of the subset,

749
00:40:56,720 --> 00:40:58,970
how would that be resolved here?

750
00:41:01,640 --> 00:41:06,890
Yeah, so if you, if you like partition into like smaller than,

751
00:41:06,890 --> 00:41:09,440
like if everyone is minorities,

752
00:41:09,470 --> 00:41:11,900
then then it would just be stuck forever,

753
00:41:12,080 --> 00:41:14,090
until you find a majority

754
00:41:14,090 --> 00:41:15,650
and then once you have a majority,

755
00:41:15,650 --> 00:41:18,770
you can like they, they automatically submit,

756
00:41:18,770 --> 00:41:21,410
like remove server RPCs,

757
00:41:21,500 --> 00:41:24,110
whenever they don't get a response from someone,

758
00:41:24,110 --> 00:41:29,870
so then the cluster like shrinks until like the non responding nodes disappeared.

759
00:41:34,130 --> 00:41:36,950
I noticed in the demo example you gave,

760
00:41:37,010 --> 00:41:40,430
you had one person who wrote two of the messages

761
00:41:40,490 --> 00:41:44,810
and so, someone else who wrote the third message

762
00:41:44,840 --> 00:41:47,210
and that meant that you could that the person who wrote two messages

763
00:41:47,210 --> 00:41:50,300
could always tell what that third person had shared,

764
00:41:50,360 --> 00:41:53,000
doesn't that sort of defeat the purpose of your protocol.

765
00:41:55,040 --> 00:41:58,640
Yeah, yeah, so the reason why it was two messages,

766
00:41:58,640 --> 00:42:04,190
because we're running, we're running both of those instances on the same computer,

767
00:42:04,750 --> 00:42:07,930
that is, that is an attack though where,

768
00:42:08,050 --> 00:42:09,130
you know you have it,

769
00:42:09,160 --> 00:42:14,320
you start around where one person is controlled by everyone,

770
00:42:14,350 --> 00:42:16,240
well like five of the servers

771
00:42:16,240 --> 00:42:18,460
and five of them submitting messages are controlled by everyone,

772
00:42:18,460 --> 00:42:20,260
and you know who the last person is,

773
00:42:20,950 --> 00:42:29,180
but, I think so we currently consider that like a byzantine or non byzantine failures,

774
00:42:29,180 --> 00:42:30,920
so the clients are acting honestly

775
00:42:30,920 --> 00:42:34,850
and they're not trying to gain an unfair amount of advantage in the system

776
00:42:35,120 --> 00:42:38,720
and the attacker or someone who's listening to all the network traffic.

777
00:42:39,950 --> 00:42:43,910
Okay, so you're not worried about people involved in the system undermining it,

778
00:42:44,030 --> 00:42:49,070
you're only worried about people who are like peeking in on the system and driving it.

779
00:42:50,610 --> 00:42:51,210
Yeah.

780
00:42:51,210 --> 00:42:51,990
Or like kind of,

781
00:42:51,990 --> 00:42:55,140
so I think we're like decently confident that,

782
00:42:55,140 --> 00:42:58,620
like a if if they're byzantine participants,

783
00:42:58,620 --> 00:43:01,110
then, the protocol will fail

784
00:43:01,110 --> 00:43:03,420
and it will fail like explicitly,

785
00:43:03,420 --> 00:43:04,860
like and we,

786
00:43:04,860 --> 00:43:09,660
but it's like there are so many different absolutely not exactly certain that that is the case,

787
00:43:09,870 --> 00:43:10,770
but we think so,

788
00:43:10,800 --> 00:43:12,090
but it's like it's.

789
00:43:12,090 --> 00:43:14,550
So for the attack that got discussed then,

790
00:43:14,580 --> 00:43:17,190
where someone takes control of the cluster

791
00:43:17,220 --> 00:43:20,160
and but it takes over all the

792
00:43:20,160 --> 00:43:24,300
and essentially is all of the clients, except for one of them,

793
00:43:24,390 --> 00:43:25,200
and if you're a leader,

794
00:43:25,200 --> 00:43:28,830
you can control, presumably control who gets added or removed from the groups,

795
00:43:28,830 --> 00:43:30,360
you could make that happen,

796
00:43:31,660 --> 00:43:34,870
would that be detected by your current system.

797
00:43:35,490 --> 00:43:38,370
No, no, not not when it's like n-1,

798
00:43:38,460 --> 00:43:40,560
so we need at least two,

799
00:43:40,590 --> 00:43:43,260
if we have two honest people,

800
00:43:43,500 --> 00:43:44,940
then that would be enough,

801
00:43:44,970 --> 00:43:48,570
because they would be like if they were disconnected from each other,

802
00:43:48,570 --> 00:43:49,830
they could know that.

803
00:43:53,810 --> 00:43:54,740
Alright cool, thank you.

804
00:43:57,960 --> 00:43:59,850
Overtime, thanks very cool,

805
00:43:59,880 --> 00:44:02,280
maybe a MIT concession will reach out,

806
00:44:02,940 --> 00:44:08,290
let's hear from the fault tolerance project,

807
00:44:09,550 --> 00:44:11,350
maybe that's a bad way of summarizing.

808
00:44:12,370 --> 00:44:12,880
No worries.

809
00:44:13,840 --> 00:44:15,310
Yeah, everyone, I'm Ariel,

810
00:44:15,340 --> 00:44:18,220
I've been working on this project with Frans for a bit now,

811
00:44:18,220 --> 00:44:20,140
so let's see I can share my screen.

812
00:44:22,050 --> 00:44:25,590
And so, yeah as you can see here,

813
00:44:25,590 --> 00:44:29,670
a project involves giving fault tolerance for free at the 9P interface

814
00:44:29,670 --> 00:44:33,990
and also break down what this means exactly and extra slides,

815
00:44:33,990 --> 00:44:36,270
we start by looking at service computation,

816
00:44:36,270 --> 00:44:44,730
so if you've heard of some things like AWS lambda or GCP also Google cloud functions,

817
00:44:44,880 --> 00:44:47,700
essentially what this does is,

818
00:44:47,700 --> 00:44:50,370
instead of having to set a bunch of VMs

819
00:44:50,370 --> 00:44:53,460
and scale them with requests to your customer load everything,

820
00:44:53,460 --> 00:44:56,100
you can just write a small stateless function,

821
00:44:56,430 --> 00:44:59,640
do you oppose your application through a bunch of small stateless functions,

822
00:44:59,640 --> 00:45:01,680
which are triggered over http requests,

823
00:45:01,920 --> 00:45:08,250
and then AWS or Google cloud or who are your service providers will handle the scaling

824
00:45:08,250 --> 00:45:10,410
and all the infrastructure management for you,

825
00:45:10,880 --> 00:45:11,750
and this is nice,

826
00:45:11,750 --> 00:45:15,680
because in theory operators can balance resources more effectively,

827
00:45:15,680 --> 00:45:17,960
they can get high utilization,

828
00:45:18,380 --> 00:45:20,960
and in theory it's easier for developers as well,

829
00:45:20,960 --> 00:45:21,860
all you have to do is,

830
00:45:22,490 --> 00:45:25,880
is you don't have to worry about all this kind of infrastructure management

831
00:45:25,880 --> 00:45:27,830
and sort of having enough headroom

832
00:45:27,920 --> 00:45:29,630
and your number of VMs

833
00:45:29,630 --> 00:45:32,930
and everything to handle spikes in customer load and things like this.

834
00:45:34,440 --> 00:45:38,100
However, the reality is not as nice as the theory,

835
00:45:38,280 --> 00:45:42,000
which is in the end,

836
00:45:42,350 --> 00:45:44,150
in current implementations,

837
00:45:44,210 --> 00:45:48,650
actually building these complex applications is quite difficult,

838
00:45:49,450 --> 00:45:53,050
and decomposing them into these state functions quite difficult,

839
00:45:53,050 --> 00:45:59,500
in part because these lambdas these stateless functions lack rich communication primitives,

840
00:46:00,220 --> 00:46:03,880
which would be important support to build complex applications,

841
00:46:03,910 --> 00:46:07,120
so they can't directly communicate or through to each other,

842
00:46:07,720 --> 00:46:10,750
therefore systems that try to utilize serverless functions,

843
00:46:10,780 --> 00:46:15,520
have had to sort of build their own custom communication solutions,

844
00:46:15,550 --> 00:46:18,310
like maybe setting up the ends on the side,

845
00:46:18,310 --> 00:46:19,780
that they round everyone talk through

846
00:46:19,780 --> 00:46:22,840
or talking through a storage systems, like S3

847
00:46:22,840 --> 00:46:27,700
and this makes them efficient and difficult for developers to actually put together in a sensible way,

848
00:46:29,290 --> 00:46:33,580
all this complexity causes data centers to actually manage resources quite poor,

849
00:46:33,610 --> 00:46:39,160
and and the problem is that,

850
00:46:39,430 --> 00:46:45,340
we still have all these data center resources segmented into a bunch of local name spaces,

851
00:46:45,340 --> 00:46:49,480
which are destroyed and and you can't share resources across them.

852
00:46:50,790 --> 00:46:59,160
So what we were looking at was reviving some work from past decades called Plan9

853
00:46:59,250 --> 00:47:04,260
and Plan9 are essentially provides plan for unifying data center resources

854
00:47:04,320 --> 00:47:06,390
and providing a single system image,

855
00:47:06,920 --> 00:47:09,140
what this means essentially when you write your application,

856
00:47:09,170 --> 00:47:11,780
you don't have to worry about what runs on what machines,

857
00:47:11,780 --> 00:47:14,900
about provisioning VMs or anything like that,

858
00:47:15,170 --> 00:47:20,600
or making sure that certain parts of the application on certain VMs,

859
00:47:20,600 --> 00:47:25,680
it all looks like one giants are name space to your application,

860
00:47:25,800 --> 00:47:29,910
so your application knows nothing about what hardware it's actually running on

861
00:47:29,910 --> 00:47:33,210
or where other services are in the data centre,

862
00:47:33,450 --> 00:47:40,380
the ways achieved is applications services communicate through a global hierarchical namespace,

863
00:47:40,970 --> 00:47:46,820
services and resources to expose uniform file system like interface,

864
00:47:47,090 --> 00:47:52,190
you don't have to worry about implementing and calling customer RPCs,

865
00:47:52,190 --> 00:47:55,590
and so for example and this, in this tree,

866
00:47:55,590 --> 00:47:56,670
you can see here,

867
00:47:56,700 --> 00:48:01,560
we have the top level root level name space,

868
00:48:01,560 --> 00:48:04,620
and then under that we have three services registered,

869
00:48:04,620 --> 00:48:06,180
we have an S3 service,

870
00:48:06,180 --> 00:48:11,010
which you can use to connect your [landers] to S3,

871
00:48:11,010 --> 00:48:15,440
you can access keys, S3 keys and values through that,

872
00:48:15,440 --> 00:48:18,800
we have an in-memory file system and sched

873
00:48:19,010 --> 00:48:20,980
and so quickly.

874
00:48:21,790 --> 00:48:28,290
Yeah, so this is what the what like a typical 9P name space would look like,

875
00:48:29,550 --> 00:48:37,410
we have done some work to implement example of of this 9P architecture,

876
00:48:37,590 --> 00:48:40,620
we have a, zookeeper like configuration service,

877
00:48:40,620 --> 00:48:42,510
which hosts the top level name space,

878
00:48:42,630 --> 00:48:47,520
we have a scheduler and a bunch of these np servers, 9P servers,

879
00:48:47,880 --> 00:48:50,880
what these do is they expose resources and functionality

880
00:48:50,880 --> 00:48:53,280
such as the if we go back here,

881
00:48:53,370 --> 00:48:55,140
this three part of name space,

882
00:48:55,140 --> 00:49:00,120
you can write a service which connected S3 buckets

883
00:49:00,120 --> 00:49:03,780
and and expose it to 9P and this would be a 9P server.

884
00:49:05,250 --> 00:49:12,670
And the RPCs or the operations that used to interact with these different services

885
00:49:12,670 --> 00:49:15,310
are all the 9P operations,

886
00:49:15,310 --> 00:49:19,060
so it's a very small, well defined set of operations,

887
00:49:19,060 --> 00:49:21,160
that, you can see a few of them here,

888
00:49:21,160 --> 00:49:27,820
such as read and write moves that small number of canonical file system operations.

889
00:49:29,470 --> 00:49:31,510
I so quickly done, what this looks like,

890
00:49:31,630 --> 00:49:34,930
so for example here,

891
00:49:34,930 --> 00:49:36,850
I can start up correct infrastructure.

892
00:49:40,690 --> 00:49:42,130
So I can look into,

893
00:49:42,160 --> 00:49:45,010
you can map this name space right into Linux,

894
00:49:45,670 --> 00:49:49,060
so I can see some services

895
00:49:49,060 --> 00:49:53,140
and different name spaces that we've mounted here,

896
00:49:53,140 --> 00:49:55,150
for example if I look at S3,

897
00:49:58,610 --> 00:50:03,410
this is actually connecting to S3 bucket that we have,

898
00:50:03,980 --> 00:50:09,440
I'm sorry do you actually see the keys in the S3 bucket,

899
00:50:09,440 --> 00:50:11,210
just as files in your file system,

900
00:50:11,510 --> 00:50:14,600
and this is S3 bucket over here,

901
00:50:14,960 --> 00:50:17,720
so if I want to create, write a new key,

902
00:50:17,720 --> 00:50:24,620
for example, all I need to do is say cat, echo to,

903
00:50:30,090 --> 00:50:35,250
so instead of having to use AWS custom library,

904
00:50:35,250 --> 00:50:37,680
in order to so write S3,

905
00:50:37,680 --> 00:50:40,620
I can just write a file system module,

906
00:50:40,620 --> 00:50:43,050
file system operation to create a file and write to it

907
00:50:43,350 --> 00:50:48,140
and then S3, this actually should show up as a new key now,

908
00:50:49,600 --> 00:50:52,030
so we have test456, which we just wrote.

909
00:50:56,630 --> 00:50:59,810
So, um, what does fault tolerance part coming here,

910
00:50:59,870 --> 00:51:01,490
well as I mentioned before,

911
00:51:01,490 --> 00:51:05,510
all these 9P services have a very well defined uniform interface,

912
00:51:05,540 --> 00:51:07,370
there are no customer RPCs,

913
00:51:07,370 --> 00:51:10,400
and this gives us a very unique opportunity,

914
00:51:10,400 --> 00:51:16,460
which is we can slice at this 9P interface to replicate unmodified services,

915
00:51:16,490 --> 00:51:23,540
so if if I am able to replicate all these operations to different server instances of servers,

916
00:51:23,540 --> 00:51:29,210
then servers gets replicated freely without having to modify it at all,

917
00:51:29,270 --> 00:51:30,080
so this is what we did,

918
00:51:30,080 --> 00:51:31,040
when I worked on,

919
00:51:31,040 --> 00:51:39,460
I implemented chain replication based fault tolerance scheme,

920
00:51:39,880 --> 00:51:42,490
I use named is like a configuration,

921
00:51:42,520 --> 00:51:47,110
configuration service, you can think of this as like a zookeeper

922
00:51:47,140 --> 00:51:51,430
and I replicated 2 different services without any modification

923
00:51:51,430 --> 00:51:59,410
and in-memory file system and a service which exposes durable storage from a local machines

924
00:51:59,530 --> 00:52:02,560
and so I'll quickly show them that.

925
00:52:03,460 --> 00:52:05,350
So let's stop this,

926
00:52:05,380 --> 00:52:05,890
let's see.

927
00:52:08,180 --> 00:52:12,980
So, for example I can,

928
00:52:20,000 --> 00:52:21,770
so I can start up a bunch of replicates,

929
00:52:32,840 --> 00:52:36,710
so if I look into the 9P name space module in Linux,

930
00:52:36,980 --> 00:52:38,780
you see this memfs-replica,

931
00:52:39,200 --> 00:52:41,600
here's where these replicas register themselves,

932
00:52:44,590 --> 00:52:47,590
so we can see that, we have five of these replicas up,

933
00:52:47,620 --> 00:52:49,600
I can write to one of them.

934
00:52:53,640 --> 00:52:54,210
Oops.

935
00:52:58,110 --> 00:53:00,240
Okay, so can I write to one of the replicas

936
00:53:00,450 --> 00:53:02,190
and then I can read from another one group,

937
00:53:02,190 --> 00:53:07,480
because we should get the same result outside wrote,

938
00:53:07,920 --> 00:53:11,520
a small string to a file in the first replica,

939
00:53:11,550 --> 00:53:12,990
you can read from the last replica,

940
00:53:13,020 --> 00:53:14,220
should get the same thing back,

941
00:53:14,610 --> 00:53:19,170
can even crash replica, because,

942
00:53:22,590 --> 00:53:26,070
so let's kill one of these guys,

943
00:53:32,960 --> 00:53:34,250
we can see this,

944
00:53:37,650 --> 00:53:40,440
see there are only four replicas left now in the name space,

945
00:53:40,770 --> 00:53:45,570
then write a different string to this file,

946
00:53:48,910 --> 00:53:51,460
and, I should,

947
00:53:52,300 --> 00:53:57,490
yeah, so, so now, despite a failure,

948
00:53:57,520 --> 00:53:59,320
the service automatically reconfigured,

949
00:53:59,800 --> 00:54:05,620
and the in-memory file systems services based off,

950
00:54:05,620 --> 00:54:06,880
did not have to be modified at all,

951
00:54:06,880 --> 00:54:09,430
you can just plug in any new service you want to,

952
00:54:09,430 --> 00:54:11,290
that implements this 9P interface

953
00:54:11,290 --> 00:54:13,120
and it should work out of the box.

954
00:54:15,100 --> 00:54:17,890
So, I think that's,

955
00:54:18,460 --> 00:54:22,150
I think that sort of concludes my presentation

956
00:54:22,150 --> 00:54:23,950
and I'd be happy to take any more questions,

957
00:54:24,430 --> 00:54:25,210
you guys might have.

958
00:54:33,330 --> 00:54:35,730
I regret not wearing my Plan9 shirt this time,

959
00:54:37,530 --> 00:54:38,190
I should have,

960
00:54:38,250 --> 00:54:43,050
I'm curious though, I'm working on something that was sort of similar earlier this semester,

961
00:54:43,050 --> 00:54:45,180
except there was no replication,

962
00:54:45,180 --> 00:54:48,360
it was just 9P services under Linux box,

963
00:54:48,420 --> 00:54:51,900
why this is sort of an offhand question, why change replication,

964
00:54:52,580 --> 00:54:55,070
I imagine there are other replication schemes,

965
00:54:55,490 --> 00:54:58,100
that are available for this sort of thing,

966
00:54:58,100 --> 00:54:59,630
and in theory because this is 9P,

967
00:54:59,630 --> 00:55:02,330
you could probably bind all these replicas over the same space,

968
00:55:02,330 --> 00:55:03,440
they look like, it's a machine,

969
00:55:03,440 --> 00:55:05,120
but why chain replication.

970
00:55:06,480 --> 00:55:08,040
Yeah, yeah, good question,

971
00:55:08,040 --> 00:55:12,120
you could in theory do any any replication scheme you'd like, under the hood,

972
00:55:12,210 --> 00:55:18,560
I did chain application, because it seemed like a simple starting point, I guess,

973
00:55:18,560 --> 00:55:21,380
I could also throw this on top of a raft interpretation,

974
00:55:21,380 --> 00:55:23,120
like the entry class or anything like that,

975
00:55:24,530 --> 00:55:26,120
it's something fundamental about the choice.

976
00:55:31,840 --> 00:55:36,550
Does your system support adding additional replicas after it started.

977
00:55:36,880 --> 00:55:38,140
Yeah, great question,

978
00:55:38,170 --> 00:55:43,300
so you already know that's a work in progress,

979
00:55:44,060 --> 00:55:50,030
but yeah currently we don't support adding additional replicas out of box.

980
00:55:56,500 --> 00:55:57,640
This is an open question,

981
00:55:57,640 --> 00:55:59,530
it's sort of like you may not know this,

982
00:55:59,530 --> 00:56:01,090
this wasn't the express intent of the purpose,

983
00:56:01,090 --> 00:56:04,060
but one of the things that's really cool about Plan9 and 9P is that,

984
00:56:04,270 --> 00:56:06,640
you treat network connections like their files as well,

985
00:56:06,850 --> 00:56:11,680
so you're like you have like memfs, you have from what I saw a scheduler queue was implemented,

986
00:56:11,680 --> 00:56:12,550
just set of files,

987
00:56:14,330 --> 00:56:16,280
do you and this is an open question,

988
00:56:16,280 --> 00:56:19,640
because the question of whether network stuff over file interfaces,

989
00:56:19,640 --> 00:56:21,890
scalable is difficult to answer,

990
00:56:21,920 --> 00:56:25,130
but do you also implement this in sort of that way

991
00:56:25,130 --> 00:56:27,770
or do you use more traditional policy interfaces to,

992
00:56:27,920 --> 00:56:32,300
when the clients use this thing they presume they use traditional sockets and things to communicate.

993
00:56:33,400 --> 00:56:35,110
Yeah, yeah, good question,

994
00:56:35,110 --> 00:56:42,910
so yeah, everything, so all the clients and services communicate over TCP at the moment,

995
00:56:44,250 --> 00:56:46,440
and yeah it is a good question

996
00:56:46,440 --> 00:56:48,930
as to whether the 9P spaces actually performed enough

997
00:56:48,930 --> 00:56:50,100
for what we're gonna do with it,

998
00:56:50,190 --> 00:56:52,200
as far as we can see now,

999
00:56:52,230 --> 00:56:53,970
there's not,

1000
00:56:55,050 --> 00:56:57,660
so we've done some performance benchmarking

1001
00:56:57,660 --> 00:57:04,020
to see how well like we've written a scheduler over 9P,

1002
00:57:04,640 --> 00:57:08,150
and we've done some performance benchmarking to see how it performs

1003
00:57:08,150 --> 00:57:10,280
and it seems to not add a ton of overhead at the moment,

1004
00:57:10,280 --> 00:57:15,250
but, alright, we can imagine those trade-offs changing for different types of services

1005
00:57:15,430 --> 00:57:18,850
and as the scheduler becomes more or less oversubscribed.

1006
00:57:20,750 --> 00:57:21,950
I guess, I guess to clarify,

1007
00:57:21,950 --> 00:57:24,410
so sorry I'm taking up taking up a lot of time,

1008
00:57:24,410 --> 00:57:27,110
but just briefly the whole 9P,

1009
00:57:27,110 --> 00:57:29,120
if you if you were able to

1010
00:57:29,120 --> 00:57:30,350
and this is again very open,

1011
00:57:30,350 --> 00:57:34,220
but if you were able to treat network connections files, you have replicas, right,

1012
00:57:34,250 --> 00:57:36,690
you could have something for each card,

1013
00:57:36,690 --> 00:57:38,850
then shared network traffic over those network cards,

1014
00:57:38,850 --> 00:57:40,710
which I guess this is serverless anyway,

1015
00:57:40,710 --> 00:57:42,720
so it probably doesn't make that much of a difference,

1016
00:57:42,840 --> 00:57:43,500
yeah, forget it,

1017
00:57:45,900 --> 00:57:48,480
because you already have the handles for you.

1018
00:57:50,440 --> 00:57:51,340
Thank you.

1019
00:57:52,150 --> 00:57:52,870
Thank you.

1020
00:57:54,130 --> 00:57:57,940
Alright, so let's hear about some verification stuff, if you're ready.

1021
00:58:02,730 --> 00:58:03,870
Can you guys hear me, okay.

1022
00:58:04,490 --> 00:58:04,940
Yes.

1023
00:58:05,590 --> 00:58:06,820
Alright, excellent,

1024
00:58:07,090 --> 00:58:10,090
so indeed, I'm gonna talk to you about my project,

1025
00:58:10,090 --> 00:58:13,540
which is focused on modular verification for distributed systems,

1026
00:58:13,870 --> 00:58:17,650
so let's start by answering the obvious question,

1027
00:58:17,650 --> 00:58:20,440
which is why bother with any of this stuff,

1028
00:58:20,560 --> 00:58:24,070
and I think anyone that's worked in the lab for 6.824

1029
00:58:24,070 --> 00:58:25,420
must have discovered at some point,

1030
00:58:25,420 --> 00:58:28,660
that getting this resistance right is hard,

1031
00:58:28,660 --> 00:58:32,380
there's a lot of non determinism caused by concurrency and network failure,

1032
00:58:32,470 --> 00:58:35,770
and that makes it very difficult to exhaust the test

1033
00:58:35,770 --> 00:58:37,630
to make sure there's no corner case bugs,

1034
00:58:38,680 --> 00:58:44,140
verification is an alternative to, alternative approach testing to try and get correctness

1035
00:58:44,230 --> 00:58:47,200
and in principle, it can entirely rule out bugs,

1036
00:58:47,440 --> 00:58:50,710
with verification you've basically mathematically modeled your system

1037
00:58:50,890 --> 00:58:53,290
and improve some theorem about that model,

1038
00:58:53,910 --> 00:58:57,600
and you know one of the downsides of verification is that,

1039
00:58:57,600 --> 00:59:01,170
it's quite a lot of work during these formal proofs is

1040
00:59:01,170 --> 00:59:02,310
by no means easy

1041
00:59:02,610 --> 00:59:07,320
and even if it was easy, verification still wouldn't be a perfect silver bullet,

1042
00:59:07,590 --> 00:59:11,400
for one verification, you have to make sure that you get your specification right,

1043
00:59:11,640 --> 00:59:14,700
if the mathematical theorem you're proving by your system

1044
00:59:14,760 --> 00:59:17,610
doesn't actually say what you really wanted to say,

1045
00:59:17,640 --> 00:59:19,290
then what you've proved is useless

1046
00:59:19,380 --> 00:59:24,600
and relatedly, you have to make sure that the model that you have to resist them is also complete,

1047
00:59:24,930 --> 00:59:29,460
if you fail to model some execution that can happen in reality,

1048
00:59:29,460 --> 00:59:33,870
but you don't consider, then your theorem would apply to the real world.

1049
00:59:35,190 --> 00:59:39,210
And some of you that are familiar with some distributed verification work,

1050
00:59:39,210 --> 00:59:42,150
might say well don't we already know how to do this,

1051
00:59:42,150 --> 00:59:44,100
indeed distributed systems have always been hard

1052
00:59:44,100 --> 00:59:50,670
and people have recently worked on a project to try to verify actual implementations of distributed systems,

1053
00:59:51,000 --> 00:59:53,670
so some of these projects include IronFleet and Verdi,

1054
00:59:54,210 --> 00:59:58,380
however, these projects didn't focus much on modularity

1055
00:59:58,710 --> 01:00:02,520
or trying to prove reusable specifications for components of systems,

1056
01:00:02,580 --> 01:00:04,680
trying to build more complicated systems out of them,

1057
01:00:05,400 --> 01:00:09,180
and I'd argue that that's the way that distributed systems are actually built,

1058
01:00:09,300 --> 01:00:11,850
the way you build a distributed system is by

1059
01:00:11,850 --> 01:00:14,070
oftentimes using building blocks,

1060
01:00:14,190 --> 01:00:17,850
like kv values services and lock services or or zookeeper

1061
01:00:17,880 --> 01:00:22,080
and putting them together with some you know added code and novel functionality

1062
01:00:22,170 --> 01:00:24,570
to build your more interesting and more useful system,

1063
01:00:25,840 --> 01:00:28,180
and are sort of thesis if you will,

1064
01:00:28,300 --> 01:00:33,160
is that verification can and should exploit this compositionality,

1065
01:00:33,190 --> 01:00:35,110
as one sort of targeted goal,

1066
01:00:35,110 --> 01:00:38,350
we aim to prove specifications for client systems,

1067
01:00:38,650 --> 01:00:44,530
firework like IronFleet Verdi simply reason about what the behavior of the actual server side [] looks like,

1068
01:00:44,680 --> 01:00:49,210
and don't explicitly model or prove anything about what the client programs actually do

1069
01:00:49,270 --> 01:00:51,220
and oftentimes there's a bit of logic on the client,

1070
01:00:51,400 --> 01:00:53,380
that's crucial for getting correctness,

1071
01:00:53,920 --> 01:00:58,660
and the approach we use is to use advances in concurrent separation logic,

1072
01:00:58,960 --> 01:01:02,140
which is a compositional means of reasoning about concurrent programs,

1073
01:01:02,140 --> 01:01:05,920
that's lately become popular for popular

1074
01:01:05,920 --> 01:01:08,740
and demonstrate to be successful at raising about real code.

1075
01:01:09,800 --> 01:01:12,230
So the first example that we worked on,

1076
01:01:12,230 --> 01:01:15,080
was verifying a sharded value system,

1077
01:01:15,080 --> 01:01:18,770
the keys in this are a statically split up into shards

1078
01:01:19,040 --> 01:01:22,490
and shards themselves can be moved between the shard servers,

1079
01:01:22,700 --> 01:01:25,670
so it's very similar to lab 4 of of 6.824,

1080
01:01:25,730 --> 01:01:27,650
except that it's not replicated,

1081
01:01:27,650 --> 01:01:29,360
so there's no there's no raft in this,

1082
01:01:29,480 --> 01:01:31,130
and, it's purely in memory.

1083
01:01:31,520 --> 01:01:36,140
Besides that, our system also has shared servers and a coordinator server

1084
01:01:36,140 --> 01:01:38,960
and the coordinator is the one that tells other shard servers

1085
01:01:38,960 --> 01:01:40,700
to move shard between themselves

1086
01:01:40,700 --> 01:01:41,960
as you nodes want to join

1087
01:01:41,960 --> 01:01:43,490
or as you need to rebalance,

1088
01:01:44,360 --> 01:01:46,970
the top of a library that we provide

1089
01:01:46,970 --> 01:01:51,110
and that we want to prove specification for is you know we call it a KVClerk,

1090
01:01:51,110 --> 01:01:54,080
which is a client object that one can use

1091
01:01:54,080 --> 01:01:57,980
and we call these three functions on to actually interact with the server,

1092
01:01:57,980 --> 01:02:00,350
so there's a Put, you you say what value to Put in,

1093
01:02:00,350 --> 01:02:03,080
the key there's a Get which will return the current value in the key,

1094
01:02:03,320 --> 01:02:04,610
and then there's a conditionalPut,

1095
01:02:04,640 --> 01:02:09,140
which will only put the new value, if the old value is the expected one,

1096
01:02:10,020 --> 01:02:15,270
and we aim to basically implement a implement a linearizable key value service

1097
01:02:15,270 --> 01:02:18,300
and approve a classification that shows linearizable

1098
01:02:18,750 --> 01:02:22,320
and the way you do this in the separation logic style

1099
01:02:22,320 --> 01:02:25,320
is basically by writing a specification that look a lot like this,

1100
01:02:25,680 --> 01:02:30,870
so this basically says that if the object ck is a key value clerk,

1101
01:02:31,140 --> 01:02:35,070
then you have specifications for the put and get functions,

1102
01:02:35,310 --> 01:02:39,420
that say for example, if you start running the put function,

1103
01:02:39,510 --> 01:02:43,860
with the precondition that key k has value w,

1104
01:02:43,980 --> 01:02:44,790
then by the end of it,

1105
01:02:44,790 --> 01:02:48,030
you'll know that key k get as value v,

1106
01:02:48,720 --> 01:02:51,720
similarly, if you do a get

1107
01:02:51,720 --> 01:02:54,540
and you know that a key k has value v at the beginning,

1108
01:02:54,540 --> 01:02:56,250
then that's the thing that's going to be returned

1109
01:02:56,460 --> 01:02:58,650
and you're still going to know that's the value of the key,

1110
01:02:59,160 --> 01:03:01,380
although the specifications look pretty simple

1111
01:03:01,380 --> 01:03:04,200
and you might think, of course the key value service does

1112
01:03:04,350 --> 01:03:09,390
and that's sort of the point that these top level client specifications are as simple as they can be

1113
01:03:09,390 --> 01:03:12,360
and and basically hide all the details of the fact,

1114
01:03:12,360 --> 01:03:14,130
that there's multiple shard servers

1115
01:03:14,130 --> 01:03:17,910
and that this clerk library might need to talk to servers multiple times,

1116
01:03:17,910 --> 01:03:21,450
it might need to refresh its information about which server owns the,

1117
01:03:21,480 --> 01:03:22,500
you know which keys,

1118
01:03:22,650 --> 01:03:27,150
and we basically you prove respect that allows you to forget all that

1119
01:03:27,150 --> 01:03:29,940
and use a key guy service just by calling these puts and gets

1120
01:03:30,060 --> 01:03:33,990
and having this idealized notion of what the key value mapping actually looks like.

1121
01:03:34,750 --> 01:03:38,020
So I won't talk too much more in detail about what the actual proof looks like,

1122
01:03:38,230 --> 01:03:43,420
instead to focus to, issue focus towards a next thing we were interested in doing,

1123
01:03:43,600 --> 01:03:46,990
which is actually doing something with a bit of fault tolerance to this,

1124
01:03:47,050 --> 01:03:50,080
so I can mention the key value itself is not replicated

1125
01:03:50,080 --> 01:03:51,430
and isn't fault tolerant

1126
01:03:51,580 --> 01:03:57,550
and so we started by trying to figure out how to verify the simplest possible fault tolerant protocol,

1127
01:03:57,640 --> 01:04:00,550
and we basically started a Single-decree Paxos

1128
01:04:00,730 --> 01:04:04,630
to decree paxos is a classic protocol for getting contents on a single value,

1129
01:04:05,060 --> 01:04:06,290
so whereas with a raft,

1130
01:04:06,380 --> 01:04:08,870
you can get you replicate entire log

1131
01:04:08,870 --> 01:04:10,460
and you keep appending new entries to log,

1132
01:04:10,490 --> 01:04:13,820
single-decree paxos is the [] of multi []

1133
01:04:13,820 --> 01:04:16,160
and basically functions of write once register,

1134
01:04:16,280 --> 01:04:19,220
if you want to set the value to something,

1135
01:04:19,250 --> 01:04:21,320
you can attempt to write to it,

1136
01:04:21,590 --> 01:04:22,760
and if someone else beat you,

1137
01:04:22,760 --> 01:04:24,830
then, that's too bad for you,

1138
01:04:24,830 --> 01:04:26,180
and now the value has already been decided

1139
01:04:26,180 --> 01:04:27,350
and it's never going to change again,

1140
01:04:28,080 --> 01:04:32,700
so we implemented and partially verified a single-decree paxos implementation

1141
01:04:32,790 --> 01:04:36,600
and basically prove a specification that shows that it's a write once register

1142
01:04:37,560 --> 01:04:40,740
and the key idea in the specification and the proof

1143
01:04:40,920 --> 01:04:44,610
is that when you commit a value in in single-decree paxos,

1144
01:04:44,850 --> 01:04:48,840
you get irrevocable knowledge of what that a committed value is

1145
01:04:48,900 --> 01:04:50,790
and you basically know that from here on out,

1146
01:04:50,790 --> 01:04:53,430
if anybody else ever sees any committed value,

1147
01:04:53,550 --> 01:04:56,160
it's going to be the exact same thing that you see right now.

1148
01:04:57,190 --> 01:05:00,400
And thinking about this a little bit after we worked on the proof of it,

1149
01:05:00,910 --> 01:05:02,740
we sort of thinking that,

1150
01:05:02,800 --> 01:05:03,970
we sort of notice that,

1151
01:05:03,970 --> 01:05:07,240
there's a slight generalization you can do to single-decree paxos,

1152
01:05:07,240 --> 01:05:10,900
which a we all Monotone Paxos for lack of better name

1153
01:05:11,320 --> 01:05:16,210
and the idea is rather than gaining knowledge about the exact value upon a commit,

1154
01:05:16,480 --> 01:05:19,450
you instead can we modify the protocol,

1155
01:05:19,630 --> 01:05:22,930
so that you only gain knowledge about a lower bound on the value,

1156
01:05:23,570 --> 01:05:27,410
so, basically when you commit a value,

1157
01:05:27,470 --> 01:05:30,410
for example you commit at number 15 to this write, once register

1158
01:05:30,500 --> 01:05:34,550
rather than knowing that 15 is the only value to anybody else in the future, we're going to see,

1159
01:05:34,760 --> 01:05:37,130
you'll know that any value that people see in the future

1160
01:05:37,130 --> 01:05:40,100
as committed is going to be at least 15,

1161
01:05:40,100 --> 01:05:45,200
and so of course doing this requires having some notion of what larger than actually means for the value type.

1162
01:05:45,550 --> 01:05:53,320
And the key idea is a replica can always find out what the latest committed value is

1163
01:05:53,410 --> 01:05:54,670
and choose to increase it

1164
01:05:54,850 --> 01:06:00,970
and other replicates can continually find out larger and larger lower bounds on basically the value so far is,

1165
01:06:01,680 --> 01:06:05,700
and once we sort of came up with this idea of monotone paxos,

1166
01:06:05,760 --> 01:06:09,780
we realized immediately that we can do log replication with this,

1167
01:06:10,200 --> 01:06:15,600
so the set of values V, we can choose to simply be all the logs,

1168
01:06:15,630 --> 01:06:18,540
that you might want to replicate your logs of operations

1169
01:06:18,660 --> 01:06:21,060
and we can define one log to be bigger than another one,

1170
01:06:21,060 --> 01:06:25,110
if the smaller one is a prefix of l1 is a prefix of l2,

1171
01:06:26,820 --> 01:06:29,040
and this basically allows us to know,

1172
01:06:29,040 --> 01:06:31,770
this sort of yield a protocol which you'll have to trust,

1173
01:06:31,770 --> 01:06:33,000
that I'm showing you the code for,

1174
01:06:33,300 --> 01:06:38,070
in which you can gain information about what the prefix of the log is

1175
01:06:38,340 --> 01:06:40,560
and over time you can add new things to log

1176
01:06:40,560 --> 01:06:41,790
by making it larger and larger

1177
01:06:41,790 --> 01:06:43,500
and that's pretty much exactly what we mean,

1178
01:06:43,500 --> 01:06:44,580
when we say log replication.

1179
01:06:45,380 --> 01:06:48,500
The problem with this exact protocol is that

1180
01:06:48,500 --> 01:06:52,790
naively if we implement the most naive version of this monotone paxos thing,

1181
01:06:53,000 --> 01:06:56,060
you would need to send around the full log on every single RPC,

1182
01:06:56,300 --> 01:07:00,320
in single-decree paxos, those you sent around the [fault] value on all the RPCs

1183
01:07:00,410 --> 01:07:05,840
and the new trivial generalization of the model in paxos would have to be sent around the full log,

1184
01:07:06,080 --> 01:07:08,990
that's not really useful to log to get larger and larger and larger

1185
01:07:08,990 --> 01:07:11,930
and the beginning of log is no longer relevant by the time,

1186
01:07:11,930 --> 01:07:13,070
everybody agreed to commit

1187
01:07:13,070 --> 01:07:17,810
and all that, so you could try to optimize this by only passing around a suffix of the log

1188
01:07:18,170 --> 01:07:21,860
and indeed, there's a whole sequence of optimization,

1189
01:07:21,860 --> 01:07:25,100
you can make to this monotone paxos based log replication,

1190
01:07:25,460 --> 01:07:27,770
and as you start doing more and more of this,

1191
01:07:27,770 --> 01:07:30,470
you'll realize that this looks exactly like raft

1192
01:07:31,010 --> 01:07:36,350
and in fact, We aimed basically to use our idea of monitoring paxos

1193
01:07:36,380 --> 01:07:38,930
not to implement a new replication protocol,

1194
01:07:38,990 --> 01:07:42,050
but rather to a verify a raft like system,

1195
01:07:42,320 --> 01:07:44,330
so we have this proof for single-decree paxos,

1196
01:07:44,450 --> 01:07:47,120
we have this clear generalization to this monotone paxos thing

1197
01:07:47,690 --> 01:07:50,120
and our hope is that we can use the idea of monotone paxos,

1198
01:07:50,120 --> 01:07:52,910
so to basically verify raft directly,

1199
01:07:52,970 --> 01:07:57,800
as opposed to rely on the much more complicated correctness argued for raft,

1200
01:07:58,010 --> 01:08:01,670
that has been described in sort of other state machine type styles.

1201
01:08:02,220 --> 01:08:04,740
So this is our future work,

1202
01:08:05,010 --> 01:08:09,960
and the key takeaway I I sort of want to leave you guys with is that

1203
01:08:10,020 --> 01:08:13,470
reasoning both formal and informal about distributed systems,

1204
01:08:13,710 --> 01:08:15,840
should be as compositional as writing code,

1205
01:08:16,230 --> 01:08:18,030
the way you scale writing code is modular,

1206
01:08:18,030 --> 01:08:20,070
and that's the way reasoning should also scale.

1207
01:08:21,680 --> 01:08:24,530
And that's all for my presentation,

1208
01:08:24,530 --> 01:08:26,090
I'm happy to take questions.

1209
01:08:35,610 --> 01:08:36,810
If the answer is too long,

1210
01:08:36,810 --> 01:08:38,190
you can look me in the chat,

1211
01:08:38,190 --> 01:08:38,850
but I was curious,

1212
01:08:38,850 --> 01:08:39,840
I know there's a closet,

1213
01:08:39,840 --> 01:08:44,400
do you have any resources for somebody interested in getting into as a from a software perspective,

1214
01:08:44,400 --> 01:08:46,050
like any brief recommendations.

1215
01:08:46,780 --> 01:08:51,130
Are you, so yeah, I guess I'm not quite sure what the,

1216
01:08:51,780 --> 01:08:53,100
so, are you interested in like,

1217
01:08:53,100 --> 01:08:54,450
I I should message afterward,

1218
01:08:54,450 --> 01:08:55,860
but if you're interested I guess,

1219
01:08:55,860 --> 01:08:59,550
in a sort most lightweight versions of verification,

1220
01:08:59,670 --> 01:09:01,350
I think [Daphne] is a great tool to learn,

1221
01:09:01,350 --> 01:09:03,720
because it's a pretty simple starting point,

1222
01:09:03,720 --> 01:09:06,540
you can write real code and get a feel for things,

1223
01:09:06,630 --> 01:09:09,360
I think a lot of a lot of verification is pretty academic

1224
01:09:09,360 --> 01:09:13,500
and not super close to being really useful at verification like this,

1225
01:09:13,740 --> 01:09:16,440
so I'm not sure how useful it really would be for,

1226
01:09:16,470 --> 01:09:17,850
real software during just yet,

1227
01:09:18,060 --> 01:09:19,830
so the hope is that one day it will be.

1228
01:09:20,840 --> 01:09:21,230
Thanks.

1229
01:09:24,670 --> 01:09:29,260
Did you implement this version of paxos you're talking about.

1230
01:09:29,770 --> 01:09:31,330
This monotone paxos those thing.

1231
01:09:32,500 --> 01:09:36,610
So, yeah I implemented rather than implementing a generic monotone paxos thing,

1232
01:09:36,610 --> 01:09:37,630
which wouldn't really make sense

1233
01:09:37,630 --> 01:09:42,400
and go anyways I implemented directly the log replication over monotone paxos,

1234
01:09:42,430 --> 01:09:46,000
so in this monotone monotone log application thing,

1235
01:09:46,000 --> 01:09:47,650
all the RPC center on the full log,

1236
01:09:47,800 --> 01:09:49,750
if you run it for a long time to get way too slow,

1237
01:09:49,750 --> 01:09:51,490
because the RPCs are sending too much stuff,

1238
01:09:51,490 --> 01:09:52,900
but yeah I did implement it,

1239
01:09:52,900 --> 01:09:56,080
and I think we're working on actually trying to reason about it.

1240
01:09:57,490 --> 01:10:02,890
Did you [vanish] to figure out how performing it actually ends up being

1241
01:10:02,890 --> 01:10:04,960
or how it works in practice.

1242
01:10:06,040 --> 01:10:10,630
So I think the exact code that we have right now is not code you'd want to run

1243
01:10:10,630 --> 01:10:15,370
and my, in a sense, it ought to be as performant as raft,

1244
01:10:15,370 --> 01:10:18,190
sort of is and we don't really have an optimized implementation,

1245
01:10:18,190 --> 01:10:20,650
so I haven't actually bothered getting performance numbers for it at all,

1246
01:10:22,200 --> 01:10:23,940
it's probably pretty slow, not really sure.

1247
01:10:29,930 --> 01:10:30,770
Great, thank you,

1248
01:10:30,950 --> 01:10:33,440
yeah, even if we verify my programs,

1249
01:10:33,440 --> 01:10:35,060
I'm sure on this lock somewhere,

1250
01:10:35,630 --> 01:10:39,480
but let's hear from PP2 now.

1251
01:10:51,340 --> 01:10:52,120
Alright, can everyone see?

1252
01:10:54,150 --> 01:10:54,600
Great.

1253
01:10:54,990 --> 01:10:59,970
So, we are the [pigeon] protocol team, me [] me Jay Timmy,

1254
01:10:59,970 --> 01:11:02,520
and we are we present a simple distributed file system,

1255
01:11:03,120 --> 01:11:05,370
and the reason we selected a distributed file system

1256
01:11:05,370 --> 01:11:09,150
is that users often times want to sort data privately in a really accessible way

1257
01:11:09,180 --> 01:11:11,130
without the implications of using a cloud company,

1258
01:11:11,130 --> 01:11:12,600
where you don't own your own data.

1259
01:11:13,500 --> 01:11:15,120
So we wanted to create a solution,

1260
01:11:15,120 --> 01:11:19,920
where you self host, your data in a fault tolerant distributed manner on commodity hardware,

1261
01:11:20,690 --> 01:11:23,600
and our file system is really similar to Frangipani,

1262
01:11:23,600 --> 01:11:25,580
except that it uses raft instead of pedal

1263
01:11:25,610 --> 01:11:28,820
and the file system is on the servers instead of the clients.

1264
01:11:29,420 --> 01:11:30,980
In terms of file system parameters,

1265
01:11:30,980 --> 01:11:33,920
we also have a 4096 byte block size

1266
01:11:33,920 --> 01:11:36,440
and a 2 megabyte maximum file size,

1267
01:11:36,950 --> 01:11:40,100
we theoretically have a 32 gigabyte maximum disk capacity,

1268
01:11:40,280 --> 01:11:42,080
however this is actually constrained by your RAM,

1269
01:11:42,080 --> 01:11:44,300
so if you only have 8 gigabyte of RAM,

1270
01:11:44,300 --> 01:11:49,340
you would have, however much left over after your, after whatever your system takes up.

1271
01:11:50,140 --> 01:11:54,220
And we support as many servers and clients as we can within reason,

1272
01:11:54,250 --> 01:11:58,090
obviously the more servers and clients that you add to the locking contention,

1273
01:11:58,090 --> 01:12:02,980
there will be less performance as you start to access the same file over and over again.

1274
01:12:03,690 --> 01:12:06,060
And, in terms of performance,

1275
01:12:06,060 --> 01:12:09,660
well, we were very heavily focused on availability and crash recovery,

1276
01:12:09,660 --> 01:12:11,970
so we didn't, we didn't measure performance

1277
01:12:11,970 --> 01:12:13,230
and we think it's probably pretty bad,

1278
01:12:13,230 --> 01:12:15,510
because our system is built on top of raft,

1279
01:12:15,510 --> 01:12:18,990
which is not known to be the most performance of systems.

1280
01:12:19,890 --> 01:12:21,780
So, over to Jay.

1281
01:12:22,760 --> 01:12:25,700
So, again performance is not the biggest thing that we have,

1282
01:12:25,700 --> 01:12:28,610
but we do have very very strong consistency guarantees,

1283
01:12:28,670 --> 01:12:30,860
in particular we enforce POSIX consistency,

1284
01:12:30,860 --> 01:12:32,720
which is a form of strong consistency,

1285
01:12:32,720 --> 01:12:34,610
we usually see on local file systems,

1286
01:12:34,820 --> 01:12:37,340
so we enforce the invariant that after a file,

1287
01:12:37,340 --> 01:12:39,170
right after you do a successful file write,

1288
01:12:39,530 --> 01:12:42,290
any read of your previously written bytes from anywhere,

1289
01:12:42,290 --> 01:12:44,540
or will turn the data specified by that previous write,

1290
01:12:44,720 --> 01:12:50,360
similarly any new writes over that data will result in visible overwrites of that data,

1291
01:12:50,610 --> 01:12:52,170
from the perspective of other readers.

1292
01:12:52,650 --> 01:12:54,360
So in order to achieve this,

1293
01:12:54,360 --> 01:12:57,450
we have a data mode journal that is built into this sort of block layer,

1294
01:12:57,600 --> 01:13:00,660
that is again distributed with raft and replicated

1295
01:13:00,840 --> 01:13:03,000
and which is effectively a write-ahead log,

1296
01:13:03,000 --> 01:13:06,630
that guarantees the atomicity of writes strong semantics in the presence of crashes,

1297
01:13:06,690 --> 01:13:08,160
the same as raft pretty much

1298
01:13:08,400 --> 01:13:11,340
and also the consistency model that we offer above.

1299
01:13:11,660 --> 01:13:13,880
So servers also in order to help with this,

1300
01:13:13,880 --> 01:13:17,030
we issue distributed locking, so we can have this very primitive block cache,

1301
01:13:17,060 --> 01:13:18,800
as you would see in a local file system,

1302
01:13:19,100 --> 01:13:23,690
and also we have, we've leases to make sure that there's mutually exclusive access to all of these blocks.

1303
01:13:27,520 --> 01:13:30,190
Right, so to allow our clients to use our file system,

1304
01:13:30,190 --> 01:13:32,140
we created a POSIX like interface,

1305
01:13:32,140 --> 01:13:34,000
where users can interact with files,

1306
01:13:34,240 --> 01:13:37,300
we mainly have four functions open, close, read, write,

1307
01:13:37,450 --> 01:13:40,240
open and close are pretty explanatory,

1308
01:13:40,240 --> 01:13:43,360
they just open and close file descriptors on our file system,

1309
01:13:44,020 --> 01:13:46,360
read, it just takes a file descriptor

1310
01:13:46,360 --> 01:13:49,900
and read a fixed number of bytes at the current file position,

1311
01:13:50,260 --> 01:13:54,190
write also takes a file descriptor and read flush is,

1312
01:13:54,340 --> 01:13:57,970
sorry write takes a file descriptor

1313
01:13:57,970 --> 01:14:01,660
and just writes the data to the file [],

1314
01:14:01,780 --> 01:14:05,380
but it's in a different way than a normal POSIX write,

1315
01:14:05,380 --> 01:14:09,940
because instead flushes the buffer copy of the file,

1316
01:14:09,940 --> 01:14:11,800
and then appends the new data on this,

1317
01:14:12,310 --> 01:14:14,260
so instead of a normal POSIX write,

1318
01:14:14,260 --> 01:14:17,750
where we just write to a [] descriptor with a buffer

1319
01:14:17,750 --> 01:14:20,840
and the number of bytes to write it does, what I just said instead.

1320
01:14:21,020 --> 01:14:25,160
And we have a demo to represent clients interacting with the system.

1321
01:14:28,290 --> 01:14:30,450
So this is just a quick demonstration of our file system

1322
01:14:30,450 --> 01:14:32,670
being run both serially and concurrently,

1323
01:14:32,670 --> 01:14:35,130
so what's going to happen is console one

1324
01:14:35,130 --> 01:14:36,900
or I should say left console

1325
01:14:36,930 --> 01:14:41,040
is going to open up a file called tt, just testing thing,

1326
01:14:42,670 --> 01:14:44,230
it's going to write something to the file,

1327
01:14:44,230 --> 01:14:46,810
and then the console on the right is going to read from it afterwards,

1328
01:14:47,020 --> 01:14:48,490
per the consistency model,

1329
01:14:48,520 --> 01:14:51,820
they should see the same thing as the left console wrote

1330
01:14:51,820 --> 01:14:57,630
and indeed after a minute, we see that.

1331
01:14:57,840 --> 01:15:00,390
Okay, so the next thing that's going to happen is,

1332
01:15:00,570 --> 01:15:06,870
both console one and console two are going to try to flush their local copies of this file at the same time,

1333
01:15:06,990 --> 01:15:08,970
this is not a traditional POSIX write,

1334
01:15:09,180 --> 01:15:11,730
they're both taking the copies of the file that they have

1335
01:15:11,730 --> 01:15:13,710
and trying to put them onto disk at the same time,

1336
01:15:14,140 --> 01:15:16,810
so it's sort of like two writes from offset zero,

1337
01:15:17,140 --> 01:15:18,550
one of these is going to win,

1338
01:15:19,060 --> 01:15:21,550
and we can look at the log on the left console,

1339
01:15:21,550 --> 01:15:22,210
as it commits

1340
01:15:22,210 --> 01:15:24,820
and actually see, after a moment,

1341
01:15:25,520 --> 01:15:28,070
that both transactions run concurrently,

1342
01:15:28,160 --> 01:15:29,960
they both take up different parts of the log,

1343
01:15:30,050 --> 01:15:31,250
but at the end of the day,

1344
01:15:31,370 --> 01:15:34,130
the left console transaction is going to win,

1345
01:15:34,520 --> 01:15:37,370
so it was completely atomic everything works.

1346
01:15:41,450 --> 01:15:41,960
So this.

1347
01:15:48,950 --> 01:15:49,970
I think you got muted.

1348
01:15:51,110 --> 01:15:52,160
Sorry, I muted,

1349
01:15:52,550 --> 01:15:53,810
due to time there are more,

1350
01:15:53,840 --> 01:15:56,810
there are some limitations and features we can add to our file system,

1351
01:15:57,020 --> 01:16:00,440
so firstly, we only have one root directory,

1352
01:16:00,440 --> 01:16:02,600
so adding more will definitely be a plus,

1353
01:16:02,990 --> 01:16:08,240
next we only, should persist blocks to disk rather than RAM,

1354
01:16:08,240 --> 01:16:09,770
because that's what we're currently doing,

1355
01:16:09,890 --> 01:16:13,190
but we should also consider that what we're doing,

1356
01:16:13,190 --> 01:16:14,120
has a lot of writes,

1357
01:16:14,120 --> 01:16:15,680
so it could be pretty bad,

1358
01:16:15,680 --> 01:16:16,940
if we just keep writing,

1359
01:16:17,310 --> 01:16:19,740
doing have a lot of writes for just one operation,

1360
01:16:19,950 --> 01:16:25,860
we also only have direct inode blocks rather than having direct and indirect inodes,

1361
01:16:25,860 --> 01:16:27,510
so that would be a plus to add those,

1362
01:16:27,510 --> 01:16:32,850
and secondly, there should be a better way for clients to interact with this file system,

1363
01:16:32,850 --> 01:16:34,080
so there could be a fuse layer

1364
01:16:34,080 --> 01:16:36,660
or they could just be better positive compliance in general.

1365
01:16:38,700 --> 01:16:40,050
Concludes our presentation.

1366
01:16:48,300 --> 01:16:50,760
Do you want to talk a little bit about how you tested this?

1367
01:16:52,290 --> 01:16:55,410
Sure, so we had a pretty at your [recommendation],

1368
01:16:55,410 --> 01:16:56,820
we had a pretty broad set of tests,

1369
01:16:56,820 --> 01:16:58,650
we had so to begin each component,

1370
01:16:58,650 --> 01:17:00,870
so each you know the block layer right here,

1371
01:17:00,870 --> 01:17:02,220
individual raft key values,

1372
01:17:02,220 --> 01:17:03,150
we had the journal

1373
01:17:03,240 --> 01:17:04,380
and we had all these things,

1374
01:17:04,410 --> 01:17:06,720
we basically mocked every layer underneath,

1375
01:17:06,750 --> 01:17:09,420
each layer we were testing and tested

1376
01:17:09,420 --> 01:17:10,890
did sort of unit testing,

1377
01:17:10,890 --> 01:17:12,870
you can't really call it unit testing, once you get high enough,

1378
01:17:13,160 --> 01:17:16,670
because you're so reliant on lower layers being correct,

1379
01:17:16,670 --> 01:17:18,620
but we did as best we could,

1380
01:17:18,620 --> 01:17:20,210
from there we did integration testing

1381
01:17:20,210 --> 01:17:22,550
and you know wrote out a set of partitions,

1382
01:17:22,550 --> 01:17:24,860
one of those partitions is sort of what you just saw,

1383
01:17:25,010 --> 01:17:26,090
the most interesting of them,

1384
01:17:26,090 --> 01:17:26,930
there were five of them,

1385
01:17:26,930 --> 01:17:29,060
you can see them in our git repository,

1386
01:17:29,360 --> 01:17:31,760
and then we also, to what degree, we could,

1387
01:17:31,760 --> 01:17:33,860
so this didn't necessarily entirely work,

1388
01:17:33,860 --> 01:17:35,960
just because of some of the time limitations that we had,

1389
01:17:35,960 --> 01:17:38,630
but we also attempted to do some stress testing,

1390
01:17:38,660 --> 01:17:40,220
obviously performance numbers aren't great,

1391
01:17:40,430 --> 01:17:42,020
because it's not supposed to be great,

1392
01:17:42,020 --> 01:17:43,820
but, the best we could,

1393
01:17:43,820 --> 01:17:46,490
so we're pretty sure this is at least correct verified here.

1394
01:18:02,790 --> 01:18:09,090
One interesting thing is that you're basically shooting for actually slightly stronger consistency property that POSIX are required,

1395
01:18:09,510 --> 01:18:13,080
just two proceeds write into a single file,

1396
01:18:13,080 --> 01:18:14,310
there's actually not much,

1397
01:18:14,340 --> 01:18:17,070
actually the write [] we have to have to do.

1398
01:18:17,880 --> 01:18:21,150
Yes, that was kind of accidental,

1399
01:18:21,180 --> 01:18:22,320
but we were,

1400
01:18:22,500 --> 01:18:24,060
so it's kind of stronger.

1401
01:18:24,240 --> 01:18:24,810
Yeah.

1402
01:18:24,810 --> 01:18:26,550
It was accidental, but it happened,

1403
01:18:26,550 --> 01:18:28,680
so we were like cool, it worked.

1404
01:18:30,480 --> 01:18:33,210
The server, I mean I think part of what sort of happened was,

1405
01:18:33,210 --> 01:18:34,650
that we guarantee we,

1406
01:18:34,650 --> 01:18:36,210
I positively guarantees I talked about,

1407
01:18:36,210 --> 01:18:38,730
but we also make the guarantee that the block writes are all atomic,

1408
01:18:38,760 --> 01:18:42,900
and I think that's why we get this sort of stronger consistency,

1409
01:18:42,900 --> 01:18:44,250
because of the whole journal thing,

1410
01:18:44,250 --> 01:18:46,050
yes, you could get your write overwritten,

1411
01:18:46,050 --> 01:18:47,070
if you do at the same time,

1412
01:18:47,070 --> 01:18:48,090
but it's always going to be clean.

1413
01:18:51,740 --> 01:18:53,150
If you have aggressive caching,

1414
01:18:53,150 --> 01:18:57,080
you might run in and don't write immediately to the log,

1415
01:18:57,080 --> 01:18:59,120
then you might get different behavior.

1416
01:19:00,370 --> 01:19:01,840
This is, why we don't aggressively cache,

1417
01:19:01,870 --> 01:19:04,240
we have a block cache of size one.

1418
01:19:10,890 --> 01:19:12,300
Awesome, thank you, very cool,

1419
01:19:12,660 --> 01:19:15,870
alright, our last group is presenting a game framework,

1420
01:19:16,430 --> 01:19:17,780
whenever you're ready, take it away.

1421
01:19:21,660 --> 01:19:26,190
Alright, so we're sure that many of you have played multiplayer games in quarantine,

1422
01:19:26,220 --> 01:19:26,850
when you're bored.

1423
01:19:27,660 --> 01:19:29,070
So let's imagine that,

1424
01:19:29,070 --> 01:19:31,740
you are a small indie game dev company

1425
01:19:32,190 --> 01:19:34,140
and you're trying to develop a most player game,

1426
01:19:34,170 --> 01:19:37,770
possibly that has either several different rooms,

1427
01:19:37,770 --> 01:19:38,910
something like chat penguin,

1428
01:19:38,910 --> 01:19:41,220
where you might interact with other players in the same room,

1429
01:19:41,370 --> 01:19:42,990
or perhaps it's like matchmaking base,

1430
01:19:42,990 --> 01:19:45,420
where you're in a Lobby with several other players.

1431
01:19:45,930 --> 01:19:48,600
Well, so traditionally, how these work is that,

1432
01:19:48,690 --> 01:19:51,810
everything goes against process on one central server,

1433
01:19:52,050 --> 01:19:54,210
but that central server is a bottleneck,

1434
01:19:54,450 --> 01:19:58,590
if every single player has to connect to that server to handle the game logic,

1435
01:19:58,650 --> 01:20:02,550
that server began a bottleneck by the number of requests that come through

1436
01:20:02,910 --> 01:20:04,560
and also if that server goes down,

1437
01:20:05,070 --> 01:20:05,970
there goes your game.

1438
01:20:06,760 --> 01:20:10,060
So what we're proposing is to create a distributed game framework,

1439
01:20:10,060 --> 01:20:12,700
that instead, that's also fault tolerant,

1440
01:20:12,940 --> 01:20:16,210
so instead of having all the processing being on that central server,

1441
01:20:16,210 --> 01:20:20,410
we actually distribute the game logic processing to several different worker servers,

1442
01:20:20,980 --> 01:20:22,030
but on top of that,

1443
01:20:22,560 --> 01:20:23,880
to be fault tolerant,

1444
01:20:23,880 --> 01:20:26,340
so when one of these worker servers goes down,

1445
01:20:26,370 --> 01:20:29,010
we need to be able to handle that game logic

1446
01:20:29,010 --> 01:20:32,550
and further move players to some other service workers,

1447
01:20:32,850 --> 01:20:37,200
so as part of that, we need to actually balance latency with fault tolerance,

1448
01:20:37,200 --> 01:20:40,050
because if we make everything stay fault tolerant,

1449
01:20:40,170 --> 01:20:44,880
we might run into a each move taking a long time to process.

1450
01:20:46,080 --> 01:20:47,820
So that's why we're introducing Pinguino,

1451
01:20:48,060 --> 01:20:49,860
which is our fault tolerant game framework,

1452
01:20:49,860 --> 01:20:53,880
that addresses all of the previous issues with distributed networks.

1453
01:20:59,160 --> 01:21:01,680
To dive a bit into the system of our framework,

1454
01:21:01,710 --> 01:21:03,690
let's imagine the game club penguin,

1455
01:21:04,050 --> 01:21:08,340
so in club penguin, user that is assigned into one room or one region,

1456
01:21:08,580 --> 01:21:11,940
it only really cares about talking and interacting with other users

1457
01:21:11,940 --> 01:21:14,610
and the objects in that one room

1458
01:21:14,640 --> 01:21:17,940
and they don't really need to care about anything else that's happening in another room,

1459
01:21:18,440 --> 01:21:24,140
so there's no reason to have the request of every user be processed by one centralized servers,

1460
01:21:24,530 --> 01:21:30,320
so instead we decided that we will have all of these requests be process across multiple workers,

1461
01:21:30,470 --> 01:21:34,550
in order to do this, we have workers that are assigned to different regions,

1462
01:21:34,550 --> 01:21:36,560
so if a player is in one region,

1463
01:21:36,680 --> 01:21:40,280
they might be talking with the worker that is assigned to that region,

1464
01:21:40,280 --> 01:21:41,510
so for example in here,

1465
01:21:41,630 --> 01:21:43,070
the penguin that is in work,

1466
01:21:43,160 --> 01:21:45,320
that is assigned to the region for worker 2,

1467
01:21:45,320 --> 01:21:46,850
we'll talk with worker 2 only,

1468
01:21:46,940 --> 01:21:50,510
but then the one in the worker and will talk with worker.

1469
01:21:51,210 --> 01:21:52,680
And additionally we mentioned,

1470
01:21:52,680 --> 01:21:56,940
we decided that this wouldn't the relation between worker and regions,

1471
01:21:56,940 --> 01:21:59,100
doesn't necessarily have to be one to one mapping,

1472
01:21:59,490 --> 01:22:02,760
for some rooms that might be less popular and have less traffic,

1473
01:22:02,790 --> 01:22:06,390
it's possible that a worker can handle multiple of those,

1474
01:22:06,510 --> 01:22:08,580
so there's that type of relation,

1475
01:22:08,820 --> 01:22:09,960
that we need to keep track of.

1476
01:22:10,540 --> 01:22:12,040
And so in order to keep track of this,

1477
01:22:12,040 --> 01:22:13,870
we do need one centralized server,

1478
01:22:13,870 --> 01:22:15,160
which is the coordinator,

1479
01:22:15,370 --> 01:22:18,250
so the coordinator will be keeping track of all these mappings

1480
01:22:18,580 --> 01:22:23,920
and some of the mapping includes the region to worker relation as well as region,

1481
01:22:23,950 --> 01:22:28,370
the worker to their replica relation,

1482
01:22:28,370 --> 01:22:30,650
as well as the players itself,

1483
01:22:30,860 --> 01:22:33,350
so for the fault tolerant aspect,

1484
01:22:33,380 --> 01:22:36,620
we have, that the workers will have two replicate each

1485
01:22:36,650 --> 01:22:40,670
and surely we can talk a little bit more about what kind of information

1486
01:22:40,670 --> 01:22:45,050
is sent to the worker from there to [there] later.

1487
01:22:45,610 --> 01:22:47,470
And additionally, the coordinator,

1488
01:22:47,470 --> 01:22:49,330
because it is that one centralized server,

1489
01:22:49,330 --> 01:22:51,970
it is also a possible a failure point,

1490
01:22:51,970 --> 01:22:53,590
so we have a coordinator backup

1491
01:22:53,650 --> 01:22:56,560
and here the coordinator's main role is

1492
01:22:56,560 --> 01:22:59,980
just to keep track of all of these relations of the game states,

1493
01:23:00,100 --> 01:23:04,660
so information about the coordinator that changes for those relations

1494
01:23:04,690 --> 01:23:06,370
will be sent to the coordinator backup,

1495
01:23:06,370 --> 01:23:10,270
before being processed a complete completely.

1496
01:23:10,690 --> 01:23:13,960
So now with this, although we have that one server,

1497
01:23:14,320 --> 01:23:16,990
the bulk of the traffic for games usually is

1498
01:23:16,990 --> 01:23:20,050
players making moves and sending request process those moves

1499
01:23:20,170 --> 01:23:23,020
and those are now divided across multiple workers

1500
01:23:23,020 --> 01:23:25,360
and the coordinator is in charge of just a mapping

1501
01:23:25,360 --> 01:23:28,900
and sending heartbeat to ensure that the workers are still alive

1502
01:23:28,930 --> 01:23:31,270
and can handle any failure cases.

1503
01:23:33,210 --> 01:23:35,640
Yep, so in the case that a worker goes down,

1504
01:23:35,730 --> 01:23:41,310
we have the coordinator handling the reassignment of the players who were in that worker

1505
01:23:41,670 --> 01:23:44,880
and because the coordinator manages only the region mapping,

1506
01:23:44,880 --> 01:23:47,550
it's also easy for us to move around regions

1507
01:23:47,550 --> 01:23:48,900
when one of us,

1508
01:23:49,360 --> 01:23:51,070
say one worker gets overloaded,

1509
01:23:51,070 --> 01:23:53,500
this allows us to perform some amount of load balancing,

1510
01:23:54,380 --> 01:23:55,460
as we mentioned earlier.

1511
01:23:56,400 --> 01:23:59,790
Cool, so I'll move on to what the developer API looks like,

1512
01:23:59,790 --> 01:24:02,550
because another key feature that we wanted was

1513
01:24:02,580 --> 01:24:04,170
for the framework to be easy to use,

1514
01:24:04,170 --> 01:24:07,290
for a developer trying to code a new game in it,

1515
01:24:07,530 --> 01:24:10,200
so we treat the game as a state machine essentially

1516
01:24:10,410 --> 01:24:12,720
and so any move that the player makes,

1517
01:24:12,840 --> 01:24:15,060
actually fits into one of two different types of moves,

1518
01:24:15,330 --> 01:24:16,470
so I mentioned earlier,

1519
01:24:16,470 --> 01:24:20,220
that we're trying to balance a bit of latency with fault tolerant,

1520
01:24:20,520 --> 01:24:24,540
so in order to provide a sort of choice for the developer,

1521
01:24:24,540 --> 01:24:28,260
we have two separate commands that we expose to the developer,

1522
01:24:28,350 --> 01:24:30,120
the first is send fast move,

1523
01:24:30,240 --> 01:24:34,890
so this fast move make sure that the move gets to the replicas as soon as possible,

1524
01:24:34,890 --> 01:24:38,220
so the move gets processed as fast as possible on the worker,

1525
01:24:39,090 --> 01:24:41,490
on the other hand, we have sendStableMove,

1526
01:24:41,580 --> 01:24:44,520
which actually is a more fault tolerant move,

1527
01:24:44,520 --> 01:24:46,200
that we expose to the developer

1528
01:24:46,350 --> 01:24:50,850
and this ensures this is mainly used for game critical logic changes,

1529
01:24:50,850 --> 01:24:52,200
such as say transaction,

1530
01:24:52,530 --> 01:24:54,540
so if you're buying something you don't want,

1531
01:24:55,090 --> 01:24:56,950
like if you've already spent that money,

1532
01:24:56,950 --> 01:25:00,730
you want to make sure that you get whatever you spend that money on your game

1533
01:25:01,030 --> 01:25:03,790
and so we guarantee that,

1534
01:25:03,790 --> 01:25:07,660
if that move gets fully processed and on the game,

1535
01:25:07,870 --> 01:25:10,800
it's, it's stored on both of the replica,

1536
01:25:10,950 --> 01:25:14,460
which ensures that if the worker that you're talking to goes down

1537
01:25:14,460 --> 01:25:16,350
and the player gets transferred to a new worker,

1538
01:25:16,620 --> 01:25:19,110
that new worker will be able to reconstruct the game,

1539
01:25:19,110 --> 01:25:20,490
including that transaction,

1540
01:25:20,670 --> 01:25:22,890
this guarantee isn't done for a fast move,

1541
01:25:22,890 --> 01:25:25,290
which prioritizes the latency aspect,

1542
01:25:25,590 --> 01:25:26,940
but you can also see here,

1543
01:25:26,940 --> 01:25:31,560
the move struct, that the developer defines are pretty general,

1544
01:25:31,560 --> 01:25:38,010
so in the game that will be that we kind of developed as a toy demo for our framework,

1545
01:25:38,310 --> 01:25:40,380
it's kind of a chat penguin like interface.

1546
01:25:40,580 --> 01:25:45,350
So each player is in several different rooms

1547
01:25:45,650 --> 01:25:47,360
and so within each room,

1548
01:25:47,360 --> 01:25:51,230
there's a chat window that you can talk to to interact with another player,

1549
01:25:51,620 --> 01:25:53,450
so the two types of main moves,

1550
01:25:53,450 --> 01:25:54,710
that you can make in this game

1551
01:25:54,710 --> 01:25:55,940
are first a move,

1552
01:25:56,030 --> 01:26:01,400
so a developer would just define like the X Y and the username of the player moving,

1553
01:26:01,670 --> 01:26:04,070
and so the chat message is kind of the same words

1554
01:26:04,070 --> 01:26:07,130
just like a chat message that you send into the window.

1555
01:26:07,760 --> 01:26:10,600
And so, we, in our game,

1556
01:26:10,600 --> 01:26:13,180
we made chat messages a stable move

1557
01:26:13,180 --> 01:26:14,950
and move as a fast move,

1558
01:26:15,160 --> 01:26:17,530
so even if like say one move gets dropped,

1559
01:26:17,530 --> 01:26:18,040
it's okay,

1560
01:26:18,040 --> 01:26:19,720
if you're a new kind of [teleports],

1561
01:26:19,750 --> 01:26:23,860
but, we don't want chat messages to randomly disappear,

1562
01:26:23,860 --> 01:26:25,420
because they could be important messages.

1563
01:26:26,080 --> 01:26:28,690
So with that, I'll move on to the demo,

1564
01:26:28,720 --> 01:26:30,070
which is a little bare bones,

1565
01:26:30,070 --> 01:26:32,380
but should show off the functionality.

1566
01:26:40,500 --> 01:26:42,960
So we have our super minimalist frontend,

1567
01:26:44,690 --> 01:26:46,430
and so when we move around the penguin,

1568
01:26:46,430 --> 01:26:48,770
we can see that, it's first sense of fast move,

1569
01:26:48,980 --> 01:26:53,630
and the replica that same move gets sent to the other replica

1570
01:26:53,630 --> 01:26:54,710
is assigned to the maine workers,

1571
01:26:54,710 --> 01:26:56,120
so right now, we're on worker zero,

1572
01:26:56,300 --> 01:26:58,340
it gets replicated to work one and two,

1573
01:26:58,370 --> 01:26:59,480
so we have two copies,

1574
01:26:59,870 --> 01:27:03,140
and the game server then receives that change

1575
01:27:03,140 --> 01:27:04,790
and so they can process that locally,

1576
01:27:05,300 --> 01:27:08,150
and then if we send a chat message,

1577
01:27:09,460 --> 01:27:11,890
we also have the player username

1578
01:27:11,890 --> 01:27:13,600
identified with the chat message sent,

1579
01:27:13,660 --> 01:27:15,610
but this is actually a stable move,

1580
01:27:15,790 --> 01:27:18,340
so it's not visible in the logs,

1581
01:27:18,340 --> 01:27:23,170
but the stable moves wait until those moves are actually replicated to the workers

1582
01:27:23,170 --> 01:27:24,280
and it's not easy to see here,

1583
01:27:24,280 --> 01:27:27,820
because normally there might be some amount of lag,

1584
01:27:28,530 --> 01:27:31,320
but when we do introduce some amount of lag into the network,

1585
01:27:31,350 --> 01:27:33,420
that's move takes longer than it does,

1586
01:27:34,160 --> 01:27:38,630
and now move it back to some future work that we want to implement.

1587
01:27:44,060 --> 01:27:45,080
In terms of the backend,

1588
01:27:45,080 --> 01:27:47,000
one additional thing that we would like to do

1589
01:27:47,000 --> 01:27:50,540
is to allow the users to move across different rooms,

1590
01:27:50,660 --> 01:27:53,570
so right now, upon a user joining the game,

1591
01:27:53,570 --> 01:27:54,740
it'd be initialized,

1592
01:27:54,740 --> 01:27:56,240
they are assigned to one room,

1593
01:27:56,300 --> 01:27:58,910
but ideally if they want to move across to it,

1594
01:27:58,940 --> 01:28:00,590
if they want to move to a different room,

1595
01:28:00,620 --> 01:28:02,780
then they should be able to talk to the coordinator to be like,

1596
01:28:02,780 --> 01:28:05,030
hey I'm gonna go to this region now,

1597
01:28:05,030 --> 01:28:08,390
can we load up the information of the game state from that region

1598
01:28:08,390 --> 01:28:11,060
and then also now I'm going to start talking to a new worker.

1599
01:28:11,620 --> 01:28:13,900
And additionally we hinted at this earlier,

1600
01:28:14,200 --> 01:28:17,380
where we wanted to deal with region based worker load balancing,

1601
01:28:17,470 --> 01:28:19,150
so the reason why we had that,

1602
01:28:19,180 --> 01:28:23,590
why we did not go for one to one mapping with worker to region

1603
01:28:23,590 --> 01:28:26,320
was to allow for this future work

1604
01:28:26,320 --> 01:28:27,640
and we hope to be able to do that

1605
01:28:27,640 --> 01:28:32,890
in order to control how much load each worker will be based with.

1606
01:28:44,260 --> 01:28:46,900
Sorry, I have a question,

1607
01:28:46,960 --> 01:28:48,970
so are your,

1608
01:28:49,880 --> 01:28:54,170
again, you have two actions sending a message and moving,

1609
01:28:54,200 --> 01:28:57,170
so those actions are all atomic right,

1610
01:28:57,260 --> 01:28:57,920
are they like.

1611
01:28:59,640 --> 01:29:01,170
Yeah.

1612
01:29:03,000 --> 01:29:04,230
They get sequentially processed

1613
01:29:04,230 --> 01:29:06,120
and since they are like individual moves,

1614
01:29:06,120 --> 01:29:10,260
they most of the time they only modify some variables

1615
01:29:10,260 --> 01:29:12,810
and they acquire that locks on those variables before the moves.

1616
01:29:16,350 --> 01:29:20,700
So you said earlier, that you have a coordinator and coordinator backup

1617
01:29:20,880 --> 01:29:24,360
and the replicas can talk to either of them,

1618
01:29:24,660 --> 01:29:27,810
what happens if you have a network partition,

1619
01:29:28,020 --> 01:29:29,640
that separates the coordinator

1620
01:29:29,640 --> 01:29:32,550
and some set of replicas from the coordinator backup

1621
01:29:32,550 --> 01:29:34,080
and some other set of replicas.

1622
01:29:37,900 --> 01:29:41,320
So the coordinator backup in case of a network partition,

1623
01:29:41,560 --> 01:29:43,600
the workers will be kind of lost,

1624
01:29:43,600 --> 01:29:48,880
like the coordinator is not a matter of the worker being able to talk to either the coordinator or the coordinator backup,

1625
01:29:48,880 --> 01:29:51,280
they'll only be able to talk to the coordinator

1626
01:29:51,310 --> 01:29:52,990
and if a coordinator goes down,

1627
01:29:52,990 --> 01:29:57,040
then the backup gets brought up to actually start processing,

1628
01:29:57,070 --> 01:29:58,900
so in that case of network partition,

1629
01:29:58,900 --> 01:30:00,550
I don't think we will be like.

1630
01:30:00,930 --> 01:30:04,950
The workers that are isolated and away from that coordinator

1631
01:30:05,010 --> 01:30:07,020
will not be able to be processed

1632
01:30:07,020 --> 01:30:09,810
with the like the coordinator itself

1633
01:30:09,900 --> 01:30:11,580
and turns out the user side,

1634
01:30:12,070 --> 01:30:13,150
it can still be processed,

1635
01:30:13,150 --> 01:30:16,270
because the player just needs to continue talking to that worker,

1636
01:30:16,330 --> 01:30:18,940
it's just that, if there's any changes in the region,

1637
01:30:19,270 --> 01:30:21,670
like the state of the game as a whole,

1638
01:30:21,700 --> 01:30:23,080
that will be processed yet.

1639
01:30:23,500 --> 01:30:25,360
Yeah, additionally I want to make that,

1640
01:30:25,570 --> 01:30:27,520
if we have a partition,

1641
01:30:27,550 --> 01:30:29,710
the coordinator backup essentially acts as a coordinator

1642
01:30:29,710 --> 01:30:31,810
for all of the workers that it can talk to

1643
01:30:31,990 --> 01:30:33,010
and this is fine,

1644
01:30:33,010 --> 01:30:36,430
because we want the game to be still running for all of the regions

1645
01:30:36,430 --> 01:30:38,620
in the workers at the coordinator backup is talking to,

1646
01:30:38,740 --> 01:30:40,510
this mainly becomes a problem,

1647
01:30:40,510 --> 01:30:42,340
when they do reunite

1648
01:30:42,370 --> 01:30:48,520
and in this case the coordinator backup then takes all of its data

1649
01:30:48,520 --> 01:30:49,990
and it can send it to the coordinator

1650
01:30:49,990 --> 01:30:51,880
and the coordinator can locally resolve it,

1651
01:30:51,910 --> 01:30:55,720
because there is kind of an original coordinator and the coordinator backup

1652
01:30:55,720 --> 01:30:59,410
and they know that the backup was, backup of the coordinator,

1653
01:30:59,440 --> 01:31:00,700
because it's stored locally.

1654
01:31:02,330 --> 01:31:05,690
But if the coordinator backup becomes a coordinator,

1655
01:31:05,720 --> 01:31:07,040
then wouldn't it,

1656
01:31:07,280 --> 01:31:09,800
for example say, oh I need to make sure that,

1657
01:31:09,800 --> 01:31:13,610
we have active replicas for all these rooms,

1658
01:31:13,610 --> 01:31:16,190
that are that are on the other side of the partition,

1659
01:31:16,250 --> 01:31:18,770
wouldn't you have the same room hosts on both sides,

1660
01:31:18,770 --> 01:31:20,810
the partition and be able to diverge.

1661
01:31:23,820 --> 01:31:28,560
No, because each room belongs only to like one worker,

1662
01:31:28,830 --> 01:31:30,210
so like,

1663
01:31:30,920 --> 01:31:34,220
so I guess, like each room, like can't,

1664
01:31:34,960 --> 01:31:37,840
like the replicas for the rooms would get abandoned,

1665
01:31:37,840 --> 01:31:39,250
so essentially what happens is,

1666
01:31:39,250 --> 01:31:40,420
if a worker,

1667
01:31:40,600 --> 01:31:41,860
like in the case of a partition,

1668
01:31:41,860 --> 01:31:44,980
the coordinator wouldn't be able to access a worker that's in the other partition,

1669
01:31:45,130 --> 01:31:46,810
so what happens is,

1670
01:31:47,290 --> 01:31:49,930
I think they like move their replicas over,

1671
01:31:50,220 --> 01:31:53,760
but because the players also can't contact worker,

1672
01:31:54,590 --> 01:31:56,510
none of the moves would be processed

1673
01:31:56,570 --> 01:32:01,910
and so the more recent replicas after the partition heals would be prioritized,

1674
01:32:01,910 --> 01:32:03,140
when healing that network.

1675
01:32:08,590 --> 01:32:12,280
Why did you decide on that API with move

1676
01:32:12,310 --> 01:32:14,380
and sending a message.

1677
01:32:16,330 --> 01:32:19,660
So specifically for this API,

1678
01:32:19,690 --> 01:32:22,510
we wanted two different types of moves,

1679
01:32:22,510 --> 01:32:23,590
two distinct types of moves,

1680
01:32:23,860 --> 01:32:26,980
demonstrate one, with a fast move, one with a stable move,

1681
01:32:26,980 --> 01:32:30,850
ideally stable move is used more like raraly

1682
01:32:30,850 --> 01:32:32,590
and used more for transactions,

1683
01:32:32,590 --> 01:32:34,630
that where it's okay for it to take longer,

1684
01:32:34,630 --> 01:32:36,850
but we wanted to not be dropped at all,

1685
01:32:37,090 --> 01:32:41,200
the easiest way to replicate this in a simple frontend was with a chat message,

1686
01:32:41,650 --> 01:32:42,610
so it's kind of arbitrary,

1687
01:32:43,670 --> 01:32:45,170
moves for sure should be fast,

1688
01:32:45,170 --> 01:32:48,380
because we are, like we don't want it to be,

1689
01:32:48,380 --> 01:32:49,760
because players move a lot.

1690
01:32:52,390 --> 01:32:53,200
Thank you.

1691
01:32:59,650 --> 01:33:00,700
Awesome, thanks so much,

1692
01:33:00,730 --> 01:33:02,500
that concludes the presentations,

1693
01:33:02,500 --> 01:33:03,190
great job everyone,

1694
01:33:03,190 --> 01:33:04,450
this was a pretty exciting.

1695
01:33:05,800 --> 01:33:10,330
I have one more question for an old presentation, if that's possible.

1696
01:33:10,820 --> 01:33:11,750
Yep, go ahead.

1697
01:33:12,420 --> 01:33:14,040
So for the leader,

1698
01:33:14,070 --> 01:33:17,490
I'm sorry, for distributed election system,

1699
01:33:19,680 --> 01:33:22,110
I'm not familiar lot with cryptography,

1700
01:33:22,110 --> 01:33:28,230
but I guess the system where you sum up all the results of the election

1701
01:33:28,590 --> 01:33:31,080
on a vote on a counter server,

1702
01:33:31,680 --> 01:33:34,110
this, wouldn't that [high] group attacks,

1703
01:33:34,110 --> 01:33:36,480
for example if I have two servers,

1704
01:33:36,480 --> 01:33:39,750
and then I vote for different people on both servers,

1705
01:33:39,750 --> 01:33:41,850
but then I coordinate with someone else

1706
01:33:42,000 --> 01:33:44,520
to also vote in the other way around,

1707
01:33:44,610 --> 01:33:47,100
will eventually get the same vote factor,

1708
01:33:47,550 --> 01:33:50,460
but I would have thought it maliciously against,

1709
01:33:50,460 --> 01:33:55,420
I guess in this case wouldn't change the vote results,

1710
01:33:55,420 --> 01:33:56,830
or like the election result,

1711
01:33:56,830 --> 01:33:59,350
but I guess I would have acted incorrectly,

1712
01:33:59,350 --> 01:34:03,310
so are there checks to make sure everybody voted correctly at each server.

1713
01:34:03,860 --> 01:34:05,930
Yeah.

1714
01:34:07,430 --> 01:34:11,840
Sorry, so we actually don't handle malicious voting,

1715
01:34:11,840 --> 01:34:14,420
which was which is which is pretty big,

1716
01:34:14,870 --> 01:34:19,640
and you know I used to be pretty important for for real-world voting system,

1717
01:34:20,130 --> 01:34:25,950
but yeah I think like you know the scope of the project that we had

1718
01:34:25,950 --> 01:34:26,940
and that we set out,

1719
01:34:27,210 --> 01:34:30,630
it was just a little too complicated,

1720
01:34:31,090 --> 01:34:33,850
so we, yeah.

1721
01:34:33,880 --> 01:34:37,540
I think we focus more on the distributed systems part,

1722
01:34:37,600 --> 01:34:40,360
but if we wanted to provide more security,

1723
01:34:40,360 --> 01:34:41,470
like for security,

1724
01:34:41,800 --> 01:34:44,320
using for example like an idea that we thought,

1725
01:34:44,320 --> 01:34:46,390
but then decided to do,

1726
01:34:46,390 --> 01:34:49,030
I was having a public ledger where you can make,

1727
01:34:49,390 --> 01:34:51,220
give us your knowledge proofs,

1728
01:34:51,220 --> 01:34:54,340
that they what you're posting like adds up

1729
01:34:54,340 --> 01:34:56,080
and is what you're saying that it is

1730
01:34:56,080 --> 01:34:59,800
and thinks of these things to handle malicious participants.

1731
01:35:02,170 --> 01:35:03,340
Yeah.

1732
01:35:03,550 --> 01:35:05,170
So we're running a little bit late,

1733
01:35:05,170 --> 01:35:07,540
yeah, it's in the class in principle

1734
01:35:07,540 --> 01:35:09,130
and anybody want to take around a question,

1735
01:35:09,130 --> 01:35:10,120
feel free to stick around,

1736
01:35:10,390 --> 01:35:14,530
as you want to say one of two things before closing since our last class meeting,

1737
01:35:14,890 --> 01:35:17,530
first of all, I want to thank you for participating,

1738
01:35:17,530 --> 01:35:19,360
even though it's another covid semester,

1739
01:35:19,540 --> 01:35:21,220
I feel I've interacted with many of you,

1740
01:35:21,220 --> 01:35:24,400
you for email, we're indirectly

1741
01:35:24,400 --> 01:35:25,900
and exchange lots of information

1742
01:35:25,900 --> 01:35:27,820
and I'd love to see you at some point in person,

1743
01:35:28,270 --> 01:35:29,770
actually know who you are,

1744
01:35:29,980 --> 01:35:33,940
the but, I appreciate all the participation.

1745
01:35:34,410 --> 01:35:36,150
The second thing I want to thank TAs,

1746
01:35:36,180 --> 01:35:39,270
it's an awesome set of TAs,

1747
01:35:39,270 --> 01:35:42,360
you probably realized, probably for many you,

1748
01:35:42,360 --> 01:35:45,390
they figured out some bugs and helped you get through the labs

1749
01:35:46,260 --> 01:35:49,740
and so I ran applause for the TAs,

1750
01:35:50,250 --> 01:35:53,400
very fortunate with this kind of quality.

1751
01:35:54,460 --> 01:35:57,280
And, I guess last thing I was to say,

1752
01:35:57,280 --> 01:35:58,060
I guess good luck on the final

1753
01:35:58,630 --> 01:36:01,210
and hopefully not too bad,

1754
01:36:01,210 --> 01:36:04,540
and I hope you had learned something in 6.824

1755
01:36:04,540 --> 01:36:06,130
and enjoyed it at the same time.

1756
01:36:07,320 --> 01:36:08,460
And anyone who wants to stick around,

1757
01:36:08,460 --> 01:36:09,270
please stick around,

1758
01:36:09,270 --> 01:36:13,230
you know more questions you want to ask to the different teams,

1759
01:36:13,230 --> 01:36:15,930
if the teams can stick around to be wonderful,

1760
01:36:15,930 --> 01:36:17,580
otherwise, well, this is the end,

1761
01:36:17,970 --> 01:36:19,980
at least for the class meetings for 6.824.

1762
01:36:20,950 --> 01:36:21,640
Thank you all.

1763
01:36:24,180 --> 01:36:24,900
Thank you.

1764
01:36:25,080 --> 01:36:26,550
Thank you so much.

1765
01:36:26,550 --> 01:36:27,090
Thank you.

1766
01:36:27,270 --> 01:36:28,170
Thank you.

1767
01:36:28,530 --> 01:36:29,700
Thank you so much.

1768
01:36:40,260 --> 01:36:41,850
Sorry, a quick question.

1769
01:36:42,180 --> 01:36:43,350
Okay.

1770
01:36:43,380 --> 01:36:47,220
One last question now for real,

1771
01:36:48,480 --> 01:36:53,880
oh, I I oh I was wondering actually for for logistics for the exam,

1772
01:36:54,630 --> 01:36:56,970
I emailed you.

1773
01:36:57,000 --> 01:37:00,600
Yeah, yeah, we haven't gotten to the point, yet,

1774
01:37:00,600 --> 01:37:02,430
we're dealing with the logistics of the example.

1775
01:37:02,460 --> 01:37:02,910
Okay.

1776
01:37:03,000 --> 01:37:07,440
A couple, aware of you two three [] to.

1777
01:37:07,530 --> 01:37:11,070
We have plan, we haven't executed yet,

1778
01:37:11,070 --> 01:37:12,690
noe shared any details.

1779
01:37:13,340 --> 01:37:13,910
Okay.

1780
01:37:14,060 --> 01:37:14,810
But it will happen.

1781
01:37:15,260 --> 01:37:16,730
Sounds good.

1782
01:37:16,970 --> 01:37:20,030
You don't need me to make sure you reach out.

1783
01:37:20,030 --> 01:37:25,040
Alright, perfect,

1784
01:37:25,040 --> 01:37:26,750
thank you so much for everything,

1785
01:37:26,780 --> 01:37:28,970
for the class and you know for the TAs.

1786
01:37:30,740 --> 01:37:33,970
Thank you very much for all the class,

1787
01:37:33,970 --> 01:37:37,530
so very fun I learned a lot.

1788
01:37:38,060 --> 01:37:41,180
Thank you, thank you for participating, asking all these questions,

1789
01:37:41,300 --> 01:37:41,870
I appreciate it.

1790
01:37:44,080 --> 01:37:44,620
Yes, thank you,

1791
01:37:44,620 --> 01:37:45,790
this is this is an awesome class,

1792
01:37:45,790 --> 01:37:46,540
I really appreciate.

1793
01:37:47,860 --> 01:37:50,710
Things were being active during the class.

1794
01:37:53,920 --> 01:37:55,840
Okay, I guess that's probably,

1795
01:37:55,870 --> 01:37:59,440
so I guess let's stop the recording.

