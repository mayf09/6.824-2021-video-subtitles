1
00:00:00,060 --> 00:00:00,600
Works again.


2
00:00:00,810 --> 00:00:03,570
Okay, so the plan for today is talking about GFS.


3
00:00:03,600 --> 00:00:05,880
I'm gonna do it in sort of multiple steps,


4
00:00:06,330 --> 00:00:09,840
one I'm gonna talk a little bit about storage in general


5
00:00:09,840 --> 00:00:13,260
and why it's so important


6
00:00:13,680 --> 00:00:17,130
and why we spend a lot of time in this class talking about it.


7
00:00:17,930 --> 00:00:25,280
Then I'm going to talk a little bit about in terms of GFS and it's main design,


8
00:00:25,460 --> 00:00:30,530
we'll focus on consistency which will be the main theme through this lecture,


9
00:00:31,100 --> 00:00:32,750
as part of the consistency,


10
00:00:32,750 --> 00:00:34,730
you know we probably I do,


11
00:00:35,390 --> 00:00:37,040
hopefully there's still time to breakout of the room


12
00:00:37,040 --> 00:00:41,290
and talk a little bit about the breakout rooms about the lecture


13
00:00:41,290 --> 00:00:43,240
or the question was posed for lecture


14
00:00:43,240 --> 00:00:49,060
and we'll resume discussion of of consistency.


15
00:00:50,920 --> 00:00:52,510
Okay, so.


16
00:00:53,100 --> 00:00:55,590
Let me talk a little bit about storage systems in general


17
00:00:56,220 --> 00:01:01,530
and why they so feature so prominently in 6.824


18
00:01:02,280 --> 00:01:06,870
and the main reason is it's just a fantastic building block


19
00:01:07,510 --> 00:01:11,800
for fault tolerance systems.


20
00:01:13,840 --> 00:01:15,490
And so the basic idea is that


21
00:01:15,490 --> 00:01:21,610
if you if you can build an durable storage system,


22
00:01:21,730 --> 00:01:28,720
then you can structure sort of your application as you know the app is basically you know state less,


23
00:01:31,680 --> 00:01:35,520
and then the storage holds all the persistent state.


24
00:01:39,660 --> 00:01:42,540
And that simplifies the design of the app tremendously,


25
00:01:43,270 --> 00:01:48,490
because the app basically you know doesn't really have any stable storage,


26
00:01:48,490 --> 00:01:50,140
you know that it has to maintain itself,


27
00:01:50,140 --> 00:01:52,720
in fact is a fact that all out to storage system,


28
00:01:53,020 --> 00:01:56,500
so you can start up a new application very quickly,


29
00:01:56,500 --> 00:01:58,030
you know it crash doesn't really matter,


30
00:01:58,030 --> 00:01:59,380
because it only has soft state,


31
00:01:59,380 --> 00:02:00,760
not any hard state,


32
00:02:00,970 --> 00:02:02,050
and then start up again,


33
00:02:02,050 --> 00:02:05,470
when you start up again, you just reach state from the distributed storage system.


34
00:02:06,340 --> 00:02:09,070
And you can see this quick look at any website,


35
00:02:09,070 --> 00:02:10,570
you know basically structured in that way,


36
00:02:10,720 --> 00:02:12,310
there's a storage backend,


37
00:02:12,310 --> 00:02:14,440
you know that means a state


38
00:02:14,440 --> 00:02:16,480
and then there's the application,


39
00:02:16,870 --> 00:02:20,380
middle tier that does the application specific competition


40
00:02:20,380 --> 00:02:22,780
or whatever runs javascript Go or whatever,


41
00:02:22,960 --> 00:02:27,760
and you know it front ends that out you know to a client on the Internet.


42
00:02:28,280 --> 00:02:32,810
And, so stores just like this fantastic building block


43
00:02:32,810 --> 00:02:37,040
and I think this is one reason that we're going to see this over and over in this class.


44
00:02:37,700 --> 00:02:40,490
That means that you know the storage system itself,


45
00:02:40,490 --> 00:02:42,170
of course has to be highly fault tolerant


46
00:02:42,380 --> 00:02:44,660
and as it turns out to be a very tricky thing to do


47
00:02:44,660 --> 00:02:48,290
and so that is the other side,


48
00:02:48,290 --> 00:02:49,130
the flip side of it,


49
00:02:49,220 --> 00:02:50,960
it will make life of the application easy,


50
00:02:50,960 --> 00:02:54,860
but you know the designing actually fault tolerant storage system is not easy.


51
00:02:55,710 --> 00:02:56,850
So why is it hard


52
00:03:00,480 --> 00:03:07,200
and basically come down to you know one reason that drives these designs


53
00:03:07,200 --> 00:03:09,330
which is like we generally want high performance


54
00:03:09,990 --> 00:03:12,330
when you think about the storage system for today,


55
00:03:12,450 --> 00:03:17,160
you know GFS its main goal is to basically support mapreduce types applications,


56
00:03:17,160 --> 00:03:20,190
and so, it really needs high performance.


57
00:03:20,580 --> 00:03:21,960
Well, what does that mean,


58
00:03:21,990 --> 00:03:28,890
well it means that you typically have to shard data across servers,


59
00:03:30,660 --> 00:03:32,790
so you cant't use one server,


60
00:03:32,790 --> 00:03:33,840
you have to use multiple servers


61
00:03:34,260 --> 00:03:37,020
and the reason you want to read from the disk


62
00:03:37,020 --> 00:03:40,290
and often particular machine has limited throughput,


63
00:03:40,470 --> 00:03:43,770
if you want to read more than actually a single disk can sustain,


64
00:03:43,770 --> 00:03:44,880
you have to use multiple disks


65
00:03:44,880 --> 00:03:46,470
and you have to use multiple network cards


66
00:03:46,470 --> 00:03:53,130
and so you can immediately get into these sort of large scale systems in the GFS where you have thousands of machines.


67
00:03:54,560 --> 00:03:55,850
But if you have many servers,


68
00:03:58,070 --> 00:03:59,060
some are gonna fail,


69
00:04:00,790 --> 00:04:01,930
you gonna get failures,


70
00:04:02,500 --> 00:04:06,160
more than maybe just to save more exclusively have constant faults


71
00:04:08,710 --> 00:04:13,060
and you know let's say the computer crashes, you know once a year,


72
00:04:13,180 --> 00:04:17,050
you know, now let's say you know you have thousands of machines,


73
00:04:17,050 --> 00:04:18,940
like in the GFS paper,


74
00:04:18,970 --> 00:04:20,440
so actually many more in the thousand machines,


75
00:04:20,440 --> 00:04:21,910
at minimal thousand machines,


76
00:04:22,320 --> 00:04:25,050
how many failures are going to see per day, roughly.


77
00:04:29,210 --> 00:04:30,440
Around 3.


78
00:04:30,680 --> 00:04:31,850
Yeah, around 3, right,


79
00:04:31,850 --> 00:04:37,820
that means that you know sort of a failure of a computer like my laptop at the beginning of the lecture,


80
00:04:37,970 --> 00:04:41,120
is just a common scenario


81
00:04:41,240 --> 00:04:45,050
and so if you're going to move up to more than a thousand, ten thousand machines,


82
00:04:45,050 --> 00:04:46,280
a hundred thousand machines,


83
00:04:46,460 --> 00:04:48,950
you're running applications using that kind of number of computers,


84
00:04:49,130 --> 00:04:50,120
you're gonna get faults


85
00:04:50,120 --> 00:04:53,450
and so that means you want a fall tolerance design.


86
00:05:01,260 --> 00:05:03,210
And you know to get fault tolerance,


87
00:05:03,210 --> 00:05:06,480
you know at least in the case where storage system with the typical approach the way to go,


88
00:05:06,480 --> 00:05:11,790
we're gonna go with replication copy data on multiple disks,


89
00:05:12,060 --> 00:05:13,350
you know so that they what this fails,


90
00:05:13,350 --> 00:05:15,930
you know the other disk hopefully have the data.


91
00:05:17,770 --> 00:05:22,510
But if you go into replication and so the data is in multiple places,


92
00:05:23,060 --> 00:05:26,720
that runs into the challenge that the data may be out of sync


93
00:05:26,960 --> 00:05:30,770
and so you actually get into inconsistent potential inconsistencies.


94
00:05:40,020 --> 00:05:42,390
You know, to avoid these inconsistencies,


95
00:05:42,540 --> 00:05:44,940
you know if you desire a strong consistency,


96
00:05:44,940 --> 00:05:50,580
basically your replicate system behaves as if it's it has the same behavior as an unreplicated system.


97
00:05:51,180 --> 00:05:55,290
Then you will need some persistency protocol.


98
00:05:57,600 --> 00:06:02,280
And that's gonna require some maybe sending messages and may be lower performance.


99
00:06:05,870 --> 00:06:09,230
Maybe the messages themselves, not really huge performance overhead,


100
00:06:09,230 --> 00:06:10,730
but you know [] knowledge we'll see


101
00:06:10,910 --> 00:06:15,320
and you might actually have to read or write to durable storage as part of that protocol


102
00:06:15,650 --> 00:06:19,220
and you know reading or writing to storage intentionally tends to be quite expensive.


103
00:06:19,740 --> 00:06:21,450
So here we see this conundrum,


104
00:06:21,450 --> 00:06:23,340
like you know we want high performance,


105
00:06:23,640 --> 00:06:24,960
we want fault tolerance,


106
00:06:24,960 --> 00:06:26,280
because we have many servers,


107
00:06:26,610 --> 00:06:29,130
we want high performance over many servers,


108
00:06:29,130 --> 00:06:30,900
many servers means fault tolerance,


109
00:06:31,020 --> 00:06:34,920
that means application that means you know inconsistencies,


110
00:06:34,950 --> 00:06:37,320
because we have data in multiple places,


111
00:06:37,320 --> 00:06:41,370
to fix the inconsistencies we need to get a protocol that might lower performance.


112
00:06:41,700 --> 00:06:46,440
So here sort of this fundamental challenge in designing these distributed storage systems,


113
00:06:46,440 --> 00:06:51,090
that user struggle between consistency and performance


114
00:06:51,150 --> 00:06:53,520
and we'll see that throughout the term.


115
00:06:55,840 --> 00:06:58,090
So let me talk a little bit about consistency.


116
00:07:02,400 --> 00:07:03,630
And there's a very high level


117
00:07:03,780 --> 00:07:06,570
and I promise you for the rest of this semester,


118
00:07:06,570 --> 00:07:09,240
we're going more detail as we go.


119
00:07:09,750 --> 00:07:12,690
So first let's talk again about the ideal consistency,


120
00:07:13,260 --> 00:07:19,530
an ideal consistency the way the simplest way to think about is that basically the machine behaves as if it's a single system,


121
00:07:28,770 --> 00:07:30,540
that's also the desired behavior


122
00:07:30,570 --> 00:07:37,350
and the sort of two things that make the desired behavior or two hazards


123
00:07:37,350 --> 00:07:40,890
that make this design behavior hard to achieve,


124
00:07:41,610 --> 00:07:44,580
or you know at least you know requires some thinking


125
00:07:44,910 --> 00:07:47,550
and one is concurrency and second thing is failures.


126
00:07:48,350 --> 00:07:50,450
So let me start with consumer concurrency,


127
00:07:50,450 --> 00:07:55,280
because even if you have a single machine with multiple clients,


128
00:07:55,280 --> 00:07:57,200
so your concurrency within a single machine,


129
00:07:57,320 --> 00:07:59,090
you actually have to think about consistency


130
00:07:59,390 --> 00:08:01,310
and this cause is quite obvious,


131
00:08:01,310 --> 00:08:04,520
let's say we have one machine, one disk,


132
00:08:04,790 --> 00:08:08,090
both of requests you know come in from different clients.


133
00:08:08,490 --> 00:08:10,140
And if the machine is a multi-processor machine,


134
00:08:10,140 --> 00:08:13,680
they might actually run these requests internally in parallel.


135
00:08:14,240 --> 00:08:17,120
So, so let's think a little bit about it,


136
00:08:17,120 --> 00:08:17,840
what does it mean,


137
00:08:17,840 --> 00:08:21,200
so so let's say we have client 1,


138
00:08:21,620 --> 00:08:27,380
that does a write operation to key x and write 1


139
00:08:28,190 --> 00:08:32,570
and at the same time, there's a request coming in with writes to x too,


140
00:08:32,570 --> 00:08:35,320
but actually writes the value 2, right.


141
00:08:35,320 --> 00:08:39,280
Now, you know if you want to specify or stage what consistency means,


142
00:08:39,280 --> 00:08:42,070
we need some rule about like what will happen,


143
00:08:42,460 --> 00:08:45,130
in the rules typically phrased from the perspective of the reader,


144
00:08:45,160 --> 00:08:47,080
so let's say there's another reader coming in,


145
00:08:47,980 --> 00:08:51,580
where another request coming in from another client and actually read x.


146
00:08:53,460 --> 00:08:57,840
The question is what is the value that actually that reader or that client observes.


147
00:08:58,460 --> 00:09:00,830
And look a little bit more complicated or more interesting,


148
00:09:00,830 --> 00:09:02,300
let's say we have four clients


149
00:09:02,300 --> 00:09:05,600
which we're going to bring out this issue of consistency definitions more clearly,


150
00:09:05,840 --> 00:09:07,520
and it also does a read of x,


151
00:09:07,580 --> 00:09:11,090
well after you know the client 3 actually read x.


152
00:09:12,580 --> 00:09:13,870
So now we have some state,


153
00:09:13,870 --> 00:09:16,390
what is desired outcomes, what are incorrect outcomes


154
00:09:16,390 --> 00:09:19,030
and that's really what defines consistency.


155
00:09:19,670 --> 00:09:22,490
So, let's take the first case,


156
00:09:22,490 --> 00:09:28,070
c3 you know what be a reasonable outcome for read c, read,


157
00:09:28,280 --> 00:09:31,010
what is reasonable outcome for the reader of 3 to return.


158
00:09:31,580 --> 00:09:32,390
What values.


159
00:09:33,660 --> 00:09:37,020
You know what value would make you happy or make an application programmer happy.


160
00:09:38,850 --> 00:09:39,420
2.


161
00:09:39,960 --> 00:09:41,340
2 be very reasonable?


162
00:09:42,100 --> 00:09:44,080
Any other reasonable values?


163
00:09:44,820 --> 00:09:45,390
1.


164
00:09:45,720 --> 00:09:46,980
Yeah, 1 would be reasonable,


165
00:09:47,280 --> 00:09:49,170
because the operation happened concurrently,


166
00:09:49,170 --> 00:09:50,970
so maybe we don't really know which one,


167
00:09:50,970 --> 00:09:53,640
we don't really want to restrict what particular order they go,


168
00:09:53,670 --> 00:09:56,400
so we gone say like either one to fine, because run concurrently.


169
00:09:57,350 --> 00:10:01,770
What are some values that we would like not to see, ror the c3 read.


170
00:10:02,410 --> 00:10:03,100
7.


171
00:10:03,370 --> 00:10:04,960
Yeah, 7, any other value, right,


172
00:10:04,990 --> 00:10:06,310
because nobody wrote that,


173
00:10:06,370 --> 00:10:07,450
so that would be undesirable.


174
00:10:08,210 --> 00:10:08,750
Okay, so good,


175
00:10:08,750 --> 00:10:13,580
so like we agree that probably used the original outcome for c3 would be either 1 or 2.


176
00:10:14,240 --> 00:10:15,440
Okay, how about c4?


177
00:10:17,520 --> 00:10:18,900
The same as c3.


178
00:10:19,580 --> 00:10:21,260
Really, exactly the same?


179
00:10:23,270 --> 00:10:25,070
So, let's say c3 return 1,


180
00:10:25,620 --> 00:10:27,330
what do we expect c4 to return?


181
00:10:28,490 --> 00:10:29,960
Whatever c3 saw.


182
00:10:30,230 --> 00:10:32,990
Yeah, because it run after c3, right.


183
00:10:32,990 --> 00:10:36,640
Yeah, if 1 was returned, we expect 1 here too.


184
00:10:37,640 --> 00:10:39,980
If 2 was returned, we expect 2 here.


185
00:10:41,800 --> 00:10:42,580
Does that make sense?


186
00:10:45,320 --> 00:10:49,700
Okay, so, you know there's like super brief you know sort of introduction


187
00:10:49,700 --> 00:10:52,730
of saying how can we define consistency


188
00:10:52,730 --> 00:10:55,010
and typically we do this using sort of traces


189
00:10:55,010 --> 00:10:58,100
and we argue about what correctness for particular traces


190
00:10:58,100 --> 00:10:59,210
and we will see more of that.


191
00:10:59,940 --> 00:11:05,070
Of course, you know the server can enforce you know this kind of concurrency by for example using locks,


192
00:11:05,070 --> 00:11:07,620
if you have done if you do mapreduce,


193
00:11:07,620 --> 00:11:09,930
you know of any concurrent Go programming you're write,


194
00:11:10,170 --> 00:11:17,910
that sort of the standard technique to enforce consistency in terms of, in the presence of concurrency is to use locks.


195
00:11:20,720 --> 00:11:24,140
In a distributed system, you know the ideal consistency,


196
00:11:24,140 --> 00:11:28,040
you know sort of similar two hazards and the second hazard is basically failure,


197
00:11:28,040 --> 00:11:29,510
so just replication in general,


198
00:11:29,540 --> 00:11:31,490
look if we have two servers [now] [assembly],


199
00:11:31,790 --> 00:11:35,000
so here S1, here S2,


200
00:11:36,440 --> 00:11:38,510
and you know both have a disk,


201
00:11:40,400 --> 00:11:42,830
and we have our same clients as before,


202
00:11:42,830 --> 00:11:44,360
C1 and C2


203
00:11:44,360 --> 00:11:46,520
and they write you know to x,


204
00:11:47,380 --> 00:11:52,810
and you know let's say you know just to illustrate what kind of complication,


205
00:11:52,810 --> 00:11:56,410
what kind of, illustrate that we have to do something,


206
00:11:56,410 --> 00:12:01,240
let's start with the most you know dumb, a replication plan,


207
00:12:01,240 --> 00:12:03,220
so what very bad replication.


208
00:12:11,240 --> 00:12:14,120
So this particular bad replication kind of plan what we're gonna do is,


209
00:12:14,120 --> 00:12:18,740
like, yeah we're gonna allow a client, when a client actually wants to update or write,


210
00:12:19,040 --> 00:12:22,340
we're gonna tell it to basically you know the protocol,


211
00:12:22,340 --> 00:12:25,100
that we're going to follow is to client write to both servers


212
00:12:25,250 --> 00:12:28,880
in you know whatever, don't really coordinate just write to both.


213
00:12:29,900 --> 00:12:33,320
And so exactly we have client 1, client 2 running,


214
00:12:33,560 --> 00:12:37,220
you know then you know may be client 2 does the same thing,


215
00:12:39,520 --> 00:12:43,900
and, and then we're going to ask ourselves the same question,


216
00:12:43,900 --> 00:12:48,580
what does you know C3 see what actually read


217
00:12:48,730 --> 00:12:52,090
and let's assume that for reading I'm gonna say like, we're either way,


218
00:12:52,970 --> 00:12:54,800
we're gonna read from any replica,


219
00:12:54,800 --> 00:12:57,170
as I said, is a very bad replication plan,


220
00:12:57,170 --> 00:12:58,670
basically there's no restrictions.


221
00:13:00,080 --> 00:13:01,610
So what are the possible outcomes.


222
00:13:02,650 --> 00:13:06,190
So this guy writes 1, this guy writes 2


223
00:13:06,670 --> 00:13:08,530
and you know we C3.


224
00:13:10,080 --> 00:13:11,820
What are the possible outcomes for C3?


225
00:13:15,130 --> 00:13:16,300
1 and 2, again.


226
00:13:16,540 --> 00:13:18,880
Yeah, 1 and 2, that really bad happens.


227
00:13:19,910 --> 00:13:20,870
How about C4?


228
00:13:22,500 --> 00:13:28,200
We do a read of x, well after C3 x, like this in the previous board.


229
00:13:28,610 --> 00:13:29,810
Also 1 and 2.


230
00:13:30,630 --> 00:13:32,490
Yeah, 1 and 2 again,


231
00:13:33,030 --> 00:13:39,740
what happens C3 reads 1, but C4 may return.


232
00:13:41,130 --> 00:13:42,030
1 or 2.


233
00:13:42,150 --> 00:13:44,490
1 or 2, it's now what we want or not.


234
00:13:45,750 --> 00:13:46,230
No.


235
00:13:46,770 --> 00:13:48,270
No, I mean you know again,


236
00:13:48,270 --> 00:13:51,270
it would be difficult for an application writer to actually read about this,


237
00:13:53,260 --> 00:13:55,570
you know particularly, the new c3 and c4 were the same thing


238
00:13:55,570 --> 00:13:56,920
you first need to teach one,


239
00:13:56,920 --> 00:13:59,770
no modification makes the next second returns another value,


240
00:13:59,770 --> 00:14:00,610
how is that possible.


241
00:14:01,170 --> 00:14:03,510
And it make the application program much difficult to write.


242
00:14:04,500 --> 00:14:08,760
And so you know the reason of course that this inconsistency shows up here,


243
00:14:08,880 --> 00:14:14,490
because we basically have no protocol to coordinate you know the clients the readers and the writers,


244
00:14:14,520 --> 00:14:16,470
so we need some form of system,


245
00:14:16,470 --> 00:14:18,960
typically we need some form of vertical to fix these


246
00:14:18,960 --> 00:14:23,370
and get the desire to enforce that we get the desired consistency.


247
00:14:24,840 --> 00:14:26,640
So we can see in rest of the semester,


248
00:14:26,640 --> 00:14:28,230
a whole bunch of potential protocols,


249
00:14:28,230 --> 00:14:31,140
they have different trade-offs in terms of fault tolerance and consistency.


250
00:14:32,340 --> 00:14:32,970
Okay?


251
00:14:33,960 --> 00:14:38,250
And in fact you should get into get our head in that kind of thinking,


252
00:14:38,550 --> 00:14:43,230
we're we're gonna use a whole bunch of different case studys


253
00:14:43,380 --> 00:14:47,400
and the case study today is GFS.


254
00:14:54,970 --> 00:14:56,590
And this is an interesting case study,


255
00:14:56,620 --> 00:14:59,050
why we would have signed it


256
00:14:59,050 --> 00:15:00,850
and one reason is an interesting case study,


257
00:15:00,850 --> 00:15:04,000
because it brings out all these core issue,


258
00:15:04,060 --> 00:15:07,720
GFS design designed to get high performance.


259
00:15:10,240 --> 00:15:19,280
Yeah, that means it actually uses replication and fault tolerance.


260
00:15:21,730 --> 00:15:25,510
And, you know it struggles with this consistency,


261
00:15:25,540 --> 00:15:27,250
so it's like a few [for] sort of [themes],


262
00:15:27,250 --> 00:15:29,830
that we're going to be consistently seeing it throughout the semester,


263
00:15:29,830 --> 00:15:31,600
will show up in this one paper.


264
00:15:32,960 --> 00:15:36,110
The other side of this why is interesting case study,


265
00:15:36,110 --> 00:15:37,010
because it is a successful system,


266
00:15:41,030 --> 00:15:46,850
Google does actually use GFS at at least at this point in my understanding,


267
00:15:46,850 --> 00:15:49,310
there's a successor file system called [],


268
00:15:49,310 --> 00:15:51,560
you know but it's inspired by GFS.


269
00:15:51,980 --> 00:15:57,620
In, but there are other sort of cluster-based file systems,


270
00:15:57,620 --> 00:16:00,680
you know for like mapreduce type used HDFS,


271
00:16:00,740 --> 00:16:03,530
you know also very much inspired by the design of GFS.


272
00:16:04,620 --> 00:16:08,400
And you know one thing that is actually interesting,


273
00:16:08,400 --> 00:16:10,440
at the point that this paper was written,


274
00:16:10,530 --> 00:16:14,100
in sort of late to 2000,


275
00:16:14,100 --> 00:16:17,160
it was pretty distributed file system and were well understood topics,


276
00:16:17,220 --> 00:16:18,630
so people knew about tolerance,


277
00:16:18,630 --> 00:16:20,070
but they knew about replication,


278
00:16:20,070 --> 00:16:21,900
people knew about consistency,


279
00:16:21,930 --> 00:16:24,660
all that kind of stuff were pretty well understood.


280
00:16:25,210 --> 00:16:31,660
However, nobody actually built you know system you know at the scale of thousands of computers


281
00:16:31,810 --> 00:16:36,250
and that sure brings out a number of challenges,


282
00:16:36,430 --> 00:16:38,560
that previous system have to not address


283
00:16:38,680 --> 00:16:42,490
and the effect the design is not completely standard.


284
00:16:43,150 --> 00:16:45,700
So the design that we're reading about,


285
00:16:45,700 --> 00:16:47,980
it's not was not sort of the standard design,


286
00:16:47,980 --> 00:16:50,020
that you would see in academic papers at that time,


287
00:16:50,920 --> 00:16:53,290
there were two aspects of making non standard,


288
00:16:53,620 --> 00:16:56,650
[] which will get more we'll spend more time on,


289
00:16:56,740 --> 00:16:59,350
one is you know there's actually a single master,


290
00:16:59,350 --> 00:17:02,190
the master, it's not replicated,


291
00:17:02,220 --> 00:17:08,830
there's a single machine that sort of charged with like almost all the coordination in the system


292
00:17:08,830 --> 00:17:10,750
and so that is unusual,


293
00:17:10,870 --> 00:17:14,800
you know why would you build file system fault tolerance system,


294
00:17:14,800 --> 00:17:20,230
which has a single point of failure is not something that people in the academic literature were doing at that time


295
00:17:21,070 --> 00:17:26,200
and the second thing is that, it has it's not consistent,


296
00:17:26,230 --> 00:17:28,240
you know it can have inconsistencies.


297
00:17:33,180 --> 00:17:37,080
And again mostly in literature in at that particular time,


298
00:17:37,080 --> 00:17:42,270
you know people were really sweating actually to build the distributed systems that actually have strong consistency


299
00:17:42,420 --> 00:17:45,810
and you don't have the anomalies that we saw on the previous board.


300
00:17:47,120 --> 00:17:53,030
Alright, and so so you know like a lot of the core techniques you were well known,


301
00:17:53,150 --> 00:17:56,720
you know the way you were putting together essentially quite different.


302
00:17:58,150 --> 00:17:59,440
And so, that makes it interesting


303
00:17:59,560 --> 00:18:04,330
and particularly the scale which you know this system actually operates is impressive.


304
00:18:05,120 --> 00:18:06,860
And pretty common even for today,


305
00:18:06,860 --> 00:18:15,590
you know this issue a struggle between fault tolerance, replication performance and consistency


306
00:18:15,590 --> 00:18:19,880
is standard problem, recurring problems for almost any distributed storage system,


307
00:18:19,880 --> 00:18:21,050
so that people built today.


308
00:18:22,660 --> 00:18:23,650
It changes over time,


309
00:18:23,680 --> 00:18:24,970
like [] for a while,


310
00:18:24,970 --> 00:18:27,250
you know they really have that strong consistency,


311
00:18:27,250 --> 00:18:29,200
lately it has gotten much stronger consistency.


312
00:18:30,990 --> 00:18:35,100
Okay, so what, since the paper is really driven


313
00:18:35,340 --> 00:18:39,570
and the design is driven by [] by performance,


314
00:18:39,600 --> 00:18:44,300
I wanted to go back to the mapreduce paper for second


315
00:18:44,330 --> 00:18:47,240
and this is a graph of the mapreduce paper


316
00:18:47,570 --> 00:18:55,550
and one way to think about GFS is that it's the file system for mapreduce.


317
00:19:00,530 --> 00:19:04,580
And so the goal is to actually run many mapreduce jobs and get high performance.


318
00:19:05,250 --> 00:19:07,200
And we know that basically from,


319
00:19:07,200 --> 00:19:11,640
we could tell from the mapreduce paper already that GFS is impressive,


320
00:19:11,640 --> 00:19:13,350
in that manner in terms of performance,


321
00:19:13,680 --> 00:19:17,940
so if you look at the you know this side of this graph,


322
00:19:17,940 --> 00:19:19,950
this is straight out of the mapreduce paper,


323
00:19:20,160 --> 00:19:23,880
this is the normal execution of one of the mapreduce jobs,


324
00:19:24,120 --> 00:19:27,690
and get it has three parts to it,


325
00:19:27,690 --> 00:19:29,100
one is the first part input,


326
00:19:29,100 --> 00:19:30,810
like reading the input files,


327
00:19:30,810 --> 00:19:33,450
the inputs to the map from the file system


328
00:19:33,450 --> 00:19:35,280
and in case you know they didn't say much about it,


329
00:19:35,280 --> 00:19:37,980
but those are written read from GFS.


330
00:19:39,320 --> 00:19:41,300
There's the internal shuffle that we really care about,


331
00:19:41,300 --> 00:19:46,340
and then at the end you know the reduce jobs write back the results into GFS.


332
00:19:47,200 --> 00:19:51,670
And, and so, do performance, you know part of the performance,


333
00:19:51,670 --> 00:19:54,520
there's mapreduce task is able to determined by you know the reader,


334
00:19:54,520 --> 00:20:00,220
at which a the mappers can actually read you know data from GFS file system,


335
00:20:00,250 --> 00:20:02,590
make sure we're running many, many mappers at the same time,


336
00:20:02,590 --> 00:20:06,520
in fact some mappers from different jobs maybe reading the same files.


337
00:20:07,150 --> 00:20:08,710
So we look at the input,


338
00:20:08,740 --> 00:20:12,100
like this, this there's top you know graph,


339
00:20:12,280 --> 00:20:16,120
shows the input in terms of megabytes per second


340
00:20:16,420 --> 00:20:21,040
at the rate at which you know the mapper actually jointly collectively for one particular job,


341
00:20:21,040 --> 00:20:22,480
you can read from the file system.


342
00:20:23,240 --> 00:20:28,700
As you can see, you know it goes over, well over a thousand or 10 000 megabytes per second.


343
00:20:30,800 --> 00:20:32,480
And you know the first question to ask you is


344
00:20:32,480 --> 00:20:34,190
maybe not an impressive number.


345
00:20:38,180 --> 00:20:40,460
Should we be impressed with that number are thinking well.


346
00:20:41,520 --> 00:20:43,230
You know give me one disk and I do too.


347
00:20:50,500 --> 00:20:51,340
Anybody?


348
00:20:51,820 --> 00:20:54,790
I think because it's older, maybe yes.


349
00:20:56,000 --> 00:21:01,910
Okay, SSD, how much what rates can you write read?


350
00:21:08,220 --> 00:21:09,300
Okay, let me tell you this,


351
00:21:09,300 --> 00:21:17,540
roughly the throughput of a single disk at the time this, this paper was around like thirty megabytes per second,


352
00:21:17,540 --> 00:21:19,250
but somewhere in the tens of megabytes a second.


353
00:21:20,810 --> 00:21:26,980
So here we're looking at you know well over 10000 megabytes per second, correct,


354
00:21:26,980 --> 00:21:28,480
and so that is an impressive number.


355
00:21:29,540 --> 00:21:30,860
And you know you have to work


356
00:21:30,860 --> 00:21:34,520
as you know the seeing the GFS design that allows that kind of throughput.


357
00:21:36,260 --> 00:21:38,690
Of course, this technology indications GFS,


358
00:21:38,690 --> 00:21:40,520
of course this technology was faster,


359
00:21:40,700 --> 00:21:43,040
it would be you know what the real goal here correctly,


360
00:21:43,040 --> 00:21:44,480
it's like we have a thousand machines,


361
00:21:44,690 --> 00:21:46,100
maybe each one has a disk,


362
00:21:46,100 --> 00:21:48,200
each one read thirty megabytes per second,


363
00:21:48,260 --> 00:21:51,170
which is 1000 times thirty megabytes per second to get out of it.


364
00:21:51,800 --> 00:21:52,400
Okay?


365
00:21:53,410 --> 00:21:57,460
So that't what you know drives a lot of this design is


366
00:21:57,460 --> 00:22:03,880
to immediately allow the mappers to read in parallel from the file system, joint file system.


367
00:22:04,910 --> 00:22:05,480
Okay?


368
00:22:06,850 --> 00:22:08,860
Let me say a little bit more about this,


369
00:22:08,860 --> 00:22:12,250
about what are the key properties that GFS has,


370
00:22:12,610 --> 00:22:19,860
you know one big large data sets remember that, I mean that.


371
00:22:23,900 --> 00:22:27,380
And so in the think the data set you should think about is like mapreduce data sets,


372
00:22:27,380 --> 00:22:32,240
you're going to have a complete crawl the world wide web,


373
00:22:32,270 --> 00:22:34,340
is stored in this distributed file system,


374
00:22:35,460 --> 00:22:37,140
has to be fast,


375
00:22:37,140 --> 00:22:41,820
we talked about the way they get like high performance is to do automatic sharding,


376
00:22:43,730 --> 00:22:45,620
shard files across multiple disks,


377
00:22:45,830 --> 00:22:48,890
allow multiple clients to read from those disks comparable.


378
00:22:49,600 --> 00:22:50,080
Alright?


379
00:22:51,710 --> 00:22:59,030
It goes global, what that mean it's shared where again all apps,


380
00:23:01,400 --> 00:23:02,690
see same file system.


381
00:23:06,190 --> 00:23:07,000
And that's convenient,


382
00:23:07,000 --> 00:23:09,940
like if you have multiple mapreduce jobs,


383
00:23:09,940 --> 00:23:12,850
you know that operate on the same set of files,


384
00:23:13,090 --> 00:23:16,000
they can, first of all read all the same set of files,


385
00:23:16,000 --> 00:23:17,410
but they can produce new files


386
00:23:17,410 --> 00:23:19,780
and another mapreduce you can use those files again


387
00:23:19,810 --> 00:23:23,440
and so it's very convenient to have a lot of sharing between applications,


388
00:23:23,440 --> 00:23:24,520
so it's very convenient to have.


389
00:23:26,000 --> 00:23:27,920
Of course, you know the GFS has to be fault tolerant.


390
00:23:33,710 --> 00:23:35,210
It's likely they're gonna be failures,


391
00:23:35,210 --> 00:23:37,040
it's what we want like automatic,


392
00:23:37,990 --> 00:23:40,000
close to automatic fault tolerance as possible,


393
00:23:40,000 --> 00:23:42,040
you'll see GFS doesn't provide completely automatic,


394
00:23:42,040 --> 00:23:45,370
but do a pretty good job with getting high with fault tolerance.


395
00:23:48,160 --> 00:23:52,120
Okay, any questions about the, this part so far,


396
00:23:52,540 --> 00:23:55,030
like a broad intro to this topic,


397
00:23:55,030 --> 00:23:59,020
then a few intro awards about GFS.


398
00:24:04,330 --> 00:24:04,900
Okay.


399
00:24:05,420 --> 00:24:06,650
Let's then talk about the design.


400
00:24:13,940 --> 00:24:20,990
So here's the design is seen in from the figure one I think in the paper,


401
00:24:21,320 --> 00:24:25,010
and there's a couple things I wanted to point out that a little bit more in detail about.


402
00:24:25,480 --> 00:24:27,280
So, first of all you know, we have an application


403
00:24:27,280 --> 00:24:28,810
and then you know the application again,


404
00:24:28,840 --> 00:24:34,150
you know might be mapreduce [] consuming multiple reduced task, multiple map task


405
00:24:34,450 --> 00:24:36,460
and they link with the GFS,


406
00:24:36,460 --> 00:24:39,940
like and so, it's not a Linux file system,


407
00:24:39,970 --> 00:24:45,550
you know this is not the file system, you used to you know whatever any files or compile,


408
00:24:45,580 --> 00:24:52,420
it is really intended, you know as a special purpose file system for these large computations.


409
00:24:53,800 --> 00:24:58,240
And as I said before our real goal correctly achieves an impressive number


410
00:24:58,240 --> 00:25:01,870
like we want the number of megabytes from a single disk times number of machines


411
00:25:01,870 --> 00:25:04,180
and a single application should be able to exploit that.


412
00:25:04,840 --> 00:25:07,030
So the way they arrange that is to


413
00:25:07,210 --> 00:25:12,010
have a master that is basically in charge of actually know where things are


414
00:25:12,190 --> 00:25:15,190
and the clients just periodically talks to the master,


415
00:25:15,190 --> 00:25:18,670
to to retrieve information,


416
00:25:18,670 --> 00:25:20,710
so for example it opens the file


417
00:25:20,770 --> 00:25:25,660
and open call will result in a message to the master


418
00:25:25,660 --> 00:25:30,790
and a master will respond back and say like all the particular filename,


419
00:25:30,850 --> 00:25:34,330
the chunks that you need are here,


420
00:25:34,640 --> 00:25:36,260
well, these are the chunks that you need


421
00:25:36,530 --> 00:25:41,540
and there's a chunk handles identifier for the particular chunks that constitute a file,


422
00:25:41,690 --> 00:25:45,080
and here are the servers that sure that chunk,


423
00:25:45,110 --> 00:25:49,460
so you get back chunk handle as well as a bunch of chunk locations.


424
00:25:50,620 --> 00:25:53,410
And one file might you know basically file consists,


425
00:25:53,410 --> 00:25:55,060
if you think about a big file,


426
00:25:55,750 --> 00:26:06,580
it consists of many many chunks, chunk 0, chunk 1, chunk 2, etc, chunk 3 blah, blah, etc, etc,


427
00:26:06,580 --> 00:26:09,610
any chunk is pretty big, 64 megabytes.


428
00:26:12,060 --> 00:26:15,930
Naturally, the application once we you know second second second 64 megabyte,


429
00:26:15,930 --> 00:26:21,990
it goes to the GFS is like okay, I wanna read you the second chunk,


430
00:26:22,320 --> 00:26:24,450
you know this particular file


431
00:26:24,450 --> 00:26:28,800
and the GFS answer will answer back with the handle for chunk 1,


432
00:26:28,860 --> 00:26:32,470
as well as the servers that actually holds chunk 1, right.


433
00:26:33,910 --> 00:26:39,130
So multiple applications might ask you know for chunks from the same file,


434
00:26:39,250 --> 00:26:43,960
and they all get you know one application might be reading chunk 0,


435
00:26:43,960 --> 00:26:46,120
another application might be read chunk 2,


436
00:26:46,150 --> 00:26:48,880
they'll get different lists back from each of these chunks.


437
00:26:50,460 --> 00:26:53,640
So then the GFS client you know once it knows chunk locations


438
00:26:53,670 --> 00:26:56,280
and basically straight talks to the chunk servers,


439
00:26:57,670 --> 00:27:04,270
and basically read you know the data at the speed of the network,


440
00:27:04,270 --> 00:27:09,700
and you know maybe every disk you know that sits behind this particular chunk server, directly to the application.


441
00:27:10,480 --> 00:27:13,030
And here you can see where we're going to get the big win, right,


442
00:27:13,030 --> 00:27:14,500
because we're gonna be able to read,


443
00:27:14,500 --> 00:27:19,570
you know for multiple you know multiple clients can be read from multiple disks at the same time


444
00:27:19,840 --> 00:27:22,330
and we're going to get tremendous amount of performance,


445
00:27:22,540 --> 00:27:25,420
so general, means like here's map task running,


446
00:27:25,750 --> 00:27:27,400
here's another map tasks running,


447
00:27:27,400 --> 00:27:28,570
that also as a client,


448
00:27:28,870 --> 00:27:31,720
you know they you know going to be talking to the set of servers,


449
00:27:31,720 --> 00:27:34,900
we have that whole you know the chunk of all the collection the data set


450
00:27:35,020 --> 00:27:38,800
and there's gonna read in parallel from all those different chunk servers.


451
00:27:39,200 --> 00:27:43,190
And I was going to give us a high throughput number.


452
00:27:45,090 --> 00:27:47,670
That makes sense, that's sort of the overall plan clear here.


453
00:27:52,590 --> 00:27:53,820
Just to sort of completed,


454
00:27:53,820 --> 00:28:01,020
like on chunk server is nothing really, and sort of a Linux box, Linux computer with you know disk to it,


455
00:28:01,230 --> 00:28:03,240
in fact there's 64 megabyte chunk,


456
00:28:03,330 --> 00:28:06,660
each store as a Linux file in the Linux file system.


457
00:28:07,710 --> 00:28:08,280
Okay?


458
00:28:12,080 --> 00:28:14,360
Okay, so I want to zoom in on the different pieces


459
00:28:14,420 --> 00:28:15,920
and I'll start with the master,


460
00:28:15,920 --> 00:28:20,360
because masters are related to the control center here,


461
00:28:20,570 --> 00:28:24,230
so talk a little bit about the state that actually the master maintains.


462
00:28:29,340 --> 00:28:42,310
Okay, so first of all you know it has the mapping from file name to an array of of trunk handles.


463
00:28:49,040 --> 00:28:50,840
And as you saw in the paper, one of the goals,


464
00:28:50,840 --> 00:28:56,870
actually is to maintain all this memory of most of the information actually directly available in memory,


465
00:28:57,140 --> 00:29:00,770
so that master response to a client very quickly


466
00:29:00,770 --> 00:29:02,840
and the reason why reason to do that is because,


467
00:29:03,020 --> 00:29:05,510
now there's one master, many clients


468
00:29:05,600 --> 00:29:08,510
you want to execute every client operation as efficient as possible,


469
00:29:08,510 --> 00:29:12,050
so that you can scale the master to at least a reasonable number of clients.


470
00:29:13,940 --> 00:29:15,380
And then, for every trunk handle,


471
00:29:19,120 --> 00:29:21,280
the mask contains some additional number,


472
00:29:21,310 --> 00:29:23,590
can make particular, it maintains a version number,


473
00:29:24,630 --> 00:29:33,590
in a list of junk servers,


474
00:29:34,970 --> 00:29:38,000
that holds a copy of that chunk.


475
00:29:39,820 --> 00:29:41,050
And as we'll see in a second,


476
00:29:41,050 --> 00:29:43,450
you know one of them is named,


477
00:29:43,450 --> 00:29:45,160
one of those servers are primary


478
00:29:46,000 --> 00:29:47,770
and the other ones are secondary


479
00:29:50,260 --> 00:29:54,340
and the typical number that you know a chunk stored at 3 servers


480
00:29:54,640 --> 00:29:57,010
and we can maybe talk a little bit later about why 3.


481
00:29:58,010 --> 00:30:02,300
And then you know there is at least associated with each primary,


482
00:30:02,300 --> 00:30:04,310
so there's at least time maintained as well.


483
00:30:06,100 --> 00:30:09,970
Then there's two sort of other big storage components,


484
00:30:09,970 --> 00:30:12,940
and these are sort of the file system level things,


485
00:30:12,940 --> 00:30:16,510
and then in terms of implementation, there's a log,


486
00:30:17,370 --> 00:30:18,510
and there are checkpoints.


487
00:30:23,620 --> 00:30:27,910
Since the master of the crucial control center


488
00:30:27,940 --> 00:30:31,780
whenever there's change to the name space


489
00:30:31,780 --> 00:30:35,020
and potentially create a new file in the GFS


490
00:30:35,260 --> 00:30:39,430
or mapping the file to chunk blocks and changes,


491
00:30:39,430 --> 00:30:41,500
all those operations are written to this log


492
00:30:41,530 --> 00:30:44,350
and log sits on stable storage.


493
00:30:49,160 --> 00:30:51,200
And the basic idea is that


494
00:30:51,200 --> 00:30:54,050
like we were before responding to the client,


495
00:30:54,050 --> 00:30:58,130
the change actually has made the master writes to stable storage first,


496
00:30:58,620 --> 00:30:59,970
and then responds to the client,


497
00:31:00,120 --> 00:31:03,750
so this means that if the master fails or crashes,


498
00:31:03,990 --> 00:31:05,280
then later comes back up,


499
00:31:05,280 --> 00:31:09,780
it can replace log to reconstruct you know the state of its internal state,


500
00:31:10,840 --> 00:31:14,320
and by writing it first to a storage before responding to the client,


501
00:31:14,470 --> 00:31:16,360
the client will never observe strange results,


502
00:31:16,360 --> 00:31:19,330
you know you could do the other way around corrected,


503
00:31:19,330 --> 00:31:20,320
that result in a problem,


504
00:31:20,320 --> 00:31:22,870
because you know the client will think that the file has been created,


505
00:31:22,870 --> 00:31:25,450
server crash backup and then the file doesn't exist.


506
00:31:26,840 --> 00:31:30,080
So, these are another consistency point.


507
00:31:31,440 --> 00:31:35,070
You know replaying always back all the operations from the beginning of time to log is


508
00:31:35,070 --> 00:31:38,010
of course undesirable it means that if the master crashes


509
00:31:38,010 --> 00:31:40,830
and we have only one of them will be down for a long time,


510
00:31:41,100 --> 00:31:45,210
so in addition to that, you know it actually keeps checkpoints in stable storage,


511
00:31:47,250 --> 00:31:51,690
so periodically, the master makes a checkpoint of its own state


512
00:31:51,690 --> 00:31:54,900
and the mapping [] array chunk handles


513
00:31:55,320 --> 00:31:58,710
and stores that on on the stable storage


514
00:31:58,830 --> 00:32:01,500
and so then they only have to replay the last part,


515
00:32:01,530 --> 00:32:04,740
basically all the operations in the log after the last checkpoint,


516
00:32:04,830 --> 00:32:06,420
so the recovery is actually quickly.


517
00:32:08,220 --> 00:32:10,590
So, there's another couple interesting questions, that we can ask ourselves,


518
00:32:10,590 --> 00:32:14,370
like what state does need to end up in a stable storage,


519
00:32:14,370 --> 00:32:16,350
you know for the [massive] [extra] function correctly.


520
00:32:17,140 --> 00:32:18,640
So the first question to ask is,


521
00:32:18,700 --> 00:32:23,770
how about this array of chunk handles the mapping from filename to chunk handles,


522
00:32:23,770 --> 00:32:26,500
does that need to be stable stored,


523
00:32:27,320 --> 00:32:28,790
or can it be only in memory.


524
00:32:37,770 --> 00:32:47,650
If the master crashes, I think you can like get that information from the servers, chunk servers,


525
00:32:48,200 --> 00:32:51,390
so maybe only, may memory.


526
00:32:52,340 --> 00:32:55,250
Yeah, well that's answer the question, what other people think.


527
00:32:56,180 --> 00:32:58,340
So it can be reconstructed from the log,


528
00:32:58,370 --> 00:33:00,380
so, when the server crashes,


529
00:33:00,470 --> 00:33:03,470
only the log needs to be in the hard storage


530
00:33:03,710 --> 00:33:05,900
and then it can reload it from the log to main memory.


531
00:33:06,200 --> 00:33:07,880
Yeah, so definitely has to be in log,


532
00:33:07,880 --> 00:33:11,930
so we agree that this array of chunk handles basically has to be stored in stable storage,


533
00:33:16,220 --> 00:33:18,800
because otherwise we lose like we create a file,


534
00:33:18,830 --> 00:33:20,210
and we didn't write the storage,


535
00:33:20,210 --> 00:33:21,530
we just lose the file right,


536
00:33:21,530 --> 00:33:25,760
so this mapping from file name chunk handles need to be in a stable storage,


537
00:33:25,760 --> 00:33:30,290
how about this, chunk handle to chunk handle to list of chunk for our servers.


538
00:33:32,220 --> 00:33:33,930
Is that actually you need to be [].


539
00:33:35,580 --> 00:33:41,100
I think in the paper, they say that when the master reboots,


540
00:33:41,340 --> 00:33:48,570
it asks the servers to tell tell the master what the trunks that they have are.


541
00:33:48,810 --> 00:33:55,600
Yeah, so this is not actually this is basically just volatile state, not not stable storage.


542
00:33:56,080 --> 00:33:59,380
So same says only through the primaries and the secondaries,


543
00:33:59,880 --> 00:34:01,770
and through the least time.


544
00:34:02,630 --> 00:34:03,710
How about the version number?


545
00:34:10,740 --> 00:34:14,910
Does the master need to remember on stable storage version number or not?


546
00:34:16,410 --> 00:34:23,190
Yes, because it needs to know if the trunks in the other servers are scale or not.


547
00:34:23,780 --> 00:34:26,030
Yeah, exactly exactly right, right,


548
00:34:26,030 --> 00:34:27,890
so the master must remember version number,


549
00:34:27,890 --> 00:34:31,580
because if it doesn't


550
00:34:31,580 --> 00:34:33,350
and the whole system went down


551
00:34:33,770 --> 00:34:35,450
and the [] servers came back up


552
00:34:35,750 --> 00:34:39,590
and maybe the chunk server actually with the most recent data does not come up


553
00:34:39,590 --> 00:34:42,230
with an older guy comes up with version number fourteen,


554
00:34:42,620 --> 00:34:49,790
then the master has to be able to tell that you know that chunk server version of fourteen was not the most recent chunk server.


555
00:34:50,620 --> 00:34:53,710
And so it needs to maintain that version number on disk,


556
00:34:53,710 --> 00:34:58,960
so that actually can tell which chunk servers actually have the most updated information on which ones don't.


557
00:35:00,010 --> 00:35:00,550
Okay?


558
00:35:01,020 --> 00:35:02,430
A I have a question here,


559
00:35:03,600 --> 00:35:07,740
if, well, I mean if if the master failed


560
00:35:07,740 --> 00:35:09,120
and then it has to come up,


561
00:35:09,330 --> 00:35:12,540
it's anyway going to connect to all of the chunk servers


562
00:35:12,540 --> 00:35:17,560
and it will find out what the largest version is.


563
00:35:18,220 --> 00:35:21,760
Yeah, has ability to find out what the last,


564
00:35:21,760 --> 00:35:25,840
first of all, it will try to talk to all chunk servers,


565
00:35:26,110 --> 00:35:27,460
some chunk servers might be down.


566
00:35:28,040 --> 00:35:28,670
Okay.


567
00:35:29,120 --> 00:35:33,890
And that's that maybe just the chunk server that actually has the most recent version right.


568
00:35:34,220 --> 00:35:34,910
Yeah, okay.


569
00:35:35,390 --> 00:35:38,990
So you can't take the max of the life chunk servers,


570
00:35:39,480 --> 00:35:41,250
that be incorrect.


571
00:35:46,110 --> 00:35:47,400
Any other questions about this?


572
00:35:51,820 --> 00:35:54,490
Okay, let's look at the two sort of basic operations,


573
00:35:55,120 --> 00:35:57,280
to really get down to consistency


574
00:35:57,280 --> 00:35:59,710
and of course you know it's going to be reading and writing,


575
00:35:59,950 --> 00:36:00,940
so reading a file,


576
00:36:01,510 --> 00:36:02,950
and then we'll talk about writing a file,


577
00:36:04,060 --> 00:36:06,790
so reading files in some sense straightforward,


578
00:36:06,820 --> 00:36:15,150
we talked about basically a client send message you know to with the filename plus offset to the master,


579
00:36:16,320 --> 00:36:20,650
and basically ask please give me you know chunk servers


580
00:36:20,740 --> 00:36:29,110
and chunk handle that hold that hold the data at that offset,


581
00:36:29,850 --> 00:36:32,010
and so ends the chunk handle.


582
00:36:32,730 --> 00:36:36,270
So like read byte whatever zero


583
00:36:36,300 --> 00:36:42,010
you know it's pretty clear that has to be the first entry in the list,


584
00:36:42,010 --> 00:36:45,390
you know from filename to trunk handle.


585
00:36:46,130 --> 00:36:52,360
So [], the master chunk handle basically replies you know with the master,


586
00:36:52,390 --> 00:36:55,270
replies to the client with the chunk handle,


587
00:36:57,440 --> 00:37:06,080
and the chunk servers for that handle and version numbers.


588
00:37:09,000 --> 00:37:14,070
So basically the client gets back a message saying you know that's chunk you know 221


589
00:37:14,400 --> 00:37:17,340
you know and here are the three machines,


590
00:37:17,340 --> 00:37:19,560
IP address, the three machines that actually have it.


591
00:37:20,140 --> 00:37:22,960
And the version number is like version of 10.


592
00:37:26,400 --> 00:37:28,200
Then the client caches this list.


593
00:37:34,820 --> 00:37:46,290
And then it basically sends a message to the closest, reads from closest server.


594
00:37:51,380 --> 00:37:55,700
and so why this the client actually read cache, this information.


595
00:37:58,100 --> 00:38:00,590
Yes, we see later, correct, that caused some troubles.


596
00:38:01,400 --> 00:38:04,340
So it doesn't have to contact the master for some time,


597
00:38:04,340 --> 00:38:07,580
if it wants to read again or write to.


598
00:38:08,780 --> 00:38:10,130
Yeah why is that important?


599
00:38:11,610 --> 00:38:15,270
To reduce the, I guess the traffic,


600
00:38:15,450 --> 00:38:20,460
and in general it takes less time if you have less communication with the master.


601
00:38:20,610 --> 00:38:21,810
Yeah, and you know the more,


602
00:38:21,900 --> 00:38:23,640
you do the same with that's correct,


603
00:38:23,640 --> 00:38:26,040
this design is that the master actually a single machine,


604
00:38:26,980 --> 00:38:31,750
and as a single machine you can just have a limited amount of memory and limit network interface


605
00:38:31,750 --> 00:38:34,150
and so you have too many clients talking to,


606
00:38:34,150 --> 00:38:36,340
it wouldn't be able to serve, right,


607
00:38:36,340 --> 00:38:39,970
so client caching is important to reduce the load on this single machine.


608
00:38:41,730 --> 00:38:44,010
Okay, why read from the closest server?


609
00:38:46,860 --> 00:38:48,300
Minimize network traffic.


610
00:38:48,660 --> 00:38:49,920
Yeah, minimize network traffic,


611
00:38:49,920 --> 00:38:51,510
you know, so the whole goal correct is


612
00:38:51,510 --> 00:38:55,500
to pump as much data to client as possible, the highest throughput,


613
00:38:55,890 --> 00:39:01,320
and you know you have to, there's two problems we had to cross with data center network,


614
00:39:01,320 --> 00:39:04,320
one you know where there's probably some topology


615
00:39:04,320 --> 00:39:07,530
and maybe swamps like the top links of topology,


616
00:39:07,980 --> 00:39:12,870
and may actually increase latency to actually get to the other side.


617
00:39:13,520 --> 00:39:15,800
Actually, it's important to be able to be to the closest side


618
00:39:15,800 --> 00:39:20,840
again to basically maximize you know the throughput you know that joint set of clients,


619
00:39:20,840 --> 00:39:25,220
you can sort of experience when they're reading in parallel from many many chunk servers.


620
00:39:26,420 --> 00:39:26,990
Okay?


621
00:39:28,110 --> 00:39:32,640
So the chunk server S you know better check the version number.


622
00:39:35,570 --> 00:39:37,190
And if the version number is okay,


623
00:39:37,280 --> 00:39:38,570
you know then send data.


624
00:39:42,570 --> 00:39:43,170
Okay?


625
00:39:44,510 --> 00:39:46,190
Why is check the version number there?


626
00:39:50,580 --> 00:39:52,530
To check if it's too stale.


627
00:39:52,770 --> 00:39:58,010
Yeah, we do our best to avoid reading stale data,


628
00:39:58,820 --> 00:40:00,140
and you know as we'll seen in a second,


629
00:40:00,140 --> 00:40:02,930
we do you don't do a perfect job at [],


630
00:40:02,930 --> 00:40:08,920
but try hard to minimize you know occurrences with the clients reading stale data.


631
00:40:09,500 --> 00:40:10,040
Okay?


632
00:40:11,470 --> 00:40:13,780
Those reading reasonable straightforward.


633
00:40:15,450 --> 00:40:16,920
So let's look at the writing.


634
00:40:20,690 --> 00:40:22,910
So this is a picture from the paper.


635
00:40:24,880 --> 00:40:26,860
And so let's say client was the,


636
00:40:26,860 --> 00:40:28,450
let's focus append.


637
00:40:40,440 --> 00:40:45,510
And so they argued that the very common operation for them needs to append a record to a file


638
00:40:45,720 --> 00:40:52,470
and can we see why, given what you guys you know from mapreduce,


639
00:40:52,470 --> 00:40:56,010
you know Google does make sense, why append is so important.


640
00:41:01,830 --> 00:41:06,660
Because largely in doing mapreduce, you need to,


641
00:41:06,810 --> 00:41:11,070
as the map function spits out information,


642
00:41:11,070 --> 00:41:15,960
it's largely just adding on information rather than changing previously [spit] [out] informations.


643
00:41:16,020 --> 00:41:18,360
Yeah, you know maybe the map is not the best example,


644
00:41:18,360 --> 00:41:20,940
because write to the local files not to GFS,


645
00:41:20,940 --> 00:41:24,630
but the reducer does, the same argument also reducer.


646
00:41:25,630 --> 00:41:30,280
Yeah, so they're going to work there at writings basically consume a lot of information


647
00:41:30,430 --> 00:41:32,680
and you know append record soon to file


648
00:41:32,680 --> 00:41:35,170
with the resulting computation with result of the computation.


649
00:41:35,470 --> 00:41:36,010
Okay?


650
00:41:36,970 --> 00:41:39,670
Good, so you know step 1 we have clients,


651
00:41:40,180 --> 00:41:43,630
it will talk to the master to figure out where to write.


652
00:41:44,520 --> 00:41:46,650
And so the master looks in it's table,


653
00:41:46,650 --> 00:41:50,250
where the filename to chunk handles,


654
00:41:56,510 --> 00:41:58,460
and finds you know the chunk handles


655
00:41:58,490 --> 00:42:06,380
and then you know looks at this table of chunk handles to servers,


656
00:42:08,360 --> 00:42:10,040
to find the list of serves that has,


657
00:42:10,190 --> 00:42:11,600
that have a particular thing.


658
00:42:12,170 --> 00:42:13,340
They have that particular chunk.


659
00:42:13,950 --> 00:42:15,210
Okay, so what happens next,


660
00:42:15,240 --> 00:42:16,890
so there's two cases,


661
00:42:16,950 --> 00:42:20,040
when there's already primary in the second case,


662
00:42:20,040 --> 00:42:22,950
the first case with two cases having a primary or not primary.


663
00:42:23,650 --> 00:42:29,110
Let's say this is the very first time that this particular client contacts the master for this particular chunk,


664
00:42:29,290 --> 00:42:30,820
nobody else has done it so far,


665
00:42:30,850 --> 00:42:31,840
so there's no primary.


666
00:42:32,510 --> 00:42:35,120
In that case, you know we need to do,


667
00:42:35,850 --> 00:42:38,070
master needs to pick a primary, right,


668
00:42:38,070 --> 00:42:38,790
how did it do that.


669
00:42:39,950 --> 00:42:44,690
I think the master just picks any of the available chunk servers, right.


670
00:42:46,290 --> 00:42:50,550
Yeah it's one, so each one of those primary and other ones are the secondary,


671
00:42:50,880 --> 00:42:52,920
what other steps are involved in this sort of.


672
00:42:53,340 --> 00:42:59,160
Yeah, and then subsequently the master grants a lease to that primary


673
00:42:59,160 --> 00:43:03,240
and at least has a certain like date of expiry.


674
00:43:03,660 --> 00:43:06,570
Yeah, what else do, you have one or more other piece of crucial information.


675
00:43:10,150 --> 00:43:10,780
Even.


676
00:43:12,260 --> 00:43:14,000
Increment the version number?


677
00:43:14,000 --> 00:43:18,710
Yeah, yeah, one is increment version number,


678
00:43:19,970 --> 00:43:22,580
because you're gonna make a new primary


679
00:43:22,580 --> 00:43:24,680
and when every time you make a new primary, or whatever you make a new primary,


680
00:43:24,680 --> 00:43:29,090
you go to sort of want to think about is a new epoch in the file system


681
00:43:29,480 --> 00:43:31,970
or this particular file and so you increase the version number,


682
00:43:32,180 --> 00:43:33,680
because you have a new data,


683
00:43:34,600 --> 00:43:37,240
so basically the master increases the version number.


684
00:43:37,880 --> 00:43:43,730
Yeah, it's sends to the primary a new version number in the secondaries


685
00:43:43,760 --> 00:43:47,570
and saying like hey guys we're gonna start a new,


686
00:43:47,570 --> 00:43:51,320
we understand a new mutation you got for forming a replica group


687
00:43:51,560 --> 00:43:56,710
and your replica group with this particular version number whatever version number 12, right.


688
00:43:57,350 --> 00:44:00,170
And the primary and the secondary store version number,


689
00:44:00,170 --> 00:44:02,000
what do you is store the version number.


690
00:44:07,210 --> 00:44:11,770
Do they store on disk or on their disk or memory, or.


691
00:44:16,160 --> 00:44:16,850
I don't know.


692
00:44:18,780 --> 00:44:21,810
Anyone, what do you think.


693
00:44:22,740 --> 00:44:23,880
Okay, let's first do memory,


694
00:44:23,880 --> 00:44:25,200
let's say storage in memory,


695
00:44:25,200 --> 00:44:26,190
would that be a good design?


696
00:44:28,900 --> 00:44:29,590
No.


697
00:44:30,720 --> 00:44:31,170
Sorry.


698
00:44:31,770 --> 00:44:32,400
You can go.


699
00:44:32,790 --> 00:44:34,200
I guess it wouldn't,


700
00:44:34,200 --> 00:44:36,360
because if the chunk server goes down


701
00:44:36,360 --> 00:44:38,190
and then it comes back up it,


702
00:44:38,840 --> 00:44:40,700
it should know what version it has.


703
00:44:41,240 --> 00:44:44,300
Yeah, because otherwise you couldn't convince the primary that has the most recent one,


704
00:44:44,360 --> 00:44:46,880
was a primary of the master could big,


705
00:44:46,910 --> 00:44:48,740
you know the structure of motivation data.


706
00:44:49,610 --> 00:44:50,990
So it has to be on this,


707
00:44:51,020 --> 00:44:57,480
so basically the version number lives on disk, both at the chunk servers and at the master, right.


708
00:44:58,340 --> 00:45:00,560
So when the chunk, the master gets back,


709
00:45:00,560 --> 00:45:06,920
you know the acknowledgement from the primary and secondary that they written the version number to disk,


710
00:45:07,010 --> 00:45:10,220
and in the primary actually has received the release,


711
00:45:10,340 --> 00:45:15,260
then you know the master also writes its version number to disk,


712
00:45:15,260 --> 00:45:16,610
and then responds to the client.


713
00:45:18,050 --> 00:45:18,650
Okay?


714
00:45:19,780 --> 00:45:23,320
So to back to the client, responds with a list of servers,


715
00:45:23,500 --> 00:45:27,520
you know primary plus secondary plus version number.


716
00:45:29,350 --> 00:45:29,980
Okay?


717
00:45:31,580 --> 00:45:33,260
Then you know the next step


718
00:45:33,260 --> 00:45:36,710
and again here we see the whole goal is to type a lot of data from the network,


719
00:45:36,710 --> 00:45:43,190
is the client actually just sends the data that wants write to the to the primary and the secondary,


720
00:45:44,000 --> 00:45:46,130
the way it actually does is sort of an interesting way


721
00:45:46,130 --> 00:45:50,360
and basically contact the closest secondary it knows off, you know out of this list


722
00:45:50,630 --> 00:45:51,770
and sends the data there.


723
00:45:52,520 --> 00:45:56,570
And that secondary you know move the data over to the next person in the list


724
00:45:56,570 --> 00:45:58,340
and then to the next server list.


725
00:46:00,500 --> 00:46:07,760
And so this way you know the data sort of pumped from the clients you know to the pipeline to all the replicas


726
00:46:07,910 --> 00:46:11,090
and you know when the secondary receives,


727
00:46:11,090 --> 00:46:12,680
the first secondary receives some of the data,


728
00:46:12,680 --> 00:46:16,100
immediately starts actually pushing the data further down the pipeline.


729
00:46:16,760 --> 00:46:17,420
Okay?


730
00:46:18,960 --> 00:46:24,240
The reason this design is this way sort of basically this network interface the client that goes through the outside world,


731
00:46:24,300 --> 00:46:28,020
uses a full network interface to push the data down the pipeline.


732
00:46:29,000 --> 00:46:30,410
So that gives us high throughput.


733
00:46:31,550 --> 00:46:32,090
Okay?


734
00:46:34,650 --> 00:46:37,200
Okay, so then you know if this all successful


735
00:46:37,230 --> 00:46:40,800
and the data has been pushed you know to all the servers,


736
00:46:40,920 --> 00:46:43,020
those servers don't store that information in this yet


737
00:46:43,020 --> 00:46:45,240
and it just sits there sort of on the side,


738
00:46:45,240 --> 00:46:47,520
you know to be used in the next step,


739
00:46:48,120 --> 00:46:51,060
so the next step is basically for the client to send a message,


740
00:46:51,060 --> 00:46:55,450
like an append message you know to the primary


741
00:46:55,810 --> 00:47:00,610
and at that point you know the primary will check you know the version number, right,


742
00:47:02,240 --> 00:47:04,670
whether it actually version corresponds to the version number,


743
00:47:04,670 --> 00:47:07,220
it doesn't correspond to, if it doesn't match,


744
00:47:07,220 --> 00:47:08,990
then they probably won't allow it.


745
00:47:09,540 --> 00:47:12,390
The primary checks this lease, is the lease valid,


746
00:47:13,290 --> 00:47:14,940
because if lease is not valid anymore,


747
00:47:14,940 --> 00:47:17,940
it cannot accept any mutation operations,


748
00:47:17,940 --> 00:47:19,590
because it if lease is not valid,


749
00:47:19,590 --> 00:47:22,170
there might be another primary outside in the world.


750
00:47:23,100 --> 00:47:25,080
So it checks the lease,


751
00:47:25,610 --> 00:47:28,940
and then, if you know basically the version numbers match the lease is still valid,


752
00:47:29,150 --> 00:47:31,280
basically picks an offset to write ahead.


753
00:47:34,190 --> 00:47:37,670
And then the next step is we basically, write you know the data that just came in,


754
00:47:37,670 --> 00:47:40,940
you know this record to a stable storage,


755
00:47:40,970 --> 00:47:45,660
so the primary actually at this point write it to stable storage, the data


756
00:47:46,380 --> 00:47:48,480
and then sends messages to the secondary,


757
00:47:48,480 --> 00:47:50,160
saying please write the data to.


758
00:47:51,480 --> 00:47:55,380
And since the primary picks the offsets you know the,


759
00:47:55,380 --> 00:47:59,280
it tells the secondaries where to write you know that particular record into the file.


760
00:48:00,310 --> 00:48:03,370
So maybe you like whatever it takes to offset 125


761
00:48:03,370 --> 00:48:05,140
and it will tell the secondaries,


762
00:48:05,290 --> 00:48:10,360
go all to write you know the data that came in earlier at offset 125.


763
00:48:11,930 --> 00:48:13,700
Then, if everything workout,


764
00:48:13,700 --> 00:48:19,160
you know everybody, all the secondary in the primary successfully write their data back you know to disk,


765
00:48:19,310 --> 00:48:21,080
then it actually responds back to the client,


766
00:48:21,080 --> 00:48:25,250
saying like okay success you append actually happens.


767
00:48:27,770 --> 00:48:32,900
There's a way that the write actually might be not successful or it might not be successful


768
00:48:32,900 --> 00:48:35,480
and namely example, the primary has written into its own disk,


769
00:48:35,690 --> 00:48:37,790
but it fails to write it,


770
00:48:37,790 --> 00:48:40,310
you know it fails to connect to one of the secondaries,


771
00:48:40,310 --> 00:48:42,200
maybe secondary actually crashed,


772
00:48:42,200 --> 00:48:45,020
or maybe the secondary just has a network connection that doesn't work.


773
00:48:46,060 --> 00:48:50,980
And in that case, the primary actually returns an error to the client,


774
00:48:51,010 --> 00:48:58,390
so error if one secondary didn't respond.


775
00:49:05,510 --> 00:49:09,230
And in that case the client library what it will do is usually try or retry,


776
00:49:09,470 --> 00:49:15,240
it will reissue the same event, and will try again,


777
00:49:15,750 --> 00:49:20,790
in the hope that the second time around you know that data actually, that actually get through.


778
00:49:21,690 --> 00:49:25,290
And so this is what they recall like you do at-least-once.


779
00:49:30,830 --> 00:49:34,460
If you retry, will the primary pick the same offset?


780
00:49:36,630 --> 00:49:37,590
I don't think so.


781
00:49:40,740 --> 00:49:43,410
No it takes a new offset and you know writes the,


782
00:49:43,440 --> 00:49:45,360
write new particular offset,


783
00:49:45,750 --> 00:49:52,440
so that means if you look at the disks a file on the three replicas,


784
00:49:53,130 --> 00:49:55,680
you know the primary S1 S2,


785
00:49:55,680 --> 00:50:00,440
it might be the case something you wrote like you know S1 125 the data,


786
00:50:00,710 --> 00:50:02,840
we succeeded maybe S2 one two,


787
00:50:02,840 --> 00:50:07,520
but S2 actually doesn't happen, there's no data, right.


788
00:50:08,110 --> 00:50:09,850
And then we try again,


789
00:50:09,850 --> 00:50:11,830
we might rather read the same data x


790
00:50:11,830 --> 00:50:14,200
and maybe we'll succeed in all three.


791
00:50:14,950 --> 00:50:17,920
So you see here that basically replicates records can be duplicated.


792
00:50:20,960 --> 00:50:23,270
Is this something that can happen in a standard file system,


793
00:50:23,270 --> 00:50:26,030
like your Linux file system on your laptop or your computer.


794
00:50:31,710 --> 00:50:32,370
No.


795
00:50:33,210 --> 00:50:36,690
No, would you be surprised if your computer did this.


796
00:50:39,680 --> 00:50:40,160
I mean, yeah,


797
00:50:40,160 --> 00:50:42,110
this is not how standard file writes work.


798
00:50:42,410 --> 00:50:49,650
Yeah it would be inconvenient to have this property, or it doesn't matter.


799
00:50:53,140 --> 00:50:55,330
Yeah, yeah, yeah.


800
00:50:55,330 --> 00:50:57,610
Yeah, gonna be a pretty bizarre,


801
00:50:57,640 --> 00:51:01,450
you know presumably like you know you compiler you produce outputs in your file


802
00:51:01,780 --> 00:51:04,180
and then maybe you know certain blocks written twice


803
00:51:04,180 --> 00:51:06,100
and then you can run the program anymore,


804
00:51:06,130 --> 00:51:09,520
like you know the whole thing is just garbage at that point.


805
00:51:10,370 --> 00:51:15,170
So it would just be weird like you write an email message and this is the body of the email message shows up twice.


806
00:51:15,640 --> 00:51:18,940
So this is not your typical file system,


807
00:51:18,940 --> 00:51:21,400
we do and so there's like a slightly bizarre


808
00:51:21,970 --> 00:51:23,710
and you know what is the justification,


809
00:51:24,390 --> 00:51:26,970
why, why, why do you think this is a good idea.


810
00:51:29,800 --> 00:51:31,270
I'm not sure what is a good idea,


811
00:51:31,270 --> 00:51:35,170
but I'm confused how that works for mapreduce specifically,


812
00:51:35,350 --> 00:51:38,620
so if you run word count and you do that,


813
00:51:38,920 --> 00:51:41,200
and like some files and you'll count,


814
00:51:41,260 --> 00:51:44,710
and they're like word a, it shows up once,


815
00:51:44,710 --> 00:51:46,510
but you do it twice,


816
00:51:46,510 --> 00:51:47,740
because something failed


817
00:51:47,800 --> 00:51:49,900
and now you have a,1 a,1,


818
00:51:50,110 --> 00:51:51,910
so your a count is going to be wrong.


819
00:51:52,370 --> 00:51:54,680
How, yeah I'm confused.


820
00:51:55,240 --> 00:51:58,120
Yes, right, how do they work,


821
00:51:58,120 --> 00:51:59,710
it seems like if you don't do anything,


822
00:51:59,710 --> 00:52:01,810
then this is really highly inconvenient,


823
00:52:02,140 --> 00:52:05,680
where it actually returns to the application, will compute the wrong result.


824
00:52:07,230 --> 00:52:19,070
They said use checksums and a unique IDs to check, you know every yeah, every record was like once.


825
00:52:20,080 --> 00:52:23,140
Additionally, when you do record appends,


826
00:52:23,140 --> 00:52:26,680
the response which has returned from the primary to the client,


827
00:52:26,680 --> 00:52:29,980
gives you the offset into the file,


828
00:52:29,980 --> 00:52:31,720
where your data was actually written


829
00:52:31,720 --> 00:52:35,500
and the rest of it is assumed to be like undefined.


830
00:52:36,740 --> 00:52:38,570
Yeah, that I think the key point here correct is


831
00:52:38,570 --> 00:52:41,960
like basically the application doesn't interact with the file system directly,


832
00:52:41,960 --> 00:52:43,400
interact for some library,


833
00:52:43,400 --> 00:52:45,980
in the library, basically if you write 10 records,


834
00:52:46,340 --> 00:52:48,320
the library sticks an id in it


835
00:52:48,680 --> 00:52:52,130
and so I also use the library to read these records


836
00:52:52,130 --> 00:52:55,340
and so if you see a record with the same id, you know skip the second one.


837
00:52:55,780 --> 00:52:58,030
Because you know it's been clearly the same one.


838
00:52:58,940 --> 00:53:04,130
And you know they have double, an extra thing in there for checksums to make sure the record didn't get garbled,


839
00:53:04,370 --> 00:53:09,920
and basically to detect change in the bytes,


840
00:53:09,920 --> 00:53:12,710
but you know the id basically helps them to decide,


841
00:53:13,070 --> 00:53:16,310
allows the library to decide, well this is the same record,


842
00:53:16,340 --> 00:53:20,450
I'm not gonna give it to the application, or the applications doesn't need to process it.


843
00:53:21,520 --> 00:53:22,360
Okay?


844
00:53:23,390 --> 00:53:27,500
My question is instead of rewriting to every replica,


845
00:53:27,500 --> 00:53:30,590
wouldn't it be better to remember which replicates failing


846
00:53:31,070 --> 00:53:33,740
and to stop until it can be written to that one.


847
00:53:34,620 --> 00:53:36,540
Yeah, there's a bunch of different designs possible,


848
00:53:36,540 --> 00:53:40,410
when let's return to later,


849
00:53:40,410 --> 00:53:44,670
in you know I think one reason that they do this this way is


850
00:53:44,670 --> 00:53:46,920
like if they're putting a temporary failure,


851
00:53:46,920 --> 00:53:50,670
like a network disconnection or whatever you know the lease the write will succeed and they will continue.


852
00:53:51,380 --> 00:53:53,510
And just have to be any reconfiguration,


853
00:53:53,510 --> 00:53:58,410
there has to be nothing, you know and the write can just keep going, right.


854
00:53:58,470 --> 00:54:00,630
Yeah, so the writers have to fail.


855
00:54:02,630 --> 00:54:03,170
Okay.


856
00:54:03,820 --> 00:54:05,980
It's just a quick question in general,


857
00:54:06,400 --> 00:54:11,170
the all of these servers are trusted, right, there's no.


858
00:54:11,200 --> 00:54:15,640
Yes, absolutely, this is actually important point,


859
00:54:15,640 --> 00:54:17,440
this is not like you know your Linux file system,


860
00:54:17,440 --> 00:54:21,400
where there's permissions and access control writes and all that kind of stuff.


861
00:54:22,180 --> 00:54:24,310
If the servers are completely trusted,


862
00:54:24,520 --> 00:54:26,590
the clients are trusted, the masters trusted,


863
00:54:26,590 --> 00:54:29,350
the software written by Google is trusted, the whole thing is trusted.


864
00:54:31,440 --> 00:54:33,360
Right, this is your complete internal file system.


865
00:54:34,220 --> 00:54:35,210
In fact, it's sort of cool,


866
00:54:35,210 --> 00:54:39,140
it's like a little bit may be surprising that we don't even know about this file system in such amout detail,


867
00:54:39,170 --> 00:54:41,660
because it's only used inside Google


868
00:54:41,660 --> 00:54:43,010
and one of the cool things is that


869
00:54:43,010 --> 00:54:46,090
you know period of time and still they do,


870
00:54:46,240 --> 00:54:49,840
they wrote up the papers and describing actually how these systems work


871
00:54:49,870 --> 00:54:51,970
and there's one reason we know that,


872
00:54:52,450 --> 00:54:55,780
it's actually extremely cool, that they did that.


873
00:54:59,110 --> 00:54:59,650
Okay.


874
00:55:01,660 --> 00:55:04,990
So okay, so we now understand how read works, what write works,


875
00:55:04,990 --> 00:55:08,770
you know there's some sort of interesting behaviors,


876
00:55:09,130 --> 00:55:11,350
I want to talk a little bit more about consistency, correct


877
00:55:11,350 --> 00:55:18,130
and that really comes down to you know what a read observe after you did append


878
00:55:18,370 --> 00:55:22,270
and the homework question really got after this


879
00:55:22,270 --> 00:55:26,080
and what I would like to do now is to take a quick break out, like five minutes


880
00:55:26,260 --> 00:55:30,220
and so you can discuss you know the answer to this question


881
00:55:30,340 --> 00:55:33,760
and then come back and talk a little bit more in detail about consistency.


882
00:55:34,220 --> 00:55:34,850
Okay?


883
00:55:36,370 --> 00:55:42,070
I'm gonna make [].


884
00:55:43,160 --> 00:55:45,210
Okay, everybody back.


885
00:55:46,110 --> 00:55:48,120
Can everybody here me, double checking.


886
00:55:48,570 --> 00:55:50,310
Yeah, hey professor question,


887
00:55:50,310 --> 00:55:53,490
can you go back to the slide with the,


888
00:55:54,580 --> 00:55:56,950
when we talked about the write slide here,


889
00:55:56,980 --> 00:56:01,230
so you mentioned, the master responds to the client through the version number,


890
00:56:02,780 --> 00:56:04,280
and if that is the key,


891
00:56:04,280 --> 00:56:07,370
then is it possible, is it even possible,


892
00:56:08,120 --> 00:56:10,220
wouldn't even have to read like a stale data,


893
00:56:10,220 --> 00:56:14,030
because the client has a version number and the chunk servers without the version number,


894
00:56:14,330 --> 00:56:15,890
so they can just compare those,


895
00:56:16,070 --> 00:56:21,890
if they don't match the client, the chunk servers can just say I I have a [] later out,


896
00:56:21,890 --> 00:56:24,410
so you should not read this.


897
00:56:25,170 --> 00:56:26,190
Okay, let's go,


898
00:56:26,250 --> 00:56:28,200
yeah, let's go for this scenario a little bit more detail.


899
00:56:28,200 --> 00:56:29,700
Let me actually get rid of this window.


900
00:56:36,090 --> 00:56:37,440
Okay, let's talk about it,


901
00:56:37,440 --> 00:56:39,120
so I think this scenario we're talking about,


902
00:56:39,450 --> 00:56:42,450
that leads into a problematic situation as follows,


903
00:56:42,630 --> 00:56:44,730
we have primary, we have secondary,


904
00:56:44,730 --> 00:56:49,980
two secondary, secondary one, secondary two,


905
00:56:49,980 --> 00:56:51,720
we have a client at this side.


906
00:56:52,420 --> 00:56:58,960
We have a primary client reach you know it's back like a version number say 10.


907
00:57:03,440 --> 00:57:09,020
The, later on another a primary will.


908
00:57:16,680 --> 00:57:19,350
Okay, so S2 get some servers,


909
00:57:19,350 --> 00:57:21,600
then at some point, the,


910
00:57:22,920 --> 00:57:26,610
so this information is cached on the side


911
00:57:26,610 --> 00:57:32,490
you know, maybe you know one of the thing secondary is like S2 crashes,


912
00:57:32,900 --> 00:57:35,960
or at least appears to be disconnected from the network.


913
00:57:36,650 --> 00:57:39,800
So what the master will do is increment version numbers,


914
00:57:39,800 --> 00:57:43,430
you know go to 11 messages 11,


915
00:57:44,160 --> 00:57:48,540
then you know another client may come around and start writing.


916
00:57:49,320 --> 00:57:54,870
So we'll write a new value to S1 and S2 for the file,


917
00:57:54,870 --> 00:57:56,340
so the chunk has now been updated,


918
00:57:56,340 --> 00:58:01,590
so let's say chunks original 10 same as version number, now it's 11 here,


919
00:58:02,460 --> 00:58:07,590
but you know it's the case that even though the master primary secondary couldn't talk secondary two,


920
00:58:07,590 --> 00:58:11,850
but the second client, first client could still talk to the secondary


921
00:58:12,060 --> 00:58:14,340
and it will read the version numbers match,


922
00:58:14,370 --> 00:58:15,510
they're both 10,


923
00:58:17,340 --> 00:58:21,990
and it will read, it'll send 10 back, right,


924
00:58:21,990 --> 00:58:24,840
so you have a case where the write has completed,


925
00:58:25,220 --> 00:58:26,990
as acknowledged to be okay


926
00:58:27,260 --> 00:58:30,980
and nevertheless there's a client that actually will read a stale value back.


927
00:58:31,680 --> 00:58:34,500
So, why doesn't the 11 go back to the client?


928
00:58:36,550 --> 00:58:38,590
The first client,


929
00:58:39,650 --> 00:58:42,620
the reason is because the first client caches it for a longer period of time,


930
00:58:43,280 --> 00:58:45,800
they don't actually have anything in the protocol that actually does that.


931
00:58:50,060 --> 00:58:57,320
So does the version increment when the when the system tries to push an update to S2


932
00:58:57,320 --> 00:58:59,210
and it's not able to, or.


933
00:58:59,300 --> 00:59:01,100
Version numbers only increment,


934
00:59:01,430 --> 00:59:03,320
the version number is maintained by the master,


935
00:59:03,320 --> 00:59:06,350
they only increment when you select a new primary,


936
00:59:12,910 --> 00:59:13,900
not when you do it,


937
00:59:13,900 --> 00:59:16,660
there's a there's also a serial number, that they talk about,


938
00:59:17,020 --> 00:59:18,460
but its different from the version number,


939
00:59:18,490 --> 00:59:20,590
that just to order you know the writes.


940
00:59:22,670 --> 00:59:23,240
Okay?


941
00:59:23,920 --> 00:59:27,730
How does the primary know which secondary is it has to check with,


942
00:59:27,880 --> 00:59:30,640
before making, before completing it write successfully.


943
00:59:31,360 --> 00:59:33,100
The primary, the master tells it,


944
00:59:34,140 --> 00:59:36,900
master tells the primary you know secondary you need to update.


945
00:59:39,720 --> 00:59:43,080
So when the master basically issues the lease to the primary


946
00:59:43,200 --> 00:59:46,710
and if one of the secondaries is down at that moment,


947
00:59:46,920 --> 00:59:48,990
does the master consider this a failure,


948
00:59:48,990 --> 00:59:52,860
or does it just update the version number for the servers that are alive


949
00:59:52,860 --> 00:59:55,350
and it just forget about the other one,


950
00:59:55,350 --> 00:59:57,930
because it's going to have an outdated version number anyway.


951
00:59:58,410 --> 01:00:01,590
Yeah, the papers is a little bit [] exactly,


952
01:00:01,590 --> 01:00:06,420
how the recovery part the reconfiguration stuff works,


953
01:00:06,630 --> 01:00:12,300
but I imagine that basically the primary actually does heartbeats with P1 S1 S2,


954
01:00:12,630 --> 01:00:15,150
it's on point, the site so S2 use that


955
01:00:15,420 --> 01:00:21,240
and at that point it will point and the lease the primary maybe runs out.


956
01:00:21,660 --> 01:00:24,930
And then it will create a new primary and a S1


957
01:00:24,930 --> 01:00:29,070
and another S you know actual hold you know, or maybe just S1,


958
01:00:29,070 --> 01:00:32,670
because there's no, no additional chunk server


959
01:00:32,850 --> 01:00:35,910
and that forms the new replica group for that chunk.


960
01:00:37,620 --> 01:00:40,860
Also, the lease doesn't run out yet, basically.


961
01:00:41,550 --> 01:00:44,310
Well, the primary we can't appoint,


962
01:00:44,310 --> 01:00:45,990
okay so here's some interesting cases,


963
01:00:45,990 --> 01:00:49,020
let's see, so, you guys are doing exactly the thing I want,


964
01:00:49,110 --> 01:00:53,580
based on this paper, which you really start thinking about all the problematic cases.


965
01:00:53,990 --> 01:00:56,090
And this is exactly how you think about consistency,


966
01:00:56,120 --> 01:00:57,740
when you start thinking about consistency,


967
01:00:57,740 --> 01:01:00,230
you need to consider all possible failures


968
01:01:00,620 --> 01:01:04,070
and argue whether you know those failures lead to inconsistencies.


969
01:01:04,700 --> 01:01:08,210
So, one thing let's talk about this one case,


970
01:01:08,240 --> 01:01:10,760
where we've got a master, we've got a primary.


971
01:01:11,560 --> 01:01:16,690
And let's say the primary in the master or get disconnected,


972
01:01:16,750 --> 01:01:18,880
actually let me draw the picture slightly differently,


973
01:01:21,170 --> 01:01:22,400
master in the middle.


974
01:01:23,920 --> 01:01:26,680
And we got server, we got server here,


975
01:01:27,500 --> 01:01:29,060
you know S1, S2,


976
01:01:29,060 --> 01:01:30,500
and let's say S2 is the primary,


977
01:01:32,310 --> 01:01:37,200
and so you know whatever you may talk to some other servers out there,


978
01:01:38,540 --> 01:01:43,160
perhaps even S1 is the, is one of the secondary for this primary.


979
01:01:43,930 --> 01:01:46,030
So let's say a network petition,


980
01:01:46,270 --> 01:01:50,560
so the master sends messages, you know heartbeat messages, doesn't get a response.


981
01:01:53,270 --> 01:01:55,640
When can the master point to a new primary.


982
01:01:59,270 --> 01:02:01,610
When the lease is over for S2?


983
01:02:04,060 --> 01:02:08,740
Yeah, right, because a primary has to wait,


984
01:02:08,740 --> 01:02:11,050
but the master has to wait until the lease has expired.


985
01:02:11,610 --> 01:02:14,910
Because if the lease was not expired,


986
01:02:15,090 --> 01:02:18,030
then maybe we have two primaries at the same time, right,


987
01:02:18,800 --> 01:02:22,100
P1 and P2 are staying at the same time


988
01:02:22,100 --> 01:02:23,390
and would that be bad?


989
01:02:26,540 --> 01:02:28,880
Yeah, then I think,


990
01:02:30,180 --> 01:02:34,170
wait would would clients, clients wouldn't know where to send to


991
01:02:34,170 --> 01:02:36,480
and the master wouldn't know which one is primary, right.


992
01:02:36,510 --> 01:02:39,750
Well, presumably some clients still talking to this primary, right,


993
01:02:40,410 --> 01:02:42,480
while you might be talking to this primary,


994
01:02:43,200 --> 01:02:44,670
in a primary for the same chunk.


995
01:02:47,820 --> 01:02:49,860
I think you get very bizarre ordering, right,


996
01:02:49,950 --> 01:02:51,990
where like some writes will get lost,


997
01:02:51,990 --> 01:02:53,910
you know you know it would be a mess.


998
01:02:54,630 --> 01:02:56,070
It would be not a principal,


999
01:02:56,070 --> 01:03:00,900
you know argument where one you know we're all writes happening order and one at a time.


1000
01:03:03,900 --> 01:03:04,980
So this is a bad situation


1001
01:03:04,980 --> 01:03:08,130
and this situation is avoided like the split brain syndrome,


1002
01:03:08,130 --> 01:03:10,200
is sometimes called split brain syndrome,


1003
01:03:10,200 --> 01:03:12,510
where you end up with a system where have two masters.


1004
01:03:12,980 --> 01:03:17,000
And this is problem here avoided, because of the lease


1005
01:03:17,990 --> 01:03:25,880
and the master will not appoint any other primary until the lease of the first primary absolutely has has expired


1006
01:03:25,880 --> 01:03:28,910
and it knows even if the primaries up, but not reachable to it,


1007
01:03:28,910 --> 01:03:30,530
but may be reasonable to other clients,


1008
01:03:30,590 --> 01:03:33,080
that primary won't accept any write messages anymore,


1009
01:03:33,080 --> 01:03:34,400
because lease has expired.


1010
01:03:40,210 --> 01:03:40,960
Does that make sense?


1011
01:03:43,400 --> 01:03:46,340
Okay, let me say one more thing before wrapping up,


1012
01:03:46,580 --> 01:03:52,510
apologizes partly because you know, I had some technical problems,


1013
01:03:52,510 --> 01:03:54,490
but I wanted to make one more point


1014
01:03:54,490 --> 01:03:58,390
and it came up in the discussion two in the breakup room,


1015
01:03:58,420 --> 01:04:00,370
which is you know how could you do better,


1016
01:04:00,940 --> 01:04:02,470
how to get strong consistency,


1017
01:04:06,070 --> 01:04:09,370
or maybe just stronger, got pretty strong consistency,


1018
01:04:09,370 --> 01:04:15,490
that you know not with some you know issues.


1019
01:04:16,100 --> 01:04:17,990
And so there's a hundred different ways you could do it


1020
01:04:17,990 --> 01:04:24,980
and in fact, you know, what we're going to be seeing.


1021
01:04:25,740 --> 01:04:28,980
You know one, I think one issue that shows up here all the time is


1022
01:04:28,980 --> 01:04:32,730
like you could instead of like obtaining the primary and then reporting,


1023
01:04:33,340 --> 01:04:35,020
making writes visible incrementally,


1024
01:04:35,020 --> 01:04:36,190
it's probably not a good idea,


1025
01:04:36,400 --> 01:04:43,900
so you probably want to do is like update all secondary primaries or not,


1026
01:04:45,220 --> 01:04:47,320
but not in this you know particular design,


1027
01:04:47,320 --> 01:04:49,960
where like somebody get updated and some may not get updated,


1028
01:04:49,960 --> 01:04:51,520
that's actually visible to the client.


1029
01:04:52,660 --> 01:04:57,730
So there's a bunch of like you know techniques or protocol changes that you could do,


1030
01:04:58,030 --> 01:05:02,350
that will make this better and in fact you will see in labs 2 and 3,


1031
01:05:02,380 --> 01:05:05,590
you will build systems that actually have the stronger properties,


1032
01:05:06,370 --> 01:05:11,080
and deal with the scenarios you know concurrently lead to consistency.


1033
01:05:11,690 --> 01:05:13,550
In fact, if you look at Google's self


1034
01:05:13,820 --> 01:05:15,860
and we'll read some of these states later,


1035
01:05:16,040 --> 01:05:21,830
Google build additional storage systems, other storage systems that have stronger consistency.


1036
01:05:25,930 --> 01:05:29,170
And basically tailored those to a different application domain,


1037
01:05:29,410 --> 01:05:33,370
for example like in like halfway to term [] just paper spanner,


1038
01:05:33,640 --> 01:05:37,540
you know that actually has a much stronger storage for consistency


1039
01:05:37,540 --> 01:05:40,030
and and even have support for transactions.


1040
01:05:40,570 --> 01:05:43,150
But it was like the application domain is quite different,


1041
01:05:43,180 --> 01:05:44,590
you know you can sort of see here,


1042
01:05:44,590 --> 01:05:46,150
the GFS is really tailored,


1043
01:05:46,150 --> 01:05:49,510
you know to sort of running mapreduce jobs.


1044
01:05:51,680 --> 01:05:55,430
Okay, so I hope this is a useful introduction for consistency


1045
01:05:55,430 --> 01:05:57,650
and start thinking about these kinds of problems,


1046
01:05:57,950 --> 01:06:00,290
because they won't be recurring set of problems,


1047
01:06:00,290 --> 01:06:02,090
that will show up in the rest of the term.


1048
01:06:03,760 --> 01:06:05,560
And I apologize for running over a little bit.


1049
01:06:07,740 --> 01:06:08,400
Thank you.


1050
01:06:08,400 --> 01:06:10,950
Hang around, so people want to ask additional questions,


1051
01:06:10,950 --> 01:06:13,350
feel free to ask it


1052
01:06:13,620 --> 01:06:15,300
and you have to run to another class,


1053
01:06:15,300 --> 01:06:16,920
you know please run to another class.


